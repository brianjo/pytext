
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<div class="section" id="pytext-models-representations-package">
<h1>pytext.models.representations package<a class="headerlink" href="#pytext-models-representations-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.representations.transformer.html">pytext.models.representations.transformer package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.transformer.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.multihead_attention">pytext.models.representations.transformer.multihead_attention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.positional_embedding">pytext.models.representations.transformer.positional_embedding module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.residual_mlp">pytext.models.representations.transformer.residual_mlp module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.sentence_encoder">pytext.models.representations.transformer.sentence_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.transformer">pytext.models.representations.transformer.transformer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-pytext.models.representations.attention">
<span id="pytext-models-representations-attention-module"></span><h2>pytext.models.representations.attention module<a class="headerlink" href="#module-pytext.models.representations.attention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.attention.DotProductSelfAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.attention.</code><code class="sig-name descname">DotProductSelfAttention</code><span class="sig-paren">(</span><em class="sig-param">input_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#DotProductSelfAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.DotProductSelfAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<p>Given vector w and token vectors = {t1, t2, …, t_n}, compute self attention
weights to weighs the tokens
* a_j = softmax(w . t_j)</p>
<dl class="method">
<dt id="pytext.models.representations.attention.DotProductSelfAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">tokens</em>, <em class="sig-param">tokens_mask</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#DotProductSelfAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.DotProductSelfAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Input:</dt><dd><p>x: batch_size * seq_len * input_dim
x_mask: batch_size * seq_len (1 for padding, 0 for true)</p>
</dd>
<dt>Output:</dt><dd><p>alpha: batch_size * seq_len</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.attention.DotProductSelfAttention.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.attention.DotProductSelfAttention.Config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#DotProductSelfAttention.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.DotProductSelfAttention.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.attention.MultiplicativeAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.attention.</code><code class="sig-name descname">MultiplicativeAttention</code><span class="sig-paren">(</span><em class="sig-param">p_hidden_dim</em>, <em class="sig-param">q_hidden_dim</em>, <em class="sig-param">normalize</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#MultiplicativeAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.MultiplicativeAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<p>Given sequence P and vector q, computes attention weights for each element
in P by matching q with each element in P using multiplicative attention.
* a_i = softmax(p_i . W . q)</p>
<dl class="method">
<dt id="pytext.models.representations.attention.MultiplicativeAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">p_seq: torch.Tensor</em>, <em class="sig-param">q: torch.Tensor</em>, <em class="sig-param">p_mask: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#MultiplicativeAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.MultiplicativeAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Input:</dt><dd><p>p_seq: batch_size * p_seq_len * p_hidden_dim
q: batch_size * q_hidden_dim
p_mask: batch_size * p_seq_len (1 for padding, 0 for true)</p>
</dd>
<dt>Output:</dt><dd><p>attn_scores: batch_size * p_seq_len</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.attention.MultiplicativeAttention.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.attention.MultiplicativeAttention.Config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#MultiplicativeAttention.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.MultiplicativeAttention.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.attention.SequenceAlignedAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.attention.</code><code class="sig-name descname">SequenceAlignedAttention</code><span class="sig-paren">(</span><em class="sig-param">proj_dim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#SequenceAlignedAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.SequenceAlignedAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<p>Given sequences P and Q, computes attention weights for each element in P by
matching Q with each element in P.
* a_i_j = softmax(p_i . q_j) where softmax is computed by summing over q_j</p>
<dl class="method">
<dt id="pytext.models.representations.attention.SequenceAlignedAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">p: torch.Tensor</em>, <em class="sig-param">q: torch.Tensor</em>, <em class="sig-param">q_mask: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#SequenceAlignedAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.SequenceAlignedAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="simple">
<dt>Input:</dt><dd><p>p: batch_size * p_seq_len * dim
q: batch_size * q_seq_len * dim
q_mask: batch_size * q_seq_len (1 for padding, 0 for true)</p>
</dd>
<dt>Output:</dt><dd><p>matched_seq: batch_size * doc_seq_len * dim</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.attention.SequenceAlignedAttention.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.attention.SequenceAlignedAttention.Config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/attention.html#SequenceAlignedAttention.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.attention.SequenceAlignedAttention.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.augmented_lstm">
<span id="pytext-models-representations-augmented-lstm-module"></span><h2>pytext.models.representations.augmented_lstm module<a class="headerlink" href="#module-pytext.models.representations.augmented_lstm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTM">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.augmented_lstm.</code><code class="sig-name descname">AugmentedLSTM</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.augmented_lstm.AugmentedLSTM.Config</em>, <em class="sig-param">input_size: int</em>, <em class="sig-param">padding_value: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/augmented_lstm.html#AugmentedLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p><cite>AugmentedLSTM</cite> implements a generic AugmentedLSTM representation layer.
AugmentedLSTM is an LSTM which optionally appends
an optional highway network to the output layer.
Furthermore the dropout_rate controlls the level of variational dropout done.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Config</em>) – Configuration object of type BiLSTM.Config.</p></li>
<li><p><strong>input_size</strong> (<em>int</em>) – The number of expected features in the input.</p></li>
<li><p><strong>padding_value</strong> (<em>float</em>) – Value for the padded elements. Defaults to 0.0.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTM.padding_value">
<code class="sig-name descname">padding_value</code><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTM.padding_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Value for the padded elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTM.forward_layers">
<code class="sig-name descname">forward_layers</code><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTM.forward_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>A module list of unidirectional AugmentedLSTM
layers moving forward in time.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.ModuleList</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTM.backward_layers">
<code class="sig-name descname">backward_layers</code><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTM.backward_layers" title="Permalink to this definition">¶</a></dt>
<dd><p>A module list of unidirectional AugmentedLSTM
layers moving backward in time.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.ModuleList</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTM.representation_dim">
<code class="sig-name descname">representation_dim</code><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTM.representation_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>The calculated dimension of the output features
of AugmentedLSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em>, <em class="sig-param">states: Optional[Tuple[torch.Tensor</em>, <em class="sig-param">torch.Tensor]] = None</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]<a class="reference internal" href="../_modules/pytext/models/representations/augmented_lstm.html#AugmentedLSTM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an input batch of sequential data such as word embeddings, produces
a AugmentedLSTM representation of the sequential input and new state
tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedded_tokens</strong> (<em>torch.Tensor</em>) – Input tensor of shape
(bsize x seq_len x input_dim).</p></li>
<li><p><strong>seq_lengths</strong> (<em>torch.Tensor</em>) – List of sequences lengths of each batch element.</p></li>
<li><p><strong>states</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Tuple of tensors containing
the initial hidden state and the cell state of each element in
the batch. Each of these tensors have a dimension of
(bsize x num_layers x num_directions * nhid). Defaults to <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>AgumentedLSTM representation of input and
the state of the LSTM <cite>t = seq_len</cite>.
Shape of representation is (bsize x seq_len x representation_dim).
Shape of each state is (bsize x num_layers * num_directions x nhid).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMCell">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.augmented_lstm.</code><code class="sig-name descname">AugmentedLSTMCell</code><span class="sig-paren">(</span><em class="sig-param">embed_dim: int</em>, <em class="sig-param">lstm_dim: int</em>, <em class="sig-param">use_highway: bool</em>, <em class="sig-param">use_bias: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/augmented_lstm.html#AugmentedLSTMCell"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMCell" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p><cite>AugmentedLSTMCell</cite> implements a AugmentedLSTM cell.
:param embed_dim: The number of expected features in the input.
:type embed_dim: int
:param lstm_dim: Number of features in the hidden state of the LSTM.
:type lstm_dim: int
:param Defaults to 32.:
:param use_highway: If <cite>True</cite> we append a highway network to the
:type use_highway: bool
:param outputs of the LSTM.:
:param use_bias: If <cite>True</cite> we use a bias in our LSTM calculations, otherwise
:type use_bias: bool
:param we don’t.:</p>
<dl class="attribute">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMCell.input_linearity">
<code class="sig-name descname">input_linearity</code><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMCell.input_linearity" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused weight matrix which
computes a linear function over the input.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMCell.state_linearity">
<code class="sig-name descname">state_linearity</code><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMCell.state_linearity" title="Permalink to this definition">¶</a></dt>
<dd><p>Fused weight matrix which
computes a linear function over the states.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMCell.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x: torch.Tensor, states=typing.Tuple[torch.Tensor, torch.Tensor], variational_dropout_mask: Optional[torch.Tensor] = None</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/representations/augmented_lstm.html#AugmentedLSTMCell.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMCell.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Warning: DO NOT USE THIS LAYER DIRECTLY, INSTEAD USE the AugmentedLSTM class</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>torch.Tensor</em>) – Input tensor of shape
(bsize x input_dim).</p></li>
<li><p><strong>states</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Tuple of tensors containing
the hidden state and the cell state of each element in
the batch. Each of these tensors have a dimension of
(bsize x nhid). Defaults to <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Returned states. Shape of each state is (bsize x nhid).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, torch.Tensor]</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMCell.reset_parameters">
<code class="sig-name descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/augmented_lstm.html#AugmentedLSTMCell.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMCell.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMUnidirectional">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.augmented_lstm.</code><code class="sig-name descname">AugmentedLSTMUnidirectional</code><span class="sig-paren">(</span><em class="sig-param">input_size: int</em>, <em class="sig-param">hidden_size: int</em>, <em class="sig-param">go_forward: bool = True</em>, <em class="sig-param">recurrent_dropout_probability: float = 0.0</em>, <em class="sig-param">use_highway: bool = True</em>, <em class="sig-param">use_input_projection_bias: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/augmented_lstm.html#AugmentedLSTMUnidirectional"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMUnidirectional" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p><cite>AugmentedLSTMUnidirectional</cite> implements a one-layer single directional
AugmentedLSTM layer. AugmentedLSTM is an LSTM which optionally
appends an optional highway network to the output layer. Furthermore the
dropout_rate controlls the level of variational dropout done.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input_size</strong> (<em>int</em>) – The number of expected features in the input.</p></li>
<li><p><strong>hidden_size</strong> (<em>int</em>) – Number of features in the hidden state of the LSTM.
Defaults to 32.</p></li>
<li><p><strong>go_forward</strong> (<em>bool</em>) – Whether to compute features left to right (forward)
or right to left (backward).</p></li>
<li><p><strong>recurrent_dropout_probability</strong> (<em>float</em>) – Variational dropout probability
to use. Defaults to 0.0.</p></li>
<li><p><strong>use_highway</strong> (<em>bool</em>) – If <cite>True</cite> we append a highway network to the
outputs of the LSTM.</p></li>
<li><p><strong>use_input_projection_bias</strong> (<em>bool</em>) – If <cite>True</cite> we use a bias in
our LSTM calculations, otherwise we don’t.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMUnidirectional.cell">
<code class="sig-name descname">cell</code><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMUnidirectional.cell" title="Permalink to this definition">¶</a></dt>
<dd><p>AugmentedLSTMCell that is applied at every timestep.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>AugmentedLSTMCell</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMUnidirectional.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.nn.utils.rnn.PackedSequence</em>, <em class="sig-param">states: Optional[Tuple[torch.Tensor</em>, <em class="sig-param">torch.Tensor]] = None</em><span class="sig-paren">)</span> → Tuple[torch.nn.utils.rnn.PackedSequence, Tuple[torch.Tensor, torch.Tensor]]<a class="reference internal" href="../_modules/pytext/models/representations/augmented_lstm.html#AugmentedLSTMUnidirectional.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMUnidirectional.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Warning: DO NOT USE THIS LAYER DIRECTLY, INSTEAD USE the AugmentedLSTM class</p>
<p>Given an input batch of sequential data such as word embeddings, produces
a single layer unidirectional AugmentedLSTM representation of the sequential
input and new state tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>inputs</strong> (<em>PackedSequence</em>) – Input tensor of shape
(bsize x seq_len x input_dim).</p></li>
<li><p><strong>states</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Tuple of tensors containing
the initial hidden state and the cell state of each element in
the batch. Each of these tensors have a dimension of
(1 x bsize x num_directions * nhid). Defaults to <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>AgumentedLSTM representation of input and the
state of the LSTM <cite>t = seq_len</cite>.
Shape of representation is (bsize x seq_len x representation_dim).
Shape of each state is (1 x bsize x nhid).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[PackedSequence, Tuple[torch.Tensor, torch.Tensor]]</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.augmented_lstm.AugmentedLSTMUnidirectional.get_dropout_mask">
<code class="sig-name descname">get_dropout_mask</code><span class="sig-paren">(</span><em class="sig-param">dropout_probability: float</em>, <em class="sig-param">tensor_for_masking: torch.Tensor</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/augmented_lstm.html#AugmentedLSTMUnidirectional.get_dropout_mask"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.augmented_lstm.AugmentedLSTMUnidirectional.get_dropout_mask" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.bilstm">
<span id="pytext-models-representations-bilstm-module"></span><h2>pytext.models.representations.bilstm module<a class="headerlink" href="#module-pytext.models.representations.bilstm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.bilstm.BiLSTM">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.bilstm.</code><code class="sig-name descname">BiLSTM</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.bilstm.BiLSTM.Config</em>, <em class="sig-param">embed_dim: int</em>, <em class="sig-param">padding_value: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/bilstm.html#BiLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.bilstm.BiLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p><cite>BiLSTM</cite> implements a multi-layer bidirectional LSTM representation layer
preceded by a dropout layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Config</em>) – Configuration object of type BiLSTM.Config.</p></li>
<li><p><strong>embed_dim</strong> (<em>int</em>) – The number of expected features in the input.</p></li>
<li><p><strong>padding_value</strong> (<em>float</em>) – Value for the padded elements. Defaults to 0.0.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm.BiLSTM.padding_value">
<code class="sig-name descname">padding_value</code><a class="headerlink" href="#pytext.models.representations.bilstm.BiLSTM.padding_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Value for the padded elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm.BiLSTM.dropout">
<code class="sig-name descname">dropout</code><a class="headerlink" href="#pytext.models.representations.bilstm.BiLSTM.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Dropout layer preceding the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm.BiLSTM.lstm">
<code class="sig-name descname">lstm</code><a class="headerlink" href="#pytext.models.representations.bilstm.BiLSTM.lstm" title="Permalink to this definition">¶</a></dt>
<dd><p>LSTM layer that operates on the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.LSTM</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm.BiLSTM.representation_dim">
<code class="sig-name descname">representation_dim</code><a class="headerlink" href="#pytext.models.representations.bilstm.BiLSTM.representation_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>The calculated dimension of the output features
of BiLSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.bilstm.BiLSTM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em>, <em class="sig-param">states: Optional[Tuple[torch.Tensor</em>, <em class="sig-param">torch.Tensor]] = None</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]<a class="reference internal" href="../_modules/pytext/models/representations/bilstm.html#BiLSTM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.bilstm.BiLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an input batch of sequential data such as word embeddings, produces
a bidirectional LSTM representation of the sequential input and new state
tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedded_tokens</strong> (<em>torch.Tensor</em>) – Input tensor of shape
(bsize x seq_len x input_dim).</p></li>
<li><p><strong>seq_lengths</strong> (<em>torch.Tensor</em>) – List of sequences lengths of each batch element.</p></li>
<li><p><strong>states</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Tuple of tensors containing
the initial hidden state and the cell state of each element in
the batch. Each of these tensors have a dimension of
(bsize x num_layers * num_directions x nhid). Defaults to <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Bidirectional</dt><dd><p>LSTM representation of input and the state of the LSTM <cite>t = seq_len</cite>.
Shape of representation is (bsize x seq_len x representation_dim).
Shape of each state is (bsize x num_layers * num_directions x nhid).</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.bilstm_doc_attention">
<span id="pytext-models-representations-bilstm-doc-attention-module"></span><h2>pytext.models.representations.bilstm_doc_attention module<a class="headerlink" href="#module-pytext.models.representations.bilstm_doc_attention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.bilstm_doc_attention.</code><code class="sig-name descname">BiLSTMDocAttention</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/bilstm_doc_attention.html#BiLSTMDocAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p><cite>BiLSTMDocAttention</cite> implements a multi-layer bidirectional LSTM based
representation for documents with or without pooling. The pooling can be
max pooling, mean pooling or self attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Config</em>) – Configuration object of type BiLSTMDocAttention.Config.</p></li>
<li><p><strong>embed_dim</strong> (<em>int</em>) – The number of expected features in the input.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.dropout">
<code class="sig-name descname">dropout</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Dropout layer preceding the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.lstm">
<code class="sig-name descname">lstm</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.lstm" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.attention">
<code class="sig-name descname">attention</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements the attention or pooling.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.dense">
<code class="sig-name descname">dense</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.dense" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements the non-linear projection over
attended representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.representation_dim">
<code class="sig-name descname">representation_dim</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.representation_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>The calculated dimension of the output features
of the <cite>BiLSTMDocAttention</cite> representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em>, <em class="sig-param">*args</em>, <em class="sig-param">states: Tuple[torch.Tensor</em>, <em class="sig-param">torch.Tensor] = None</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]<a class="reference internal" href="../_modules/pytext/models/representations/bilstm_doc_attention.html#BiLSTMDocAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an input batch of sequential data such as word embeddings, produces
a bidirectional LSTM representation with or without pooling of the
sequential input and new state tensors.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedded_tokens</strong> (<em>torch.Tensor</em>) – Input tensor of shape
(bsize x seq_len x input_dim).</p></li>
<li><p><strong>seq_lengths</strong> (<em>torch.Tensor</em>) – List of sequences lengths of each batch element.</p></li>
<li><p><strong>states</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Tuple of tensors containing
the initial hidden state and the cell state of each element in
the batch. Each of these tensors have a dimension of
(bsize x num_layers * num_directions x nhid). Defaults to <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Bidirectional</dt><dd><p>LSTM representation of input and the state of the LSTM at <cite>t = seq_len</cite>.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.bilstm_doc_slot_attention">
<span id="pytext-models-representations-bilstm-doc-slot-attention-module"></span><h2>pytext.models.representations.bilstm_doc_slot_attention module<a class="headerlink" href="#module-pytext.models.representations.bilstm_doc_slot_attention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.bilstm_doc_slot_attention.</code><code class="sig-name descname">BiLSTMDocSlotAttention</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/bilstm_doc_slot_attention.html#BiLSTMDocSlotAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p><cite>BiLSTMDocSlotAttention</cite> implements a multi-layer bidirectional LSTM based
representation with support for various attention mechanisms.</p>
<p>In default mode, when attention configuration is not provided, it behaves
like a multi-layer LSTM encoder and returns the output features from the
last layer of the LSTM, for each t. When document_attention configuration is
provided, it produces a fixed-sized document representation. When
slot_attention configuration is provide, it attends on output of each cell
of LSTM module to produce a fixed sized word representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Config</em>) – Configuration object of type
BiLSTMDocSlotAttention.Config.</p></li>
<li><p><strong>embed_dim</strong> (<em>int</em>) – The number of expected features in the input.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.dropout">
<code class="sig-name descname">dropout</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Dropout layer preceding the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.relu">
<code class="sig-name descname">relu</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.relu" title="Permalink to this definition">¶</a></dt>
<dd><p>An instance of the ReLU layer.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.ReLU</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.lstm">
<code class="sig-name descname">lstm</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.lstm" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.use_doc_attention">
<code class="sig-name descname">use_doc_attention</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.use_doc_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>True</cite>, indicates using document attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.doc_attention">
<code class="sig-name descname">doc_attention</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.doc_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements document attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.self.projection_d">
<code class="sig-prename descclassname">self.</code><code class="sig-name descname">projection_d</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.self.projection_d" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequence of dense layers for
projection over document representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Sequential</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.use_word_attention">
<code class="sig-name descname">use_word_attention</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.use_word_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>If <cite>True</cite>, indicates using word attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>bool</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.word_attention">
<code class="sig-name descname">word_attention</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.word_attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements word attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.self.projection_w">
<code class="sig-prename descclassname">self.</code><code class="sig-name descname">projection_w</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.self.projection_w" title="Permalink to this definition">¶</a></dt>
<dd><p>A sequence of dense layers for
projection over word representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Sequential</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.representation_dim">
<code class="sig-name descname">representation_dim</code><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.representation_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>The calculated dimension of the output features
of the <cite>BiLSTMDocAttention</cite> representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em>, <em class="sig-param">*args</em>, <em class="sig-param">states: torch.Tensor = None</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]<a class="reference internal" href="../_modules/pytext/models/representations/bilstm_doc_slot_attention.html#BiLSTMDocSlotAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.bilstm_doc_slot_attention.BiLSTMDocSlotAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an input batch of sequential data such as word embeddings, produces
a bidirectional LSTM representation the appropriate attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedded_tokens</strong> (<em>torch.Tensor</em>) – Input tensor of shape
(bsize x seq_len x input_dim).</p></li>
<li><p><strong>seq_lengths</strong> (<em>torch.Tensor</em>) – List of sequences lengths of each batch
element.</p></li>
<li><p><strong>states</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Tuple of tensors
containing the initial hidden state and the cell state of each
element in the batch. Each of these tensors have a dimension of
(bsize x num_layers * num_directions x nhid). Defaults to <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensors containing the document and the word representation of
the input.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Tuple[torch.Tensor, torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.bilstm_slot_attn">
<span id="pytext-models-representations-bilstm-slot-attn-module"></span><h2>pytext.models.representations.bilstm_slot_attn module<a class="headerlink" href="#module-pytext.models.representations.bilstm_slot_attn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.bilstm_slot_attn.</code><code class="sig-name descname">BiLSTMSlotAttention</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/bilstm_slot_attn.html#BiLSTMSlotAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p><cite>BiLSTMSlotAttention</cite> implements a multi-layer bidirectional LSTM based
representation with attention over slots.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Config</em>) – Configuration object of type BiLSTMSlotAttention.Config.</p></li>
<li><p><strong>embed_dim</strong> (<em>int</em>) – The number of expected features in the input.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.dropout">
<code class="sig-name descname">dropout</code><a class="headerlink" href="#pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Dropout layer preceding the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.lstm">
<code class="sig-name descname">lstm</code><a class="headerlink" href="#pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.lstm" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.attention">
<code class="sig-name descname">attention</code><a class="headerlink" href="#pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.attention" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements the attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.dense">
<code class="sig-name descname">dense</code><a class="headerlink" href="#pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.dense" title="Permalink to this definition">¶</a></dt>
<dd><p>Module that implements the non-linear projection over
attended representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Module</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.representation_dim">
<code class="sig-name descname">representation_dim</code><a class="headerlink" href="#pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.representation_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>The calculated dimension of the output features
of the <cite>SlotAttention</cite> representation.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em>, <em class="sig-param">*args</em>, <em class="sig-param">states: torch.Tensor = None</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/bilstm_slot_attn.html#BiLSTMSlotAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Given an input batch of sequential data such as word embeddings, produces
a bidirectional LSTM representation with or without Slot attention.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedded_tokens</strong> (<em>torch.Tensor</em>) – Input tensor of shape
(bsize x seq_len x input_dim).</p></li>
<li><p><strong>seq_lengths</strong> (<em>torch.Tensor</em>) – List of sequences lengths of each batch
element.</p></li>
<li><p><strong>states</strong> (<em>Tuple</em><em>[</em><em>torch.Tensor</em><em>, </em><em>torch.Tensor</em><em>]</em>) – Tuple of tensors containing
the initial hidden state and the cell state of each element in
the batch. Each of these tensors have a dimension of
(bsize x num_layers * num_directions x nhid). Defaults to <cite>None</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Bidirectional LSTM representation of input with or</dt><dd><p>without slot attention.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>torch.Tensor</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.biseqcnn">
<span id="pytext-models-representations-biseqcnn-module"></span><h2>pytext.models.representations.biseqcnn module<a class="headerlink" href="#module-pytext.models.representations.biseqcnn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.biseqcnn.BSeqCNNRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.biseqcnn.</code><code class="sig-name descname">BSeqCNNRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.biseqcnn.BSeqCNNRepresentation.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/biseqcnn.html#BSeqCNNRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.biseqcnn.BSeqCNNRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p>This class is an implementation of the paper <a class="reference external" href="https://arxiv.org/pdf/1606.07783">https://arxiv.org/pdf/1606.07783</a>.
It is a bidirectional CNN model that captures context like RNNs do.</p>
<p>The module expects that input mini-batch is already padded.</p>
<p>TODO: Current implementation has a single layer conv-maxpool operation.</p>
<dl class="method">
<dt id="pytext.models.representations.biseqcnn.BSeqCNNRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/biseqcnn.html#BSeqCNNRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.biseqcnn.BSeqCNNRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.biseqcnn.ContextualWordConvolution">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.biseqcnn.</code><code class="sig-name descname">ContextualWordConvolution</code><span class="sig-paren">(</span><em class="sig-param">in_channels: int, out_channels: int, kernel_sizes: List[int]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/biseqcnn.html#ContextualWordConvolution"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.biseqcnn.ContextualWordConvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytext.models.representations.biseqcnn.ContextualWordConvolution.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">words: torch.Tensor</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/biseqcnn.html#ContextualWordConvolution.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.biseqcnn.ContextualWordConvolution.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.contextual_intent_slot_rep">
<span id="pytext-models-representations-contextual-intent-slot-rep-module"></span><h2>pytext.models.representations.contextual_intent_slot_rep module<a class="headerlink" href="#module-pytext.models.representations.contextual_intent_slot_rep" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.contextual_intent_slot_rep.ContextualIntentSlotRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.contextual_intent_slot_rep.</code><code class="sig-name descname">ContextualIntentSlotRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.contextual_intent_slot_rep.ContextualIntentSlotRepresentation.Config, embed_dim: Tuple[int, ...]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/contextual_intent_slot_rep.html#ContextualIntentSlotRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.contextual_intent_slot_rep.ContextualIntentSlotRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p>Representation for a contextual intent slot model</p>
<p>The inputs are two embeddings: word level embedding containing dictionary features,
sequence (contexts) level embedding. See following diagram for the representation
implementation that combines the two embeddings. Seq_representation is concatenated
with word_embeddings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">+-----------+</span>
<span class="o">|</span> <span class="n">word_embed</span><span class="o">|---------------------------&gt;+</span>   <span class="o">+--------------------+</span>
<span class="o">+-----------+</span>                            <span class="o">|</span>   <span class="o">|</span> <span class="n">doc_representation</span> <span class="o">|</span>
<span class="o">+-----------+</span>   <span class="o">+-------------------+</span>    <span class="o">|--&gt;+--------------------+</span>
<span class="o">|</span> <span class="n">seq_embed</span> <span class="o">|--&gt;|</span> <span class="n">seq_representation</span><span class="o">|---&gt;+</span>   <span class="o">|</span> <span class="n">word_representation</span><span class="o">|</span>
<span class="o">+-----------+</span>   <span class="o">+-------------------+</span>        <span class="o">+--------------------+</span>
                                              <span class="n">joint_representation</span>
</pre></div>
</div>
<dl class="method">
<dt id="pytext.models.representations.contextual_intent_slot_rep.ContextualIntentSlotRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">word_seq_embed: Tuple[torch.Tensor, torch.Tensor], word_lengths: torch.Tensor, seq_lengths: torch.Tensor, *args</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/representations/contextual_intent_slot_rep.html#ContextualIntentSlotRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.contextual_intent_slot_rep.ContextualIntentSlotRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.deepcnn">
<span id="pytext-models-representations-deepcnn-module"></span><h2>pytext.models.representations.deepcnn module<a class="headerlink" href="#module-pytext.models.representations.deepcnn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.deepcnn.DeepCNNRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.deepcnn.</code><code class="sig-name descname">DeepCNNRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.deepcnn.DeepCNNRepresentation.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/deepcnn.html#DeepCNNRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.deepcnn.DeepCNNRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p><cite>DeepCNNRepresentation</cite> implements CNN representation layer
preceded by a dropout layer. CNN representation layer is based on the encoder
in the architecture proposed by Gehring et. al. in Convolutional Sequence to
Sequence Learning.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Config</em>) – Configuration object of type DeepCNNRepresentation.Config.</p></li>
<li><p><strong>embed_dim</strong> (<em>int</em>) – The number of expected features in the input.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pytext.models.representations.deepcnn.DeepCNNRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/deepcnn.html#DeepCNNRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.deepcnn.DeepCNNRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.deepcnn.SeparableConv1d">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.deepcnn.</code><code class="sig-name descname">SeparableConv1d</code><span class="sig-paren">(</span><em class="sig-param">input_channels: int</em>, <em class="sig-param">output_channels: int</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">padding: int</em>, <em class="sig-param">dilation: int</em>, <em class="sig-param">bottleneck: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/deepcnn.html#SeparableConv1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.deepcnn.SeparableConv1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Implements a 1d depthwise separable convolutional layer. In regular convolutional
layers, the input channels are mixed with each other to produce each output channel.
Depthwise separable convolutions decompose this process into two smaller
convolutions – a depthwise and pointwise convolution.</p>
<p>The depthwise convolution spatially convolves each input channel separately,
then the pointwise convolution projects this result into a new channel space.
This process reduces the number of FLOPS used to compute a convolution and also
exhibits a regularization effect. The general behavior – including the input
parameters – is equivalent to <cite>nn.Conv1d</cite>.</p>
<p><cite>bottleneck</cite> controls the behavior of the pointwise convolution. Instead of
upsampling directly, we split the pointwise convolution into two pieces: the first
convolution downsamples into a (sufficiently small) low dimension and the
second convolution upsamples into the target (higher) dimension. Creating this
bottleneck significantly cuts the number of parameters with minimal loss
in performance.</p>
<dl class="method">
<dt id="pytext.models.representations.deepcnn.SeparableConv1d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/deepcnn.html#SeparableConv1d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.deepcnn.SeparableConv1d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.deepcnn.Trim1d">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.deepcnn.</code><code class="sig-name descname">Trim1d</code><span class="sig-paren">(</span><em class="sig-param">trim</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/deepcnn.html#Trim1d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.deepcnn.Trim1d" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Trims a 1d convolutional output. Used to implement history-padding
by removing excess padding from the right.</p>
<dl class="method">
<dt id="pytext.models.representations.deepcnn.Trim1d.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">x</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/deepcnn.html#Trim1d.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.deepcnn.Trim1d.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="function">
<dt id="pytext.models.representations.deepcnn.create_conv_package">
<code class="sig-prename descclassname">pytext.models.representations.deepcnn.</code><code class="sig-name descname">create_conv_package</code><span class="sig-paren">(</span><em class="sig-param">index: int</em>, <em class="sig-param">activation: pytext.config.module_config.Activation</em>, <em class="sig-param">in_channels: int</em>, <em class="sig-param">out_channels: int</em>, <em class="sig-param">kernel_size: int</em>, <em class="sig-param">causal: bool</em>, <em class="sig-param">dilated: bool</em>, <em class="sig-param">separable: bool</em>, <em class="sig-param">bottleneck: int</em>, <em class="sig-param">weight_norm: bool</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/deepcnn.html#create_conv_package"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.deepcnn.create_conv_package" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a convolutional layer with the specified arguments.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<em>int</em>) – Index of a convolutional layer in the stack.</p></li>
<li><p><strong>activation</strong> (<em>Activation</em>) – Activation function.</p></li>
<li><p><strong>in_channels</strong> (<em>int</em>) – Number of input channels.</p></li>
<li><p><strong>out_channels</strong> (<em>int</em>) – Number of output channels.</p></li>
<li><p><strong>kernel_size</strong> (<em>int</em>) – Size of 1d convolutional filter.</p></li>
<li><p><strong>causal</strong> (<em>bool</em>) – Whether the convolution is causal or not. If set, it</p></li>
<li><p><strong>for the temporal ordering of the inputs.</strong> (<em>accounts</em>) – </p></li>
<li><p><strong>dilated</strong> (<em>bool</em>) – Whether the convolution is dilated or not. If set,</p></li>
<li><p><strong>receptive field of the convolutional stack grows exponentially.</strong> (<em>the</em>) – </p></li>
<li><p><strong>separable</strong> (<em>bool</em>) – Whether to use depthwise separable convolutions</p></li>
<li><p><strong>not -- see SeparableConv1d.</strong> (<em>or</em>) – </p></li>
<li><p><strong>bottleneck</strong> (<em>int</em>) – Bottleneck channel dimension for depthwise separable</p></li>
<li><p><strong>See SeparableConv1d for an in-depth explanation.</strong> (<em>convolutions.</em>) – </p></li>
<li><p><strong>weight_norm</strong> (<em>bool</em>) – Whether to add weight normalization to the</p></li>
<li><p><strong>convolutions</strong><strong> or </strong><strong>not.</strong> (<em>regular</em>) – </p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.models.representations.deepcnn.pool">
<code class="sig-prename descclassname">pytext.models.representations.deepcnn.</code><code class="sig-name descname">pool</code><span class="sig-paren">(</span><em class="sig-param">pooling_type</em>, <em class="sig-param">words</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/deepcnn.html#pool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.deepcnn.pool" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.docnn">
<span id="pytext-models-representations-docnn-module"></span><h2>pytext.models.representations.docnn module<a class="headerlink" href="#module-pytext.models.representations.docnn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.docnn.DocNNRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.docnn.</code><code class="sig-name descname">DocNNRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.docnn.DocNNRepresentation.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/docnn.html#DocNNRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.docnn.DocNNRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p>CNN based representation of a document.</p>
<dl class="method">
<dt id="pytext.models.representations.docnn.DocNNRepresentation.conv_and_pool">
<code class="sig-name descname">conv_and_pool</code><span class="sig-paren">(</span><em class="sig-param">x</em>, <em class="sig-param">conv</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/docnn.html#DocNNRepresentation.conv_and_pool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.docnn.DocNNRepresentation.conv_and_pool" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.representations.docnn.DocNNRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/docnn.html#DocNNRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.docnn.DocNNRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.huggingface_bert_sentence_encoder">
<span id="pytext-models-representations-huggingface-bert-sentence-encoder-module"></span><h2>pytext.models.representations.huggingface_bert_sentence_encoder module<a class="headerlink" href="#module-pytext.models.representations.huggingface_bert_sentence_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.huggingface_bert_sentence_encoder.HuggingFaceBertSentenceEncoder">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.huggingface_bert_sentence_encoder.</code><code class="sig-name descname">HuggingFaceBertSentenceEncoder</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.huggingface_bert_sentence_encoder.HuggingFaceBertSentenceEncoder.Config</em>, <em class="sig-param">output_encoded_layers: bool</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/huggingface_bert_sentence_encoder.html#HuggingFaceBertSentenceEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.huggingface_bert_sentence_encoder.HuggingFaceBertSentenceEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase" title="pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase</span></code></a></p>
<p>Generate sentence representation using the open source HuggingFace BERT
model. This class implements loading the model weights from a
pre-trained model file.</p>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.jointcnn_rep">
<span id="pytext-models-representations-jointcnn-rep-module"></span><h2>pytext.models.representations.jointcnn_rep module<a class="headerlink" href="#module-pytext.models.representations.jointcnn_rep" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.jointcnn_rep.JointCNNRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.jointcnn_rep.</code><code class="sig-name descname">JointCNNRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.jointcnn_rep.JointCNNRepresentation.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/jointcnn_rep.html#JointCNNRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.jointcnn_rep.JointCNNRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.jointcnn_rep.JointCNNRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/representations/jointcnn_rep.html#JointCNNRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.jointcnn_rep.JointCNNRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.jointcnn_rep.SharedCNNRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.jointcnn_rep.</code><code class="sig-name descname">SharedCNNRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.jointcnn_rep.SharedCNNRepresentation.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/jointcnn_rep.html#SharedCNNRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.jointcnn_rep.SharedCNNRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.jointcnn_rep.SharedCNNRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/representations/jointcnn_rep.html#SharedCNNRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.jointcnn_rep.SharedCNNRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.ordered_neuron_lstm">
<span id="pytext-models-representations-ordered-neuron-lstm-module"></span><h2>pytext.models.representations.ordered_neuron_lstm module<a class="headerlink" href="#module-pytext.models.representations.ordered_neuron_lstm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTM">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.ordered_neuron_lstm.</code><code class="sig-name descname">OrderedNeuronLSTM</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTM.Config</em>, <em class="sig-param">embed_dim: int</em>, <em class="sig-param">padding_value: Optional[float] = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/ordered_neuron_lstm.html#OrderedNeuronLSTM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTM" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTM.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">rep: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em>, <em class="sig-param">states: Optional[Tuple[torch.Tensor</em>, <em class="sig-param">torch.Tensor]] = None</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]<a class="reference internal" href="../_modules/pytext/models/representations/ordered_neuron_lstm.html#OrderedNeuronLSTM.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTM.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTMLayer">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.ordered_neuron_lstm.</code><code class="sig-name descname">OrderedNeuronLSTMLayer</code><span class="sig-paren">(</span><em class="sig-param">embed_dim: int</em>, <em class="sig-param">lstm_dim: int</em>, <em class="sig-param">padding_value: float</em>, <em class="sig-param">dropout: float</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/ordered_neuron_lstm.html#OrderedNeuronLSTMLayer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTMLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTMLayer.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor, states: Tuple[torch.Tensor, torch.Tensor], seq_lengths: List[int]</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]<a class="reference internal" href="../_modules/pytext/models/representations/ordered_neuron_lstm.html#OrderedNeuronLSTMLayer.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.ordered_neuron_lstm.OrderedNeuronLSTMLayer.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.pair_rep">
<span id="pytext-models-representations-pair-rep-module"></span><h2>pytext.models.representations.pair_rep module<a class="headerlink" href="#module-pytext.models.representations.pair_rep" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.pair_rep.PairRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pair_rep.</code><code class="sig-name descname">PairRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.pair_rep.PairRepresentation.Config, embed_dim: Tuple[int, ...]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pair_rep.html#PairRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pair_rep.PairRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p>Wrapper representation for a pair of inputs.</p>
<p>Takes a tuple of inputs: the left sentence, and the right sentence(s). Returns
a representation of the pair of sentences, either as a concatenation of the two
sentence embeddings or as a “siamese” representation which also includes their
difference and elementwise product (arXiv:1705.02364).
If more than two inputs are provided, the extra inputs are assumed to be extra
“right” sentences, and the output will be the stacked pair representations
of the left sentence together with all right sentences. This is more efficient
than separately computing all these pair representations, because the left
sentence will not need to be re-embedded multiple times.</p>
<dl class="method">
<dt id="pytext.models.representations.pair_rep.PairRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embeddings: Tuple[torch.Tensor, ...], *lengths: torch.Tensor</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/pair_rep.html#PairRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pair_rep.PairRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes the pair representations.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embeddings</strong> – token embeddings of the left sentence, followed by the
token embeddings of the right sentence(s).</p></li>
<li><p><strong>lengths</strong> – the corresponding sequence lengths.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tensor of shape <cite>(num_right_inputs, batch_size, rep_size)</cite>, with
the first dimension squeezed if one.</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.pass_through">
<span id="pytext-models-representations-pass-through-module"></span><h2>pytext.models.representations.pass_through module<a class="headerlink" href="#module-pytext.models.representations.pass_through" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.pass_through.PassThroughRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pass_through.</code><code class="sig-name descname">PassThroughRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.config.component.ComponentMeta.__new__.&lt;locals&gt;.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pass_through.html#PassThroughRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pass_through.PassThroughRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.pass_through.PassThroughRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/pass_through.html#PassThroughRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pass_through.PassThroughRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.pooling">
<span id="pytext-models-representations-pooling-module"></span><h2>pytext.models.representations.pooling module<a class="headerlink" href="#module-pytext.models.representations.pooling" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.pooling.BoundaryPool">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pooling.</code><code class="sig-name descname">BoundaryPool</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.pooling.BoundaryPool.Config</em>, <em class="sig-param">n_input: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#BoundaryPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.BoundaryPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.pooling.BoundaryPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor = None</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#BoundaryPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.BoundaryPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.pooling.LastTimestepPool">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pooling.</code><code class="sig-name descname">LastTimestepPool</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.config.module_config.ModuleConfig</em>, <em class="sig-param">n_input: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#LastTimestepPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.LastTimestepPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.pooling.LastTimestepPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#LastTimestepPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.LastTimestepPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.pooling.MaxPool">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pooling.</code><code class="sig-name descname">MaxPool</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.config.module_config.ModuleConfig</em>, <em class="sig-param">n_input: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#MaxPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.MaxPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.pooling.MaxPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor = None</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#MaxPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.MaxPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.pooling.MeanPool">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pooling.</code><code class="sig-name descname">MeanPool</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.config.module_config.ModuleConfig</em>, <em class="sig-param">n_input: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#MeanPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.MeanPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.pooling.MeanPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#MeanPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.MeanPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.pooling.NoPool">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pooling.</code><code class="sig-name descname">NoPool</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.config.module_config.ModuleConfig</em>, <em class="sig-param">n_input: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#NoPool"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.NoPool" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.pooling.NoPool.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor = None</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#NoPool.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.NoPool.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.pooling.SelfAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pooling.</code><code class="sig-name descname">SelfAttention</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.pooling.SelfAttention.Config</em>, <em class="sig-param">n_input: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#SelfAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.SelfAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.pooling.SelfAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor = None</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#SelfAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.SelfAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.pooling.SelfAttention.init_weights">
<code class="sig-name descname">init_weights</code><span class="sig-paren">(</span><em class="sig-param">init_range: float = 0.1</em><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/models/representations/pooling.html#SelfAttention.init_weights"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pooling.SelfAttention.init_weights" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.pure_doc_attention">
<span id="pytext-models-representations-pure-doc-attention-module"></span><h2>pytext.models.representations.pure_doc_attention module<a class="headerlink" href="#module-pytext.models.representations.pure_doc_attention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.pure_doc_attention.PureDocAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.pure_doc_attention.</code><code class="sig-name descname">PureDocAttention</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.pure_doc_attention.PureDocAttention.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/pure_doc_attention.html#PureDocAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pure_doc_attention.PureDocAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p>pooling (e.g. max pooling or self attention)
followed by optional MLP</p>
<dl class="method">
<dt id="pytext.models.representations.pure_doc_attention.PureDocAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_tokens: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor = None</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span> → Any<a class="reference internal" href="../_modules/pytext/models/representations/pure_doc_attention.html#PureDocAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.pure_doc_attention.PureDocAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.representation_base">
<span id="pytext-models-representations-representation-base-module"></span><h2>pytext.models.representations.representation_base module<a class="headerlink" href="#module-pytext.models.representations.representation_base" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.representation_base.RepresentationBase">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.representation_base.</code><code class="sig-name descname">RepresentationBase</code><span class="sig-paren">(</span><em class="sig-param">config</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/representation_base.html#RepresentationBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.representation_base.RepresentationBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.representation_base.RepresentationBase.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/representation_base.html#RepresentationBase.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.representation_base.RepresentationBase.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.representation_base.RepresentationBase.get_representation_dim">
<code class="sig-name descname">get_representation_dim</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/representation_base.html#RepresentationBase.get_representation_dim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.representation_base.RepresentationBase.get_representation_dim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.seq_rep">
<span id="pytext-models-representations-seq-rep-module"></span><h2>pytext.models.representations.seq_rep module<a class="headerlink" href="#module-pytext.models.representations.seq_rep" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.seq_rep.SeqRepresentation">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.seq_rep.</code><code class="sig-name descname">SeqRepresentation</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.seq_rep.SeqRepresentation.Config</em>, <em class="sig-param">embed_dim: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/seq_rep.html#SeqRepresentation"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.seq_rep.SeqRepresentation" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p>Representation for a sequence of sentences
Each sentence will be embedded with a DocNN model,
then all the sentences are embedded with another DocNN/BiLSTM model</p>
<dl class="method">
<dt id="pytext.models.representations.seq_rep.SeqRepresentation.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">embedded_seqs: torch.Tensor</em>, <em class="sig-param">seq_lengths: torch.Tensor</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/seq_rep.html#SeqRepresentation.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.seq_rep.SeqRepresentation.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.slot_attention">
<span id="pytext-models-representations-slot-attention-module"></span><h2>pytext.models.representations.slot_attention module<a class="headerlink" href="#module-pytext.models.representations.slot_attention" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.slot_attention.SlotAttention">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.slot_attention.</code><code class="sig-name descname">SlotAttention</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.slot_attention.SlotAttention.Config</em>, <em class="sig-param">n_input: int</em>, <em class="sig-param">batch_first: bool = True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/slot_attention.html#SlotAttention"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.slot_attention.SlotAttention" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<dl class="method">
<dt id="pytext.models.representations.slot_attention.SlotAttention.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">inputs: torch.Tensor</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/representations/slot_attention.html#SlotAttention.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.slot_attention.SlotAttention.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.sparse_transformer_sentence_encoder">
<span id="pytext-models-representations-sparse-transformer-sentence-encoder-module"></span><h2>pytext.models.representations.sparse_transformer_sentence_encoder module<a class="headerlink" href="#module-pytext.models.representations.sparse_transformer_sentence_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.sparse_transformer_sentence_encoder.SparseTransformerSentenceEncoder">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.sparse_transformer_sentence_encoder.</code><code class="sig-name descname">SparseTransformerSentenceEncoder</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.sparse_transformer_sentence_encoder.SparseTransformerSentenceEncoder.Config</em>, <em class="sig-param">output_encoded_layers: bool</em>, <em class="sig-param">padding_idx: int</em>, <em class="sig-param">vocab_size: int</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwarg</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/sparse_transformer_sentence_encoder.html#SparseTransformerSentenceEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.sparse_transformer_sentence_encoder.SparseTransformerSentenceEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder" title="pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder</span></code></a></p>
<p>Implementation of the Transformer Sentence Encoder. This directly makes
use of the TransformerSentenceEncoder module in Fairseq.</p>
<dl class="simple">
<dt>A few interesting config options:</dt><dd><ul class="simple">
<li><p>encoder_normalize_before detemines whether the layer norm is applied
before or after self_attention. This is similar to original
implementation from Google.</p></li>
<li><p>activation_fn can be set to ‘gelu’ instead of the default of ‘relu’.</p></li>
<li><p>project_representation adds a linear projection + tanh to the pooled output
in the style of BERT.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.stacked_bidirectional_rnn">
<span id="pytext-models-representations-stacked-bidirectional-rnn-module"></span><h2>pytext.models.representations.stacked_bidirectional_rnn module<a class="headerlink" href="#module-pytext.models.representations.stacked_bidirectional_rnn" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.RnnType">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.stacked_bidirectional_rnn.</code><code class="sig-name descname">RnnType</code><a class="reference internal" href="../_modules/pytext/models/representations/stacked_bidirectional_rnn.html#RnnType"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.RnnType" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="attribute">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.RnnType.GRU">
<code class="sig-name descname">GRU</code><em class="property"> = 'gru'</em><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.RnnType.GRU" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.RnnType.LSTM">
<code class="sig-name descname">LSTM</code><em class="property"> = 'lstm'</em><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.RnnType.LSTM" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.RnnType.RNN">
<code class="sig-name descname">RNN</code><em class="property"> = 'rnn'</em><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.RnnType.RNN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.stacked_bidirectional_rnn.</code><code class="sig-name descname">StackedBidirectionalRNN</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.Config</em>, <em class="sig-param">input_size: int</em>, <em class="sig-param">padding_value: float = 0.0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/stacked_bidirectional_rnn.html#StackedBidirectionalRNN"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.html#pytext.models.module.Module" title="pytext.models.module.Module"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.module.Module</span></code></a></p>
<p><cite>StackedBidirectionalRNN</cite> implements a multi-layer bidirectional RNN with an
option to return outputs from all the layers of RNN.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>config</strong> (<em>Config</em>) – Configuration object of type BiLSTM.Config.</p></li>
<li><p><strong>embed_dim</strong> (<em>int</em>) – The number of expected features in the input.</p></li>
<li><p><strong>padding_value</strong> (<em>float</em>) – Value for the padded elements. Defaults to 0.0.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.padding_value">
<code class="sig-name descname">padding_value</code><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.padding_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Value for the padded elements.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.dropout">
<code class="sig-name descname">dropout</code><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Dropout layer preceding the LSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.Dropout</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.lstm">
<code class="sig-name descname">lstm</code><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.lstm" title="Permalink to this definition">¶</a></dt>
<dd><p>LSTM layer that operates on the inputs.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>nn.LSTM</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.representation_dim">
<code class="sig-name descname">representation_dim</code><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.representation_dim" title="Permalink to this definition">¶</a></dt>
<dd><p>The calculated dimension of the output features
of BiLSTM.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">tokens</em>, <em class="sig-param">tokens_mask</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/stacked_bidirectional_rnn.html#StackedBidirectionalRNN.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.stacked_bidirectional_rnn.StackedBidirectionalRNN.forward" title="Permalink to this definition">¶</a></dt>
<dd><dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>tokens</strong> – batch, max_seq_len, hidden_size</p></li>
<li><p><strong>tokens_mask</strong> – batch, max_seq_len (1 for padding, 0 for true)</p></li>
</ul>
</dd>
</dl>
<dl class="simple">
<dt>Output:</dt><dd><dl class="simple">
<dt>tokens_encoded: batch, max_seq_len, hidden_size * num_layers if</dt><dd><p>concat_layers = True else batch, max_seq_len, hidden_size</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.traced_transformer_encoder">
<span id="pytext-models-representations-traced-transformer-encoder-module"></span><h2>pytext.models.representations.traced_transformer_encoder module<a class="headerlink" href="#module-pytext.models.representations.traced_transformer_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.traced_transformer_encoder.TraceableTransformerWrapper">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.traced_transformer_encoder.</code><code class="sig-name descname">TraceableTransformerWrapper</code><span class="sig-paren">(</span><em class="sig-param">eager_encoder: fairseq.modules.transformer_sentence_encoder.TransformerSentenceEncoder</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/traced_transformer_encoder.html#TraceableTransformerWrapper"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.traced_transformer_encoder.TraceableTransformerWrapper" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytext.models.representations.traced_transformer_encoder.TraceableTransformerWrapper.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">tokens: torch.Tensor</em>, <em class="sig-param">segment_labels: torch.Tensor = None</em>, <em class="sig-param">positions: torch.Tensor = None</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/representations/traced_transformer_encoder.html#TraceableTransformerWrapper.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.traced_transformer_encoder.TraceableTransformerWrapper.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.traced_transformer_encoder.TracedTransformerEncoder">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.traced_transformer_encoder.</code><code class="sig-name descname">TracedTransformerEncoder</code><span class="sig-paren">(</span><em class="sig-param">eager_encoder: fairseq.modules.transformer_sentence_encoder.TransformerSentenceEncoder</em>, <em class="sig-param">tokens: torch.Tensor</em>, <em class="sig-param">segment_labels: torch.Tensor = None</em>, <em class="sig-param">positions: torch.Tensor = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/traced_transformer_encoder.html#TracedTransformerEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.traced_transformer_encoder.TracedTransformerEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<dl class="method">
<dt id="pytext.models.representations.traced_transformer_encoder.TracedTransformerEncoder.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">tokens: torch.Tensor</em>, <em class="sig-param">segment_labels: torch.Tensor = None</em>, <em class="sig-param">positions: torch.Tensor = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/traced_transformer_encoder.html#TracedTransformerEncoder.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.traced_transformer_encoder.TracedTransformerEncoder.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.transformer_sentence_encoder">
<span id="pytext-models-representations-transformer-sentence-encoder-module"></span><h2>pytext.models.representations.transformer_sentence_encoder module<a class="headerlink" href="#module-pytext.models.representations.transformer_sentence_encoder" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.transformer_sentence_encoder.</code><code class="sig-name descname">TransformerSentenceEncoder</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.Config</em>, <em class="sig-param">output_encoded_layers: bool</em>, <em class="sig-param">padding_idx: int</em>, <em class="sig-param">vocab_size: int</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwarg</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/transformer_sentence_encoder.html#TransformerSentenceEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase" title="pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase</span></code></a></p>
<p>Implementation of the Transformer Sentence Encoder. This directly makes
use of the TransformerSentenceEncoder module in Fairseq.</p>
<dl class="simple">
<dt>A few interesting config options:</dt><dd><ul class="simple">
<li><p>encoder_normalize_before detemines whether the layer norm is applied
before or after self_attention. This is similar to original
implementation from Google.</p></li>
<li><p>activation_fn can be set to ‘gelu’ instead of the default of ‘relu’.</p></li>
<li><p>projection_dim adds a linear projection to projection_dim + tanh to
the pooled output in the style of BERT.</p></li>
</ul>
</dd>
</dl>
<dl class="method">
<dt id="pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/transformer_sentence_encoder.html#TransformerSentenceEncoder.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.upgrade_state_dict_named">
<code class="sig-name descname">upgrade_state_dict_named</code><span class="sig-paren">(</span><em class="sig-param">state_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/transformer_sentence_encoder.html#TransformerSentenceEncoder.upgrade_state_dict_named"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.upgrade_state_dict_named" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations.transformer_sentence_encoder_base">
<span id="pytext-models-representations-transformer-sentence-encoder-base-module"></span><h2>pytext.models.representations.transformer_sentence_encoder_base module<a class="headerlink" href="#module-pytext.models.representations.transformer_sentence_encoder_base" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.transformer_sentence_encoder_base.</code><code class="sig-name descname">PoolingMethod</code><a class="reference internal" href="../_modules/pytext/models/representations/transformer_sentence_encoder_base.html#PoolingMethod"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>Pooling Methods are chosen from the “Feature-based Approachs” section in
<a class="reference external" href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p>
<dl class="attribute">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.AVG_CONCAT_LAST_4_LAYERS">
<code class="sig-name descname">AVG_CONCAT_LAST_4_LAYERS</code><em class="property"> = 'avg_concat_last_4_layers'</em><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.AVG_CONCAT_LAST_4_LAYERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.AVG_LAST_LAYER">
<code class="sig-name descname">AVG_LAST_LAYER</code><em class="property"> = 'avg_last_layer'</em><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.AVG_LAST_LAYER" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.AVG_SECOND_TO_LAST_LAYER">
<code class="sig-name descname">AVG_SECOND_TO_LAST_LAYER</code><em class="property"> = 'avg_second_to_last_layer'</em><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.AVG_SECOND_TO_LAST_LAYER" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.AVG_SUM_LAST_4_LAYERS">
<code class="sig-name descname">AVG_SUM_LAST_4_LAYERS</code><em class="property"> = 'avg_sum_last_4_layers'</em><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.AVG_SUM_LAST_4_LAYERS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.CLS_TOKEN">
<code class="sig-name descname">CLS_TOKEN</code><em class="property"> = 'cls_token'</em><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.CLS_TOKEN" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.NO_POOL">
<code class="sig-name descname">NO_POOL</code><em class="property"> = 'no_pool'</em><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.PoolingMethod.NO_POOL" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.representations.transformer_sentence_encoder_base.</code><code class="sig-name descname">TransformerSentenceEncoderBase</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase.Config</em>, <em class="sig-param">output_encoded_layers=False</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/transformer_sentence_encoder_base.html#TransformerSentenceEncoderBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.representations.representation_base.RepresentationBase" title="pytext.models.representations.representation_base.RepresentationBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.representation_base.RepresentationBase</span></code></a></p>
<p>Base class for all Bi-directional Transformer based Sentence Encoders. All
children of this class should implement an _encoder function which takes
as input: tokens, [optional] segment labels and a pad mask and outputs both
the sentence representation (output of _pool_encoded_layers) and the
output states of all the intermediate Transformer layers as a list of
tensors.</p>
<p>Input tuple consists of the following elements:
1) tokens: torch tensor of size B x T which contains tokens ids
2) pad_mask: torch tensor of size B x T generated with the condition
tokens != self.vocab.get_pad_index()
3) segment_labels: torch tensor of size B x T which contains the segment
id of each token</p>
<p>Output tuple consists of the following elements:
1) encoded_layers: List of torch tensors where each tensor has shape
B x T x C and there are num_transformer_layers + 1 of these.
Each tensor represents the output of the intermediate
transformer layers with the 0th element being the input to the
first transformer layer (token + segment + position emebdding).
2) [Optional] pooled_output: Output of the pooling operation associated
with config.pooling_method to the encoded_layers.
Size B x C (or B x 4C if pooling = AVG_CONCAT_LAST_4_LAYERS)</p>
<dl class="method">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input_tuple: Tuple[torch.Tensor, ...], *args</em><span class="sig-paren">)</span> → Tuple[torch.Tensor, ...]<a class="reference internal" href="../_modules/pytext/models/representations/transformer_sentence_encoder_base.html#TransformerSentenceEncoderBase.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase.Config</em>, <em class="sig-param">output_encoded_layers=False</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/representations/transformer_sentence_encoder_base.html#TransformerSentenceEncoderBase.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.representations">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pytext.models.representations" title="Permalink to this headline">¶</a></h2>
</div>
</div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">PyText</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../train_your_first_model.html">Train your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../execute_your_first_model.html">Execute your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize_your_model.html">Visualize Model Training with TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytext_models_in_your_app.html">Use PyText models in your app</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serving_models_in_production.html">Serve Models in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_files.html">Config Files Explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_commands.html">Config Commands</a></li>
</ul>
<p class="caption"><span class="caption-text">Training More Advanced Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../atis_tutorial.html">Train Intent-Slot model on ATIS Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hierarchical_intent_slot_tutorial.html">Hierarchical intent and slot filling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../disjoint_multitask_tutorial.html">Multitask training with disjoint datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_training_tutorial.html">Data Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../xlm_r.html">XLM-RoBERTa</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending PyText</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasource_tutorial.html">Custom Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorizer.html">Custom Tensorizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dense.html">Using External Dense Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../create_new_model.html">Creating A New Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hacking_pytext.html">Hacking PyText</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../configs/pytext.html">pytext</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="pytext.html">pytext package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="pytext.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.builtin_task">pytext.builtin_task module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.main">pytext.main module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.workflow">pytext.workflow module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext">Module contents</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="../index.html">Documentation overview</a><ul>
<li><a href="pytext.html">pytext package</a><ul>
<li><a href="pytext.models.html">pytext.models package</a><ul>
<li>Previous: <a href="pytext.models.qna.html" title="previous chapter">pytext.models.qna package</a></li>
<li>Next: <a href="pytext.models.representations.transformer.html" title="next chapter">pytext.models.representations.transformer package</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="../search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>