
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<div class="section" id="pytext-metrics-package">
<h1>pytext.metrics package<a class="headerlink" href="#pytext-metrics-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-pytext.metrics.intent_slot_metrics">
<span id="pytext-metrics-intent-slot-metrics-module"></span><h2>pytext.metrics.intent_slot_metrics module<a class="headerlink" href="#module-pytext.metrics.intent_slot_metrics" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">AllMetrics</code><a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#AllMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Aggregated class for intent-slot related metrics.</p>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics.top_intent_accuracy">
<code class="sig-name descname">top_intent_accuracy</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics.top_intent_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Accuracy of the top-level intent.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics.frame_accuracy">
<code class="sig-name descname">frame_accuracy</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics.frame_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Frame accuracy.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics.frame_accuracies_by_depth">
<code class="sig-name descname">frame_accuracies_by_depth</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics.frame_accuracies_by_depth" title="Permalink to this definition">¶</a></dt>
<dd><p>Frame accuracies bucketized by depth of the gold
tree.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics.bracket_metrics">
<code class="sig-name descname">bracket_metrics</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics.bracket_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bracket metrics for intents and slots. For details, see the
function <cite>compute_intent_slot_metrics()</cite>.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics.tree_metrics">
<code class="sig-name descname">tree_metrics</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics.tree_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Tree metrics for intents and slots. For details, see the function
<cite>compute_intent_slot_metrics()</cite>.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics.loss">
<code class="sig-name descname">loss</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Cross entropy loss.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">bracket_metrics</code></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">frame_accuracies_by_depth</code></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">frame_accuracy</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics.frame_accuracy_top_k">
<em class="property">property </em><code class="sig-name descname">frame_accuracy_top_k</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics.frame_accuracy_top_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">loss</code></dt>
<dd><p>Alias for field number 6</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.AllMetrics.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#AllMetrics.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.AllMetrics.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">top_intent_accuracy</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">tree_metrics</code></dt>
<dd><p>Alias for field number 5</p>
</dd></dl>
</dd></dl>
<dl class="data">
<dt id="pytext.metrics.intent_slot_metrics.FrameAccuraciesByDepth">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">FrameAccuraciesByDepth</code><em class="property"> = typing.Dict[int, pytext.metrics.intent_slot_metrics.FrameAccuracy]</em><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.FrameAccuraciesByDepth" title="Permalink to this definition">¶</a></dt>
<dd><p>Frame accuracies bucketized by depth of the gold tree.</p>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.intent_slot_metrics.FrameAccuracy">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">FrameAccuracy</code><a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#FrameAccuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.FrameAccuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Frame accuracy for a collection of intent frame predictions.</p>
<p>Frame accuracy means the entire tree structure of the predicted frame matches that
of the gold frame.</p>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.FrameAccuracy.frame_accuracy">
<em class="property">property </em><code class="sig-name descname">frame_accuracy</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.FrameAccuracy.frame_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.FrameAccuracy.num_samples">
<em class="property">property </em><code class="sig-name descname">num_samples</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.FrameAccuracy.num_samples" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.intent_slot_metrics.FramePredictionPair">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">FramePredictionPair</code><a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#FramePredictionPair"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.FramePredictionPair" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Pair of predicted and gold intent frames.</p>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.FramePredictionPair.expected_frame">
<em class="property">property </em><code class="sig-name descname">expected_frame</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.FramePredictionPair.expected_frame" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.FramePredictionPair.predicted_frame">
<em class="property">property </em><code class="sig-name descname">predicted_frame</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.FramePredictionPair.predicted_frame" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.intent_slot_metrics.IntentSlotConfusions">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">IntentSlotConfusions</code><a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#IntentSlotConfusions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentSlotConfusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Aggregated class for intent and slot confusions.</p>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.IntentSlotConfusions.intent_confusions">
<code class="sig-name descname">intent_confusions</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentSlotConfusions.intent_confusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Confusion counts for intents.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.IntentSlotConfusions.slot_confusions">
<code class="sig-name descname">slot_confusions</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentSlotConfusions.slot_confusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Confusion counts for slots.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">intent_confusions</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">slot_confusions</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.intent_slot_metrics.IntentSlotMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">IntentSlotMetrics</code><a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#IntentSlotMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentSlotMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Precision/recall/F1 metrics for intents and slots.</p>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.IntentSlotMetrics.intent_metrics">
<code class="sig-name descname">intent_metrics</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentSlotMetrics.intent_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Precision/recall/F1 metrics for intents.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.IntentSlotMetrics.slot_metrics">
<code class="sig-name descname">slot_metrics</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentSlotMetrics.slot_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Precision/recall/F1 metrics for slots.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.IntentSlotMetrics.overall_metrics">
<code class="sig-name descname">overall_metrics</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentSlotMetrics.overall_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Combined precision/recall/F1 metrics for all nodes (merging
intents and slots).</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">intent_metrics</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">overall_metrics</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.IntentSlotMetrics.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#IntentSlotMetrics.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentSlotMetrics.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">slot_metrics</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.intent_slot_metrics.IntentsAndSlots">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">IntentsAndSlots</code><a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#IntentsAndSlots"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentsAndSlots" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Collection of intents and slots in an intent frame.</p>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.IntentsAndSlots.intents">
<em class="property">property </em><code class="sig-name descname">intents</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentsAndSlots.intents" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.IntentsAndSlots.slots">
<em class="property">property </em><code class="sig-name descname">slots</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.IntentsAndSlots.slots" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.intent_slot_metrics.Node">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">Node</code><span class="sig-paren">(</span><em class="sig-param">label: str</em>, <em class="sig-param">span: pytext.data.data_structures.node.Span</em>, <em class="sig-param">children: Optional[AbstractSet[Node]] = None</em>, <em class="sig-param">text: str = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#Node"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.Node" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.data.data_structures.html#pytext.data.data_structures.node.Node" title="pytext.data.data_structures.node.Node"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.data.data_structures.node.Node</span></code></a></p>
<p>Subclass of the base Node class, used for metric purposes. It is immutable so that
hashing can be done on the class.</p>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.Node.label">
<code class="sig-name descname">label</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.Node.label" title="Permalink to this definition">¶</a></dt>
<dd><p>Label of the node.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.Node.span">
<code class="sig-name descname">span</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.Node.span" title="Permalink to this definition">¶</a></dt>
<dd><p>Span of the node.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>Span</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.Node.children">
<code class="sig-name descname">children</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.Node.children" title="Permalink to this definition">¶</a></dt>
<dd><p>frozenset of the node’s children,
left empty when computing bracketing metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p><code class="xref py py-obj docutils literal notranslate"><span class="pre">frozenset</span></code> of <a class="reference internal" href="#pytext.metrics.intent_slot_metrics.Node" title="pytext.metrics.intent_slot_metrics.Node"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Node</span></code></a></p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.intent_slot_metrics.Node.text">
<code class="sig-name descname">text</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.Node.text" title="Permalink to this definition">¶</a></dt>
<dd><p>Text the node covers (=utterance[span.start:span.end])</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>str</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">children</code></dt>
<dd></dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">label</code></dt>
<dd></dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">span</code></dt>
<dd></dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">text</code></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.intent_slot_metrics.NodesPredictionPair">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">NodesPredictionPair</code><a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#NodesPredictionPair"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.NodesPredictionPair" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Pair of predicted and expected sets of nodes.</p>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.NodesPredictionPair.expected_nodes">
<em class="property">property </em><code class="sig-name descname">expected_nodes</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.NodesPredictionPair.expected_nodes" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.intent_slot_metrics.NodesPredictionPair.predicted_nodes">
<em class="property">property </em><code class="sig-name descname">predicted_nodes</code><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.NodesPredictionPair.predicted_nodes" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compare_frames">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compare_frames</code><span class="sig-paren">(</span><em class="sig-param">predicted_frame: pytext.metrics.intent_slot_metrics.Node</em>, <em class="sig-param">expected_frame: pytext.metrics.intent_slot_metrics.Node</em>, <em class="sig-param">tree_based: bool</em>, <em class="sig-param">intent_per_label_confusions: Optional[pytext.metrics.PerLabelConfusions] = None</em>, <em class="sig-param">slot_per_label_confusions: Optional[pytext.metrics.PerLabelConfusions] = None</em><span class="sig-paren">)</span> → pytext.metrics.intent_slot_metrics.IntentSlotConfusions<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compare_frames"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compare_frames" title="Permalink to this definition">¶</a></dt>
<dd><p>Compares two intent frames and returns TP, FP, FN counts for intents and slots.
Optionally collects the per label TP, FP, FN counts.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predicted_frame</strong> – Predicted intent frame.</p></li>
<li><p><strong>expected_frame</strong> – Gold intent frame.</p></li>
<li><p><strong>tree_based</strong> – Whether to get the tree-based confusions (if True) or bracket-based
confusions (if False). For details, see the function
<cite>compute_intent_slot_metrics()</cite>.</p></li>
<li><p><strong>intent_per_label_confusions</strong> – If provided, update the per label confusions for
intents as well. Defaults to None.</p></li>
<li><p><strong>slot_per_label_confusions</strong> – If provided, update the per label confusions for
slots as well. Defaults to None.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>IntentSlotConfusions, containing confusion counts for intents and slots.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compute_all_metrics">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compute_all_metrics</code><span class="sig-paren">(</span><em class="sig-param">frame_pairs: Sequence[pytext.metrics.intent_slot_metrics.FramePredictionPair], top_intent_accuracy: bool = True, frame_accuracy: bool = True, frame_accuracies_by_depth: bool = True, bracket_metrics: bool = True, tree_metrics: bool = True, overall_metrics: bool = False, all_predicted_frames: List[List[pytext.metrics.intent_slot_metrics.Node]] = None, calculated_loss: float = None</em><span class="sig-paren">)</span> → pytext.metrics.intent_slot_metrics.AllMetrics<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compute_all_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compute_all_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a list of predicted and gold intent frames, computes intent-slot related
metrics.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>frame_pairs</strong> – List of predicted and gold intent frames.</p></li>
<li><p><strong>top_intent_accuracy</strong> – Whether to compute top intent accuracy or not. Defaults to
True.</p></li>
<li><p><strong>frame_accuracy</strong> – Whether to compute frame accuracy or not. Defaults to True.</p></li>
<li><p><strong>frame_accuracies_by_depth</strong> – Whether to compute frame accuracies by depth or not.
Defaults to True.</p></li>
<li><p><strong>bracket_metrics</strong> – Whether to compute bracket metrics or not. Defaults to True.</p></li>
<li><p><strong>tree_metrics</strong> – Whether to compute tree metrics or not. Defaults to True.</p></li>
<li><p><strong>overall_metrics</strong> – If <cite>bracket_metrics</cite> or <cite>tree_metrics</cite> is true, decides whether
to compute overall (merging intents and slots) metrics for them. Defaults to
False.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>AllMetrics which contains intent-slot related metrics.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compute_frame_accuracies_by_depth">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compute_frame_accuracies_by_depth</code><span class="sig-paren">(</span><em class="sig-param">frame_pairs: Sequence[pytext.metrics.intent_slot_metrics.FramePredictionPair]</em><span class="sig-paren">)</span> → Dict[int, pytext.metrics.intent_slot_metrics.FrameAccuracy]<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compute_frame_accuracies_by_depth"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compute_frame_accuracies_by_depth" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a list of predicted and gold intent frames, splits the predictions into
buckets according to the depth of the gold trees, and computes frame accuracy for
each bucket.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>frame_pairs</strong> – List of predicted and gold intent frames.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>FrameAccuraciesByDepth, a map from depths to their corresponding frame
accuracies.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compute_frame_accuracy">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compute_frame_accuracy</code><span class="sig-paren">(</span><em class="sig-param">frame_pairs: Sequence[pytext.metrics.intent_slot_metrics.FramePredictionPair]</em><span class="sig-paren">)</span> → float<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compute_frame_accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compute_frame_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes frame accuracy given a list of predicted and gold intent frames.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>frame_pairs</strong> – List of predicted and gold intent frames.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Frame accuracy. For a prediction, frame accuracy is achieved if the entire tree
structure of the predicted frame matches that of the gold frame.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compute_frame_accuracy_top_k">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compute_frame_accuracy_top_k</code><span class="sig-paren">(</span><em class="sig-param">frame_pairs: List[pytext.metrics.intent_slot_metrics.FramePredictionPair], all_frames: List[List[pytext.metrics.intent_slot_metrics.Node]]</em><span class="sig-paren">)</span> → float<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compute_frame_accuracy_top_k"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compute_frame_accuracy_top_k" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compute_intent_slot_metrics">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compute_intent_slot_metrics</code><span class="sig-paren">(</span><em class="sig-param">frame_pairs: Sequence[pytext.metrics.intent_slot_metrics.FramePredictionPair], tree_based: bool, overall_metrics: bool = True</em><span class="sig-paren">)</span> → pytext.metrics.intent_slot_metrics.IntentSlotMetrics<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compute_intent_slot_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compute_intent_slot_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a list of predicted and gold intent frames, computes precision, recall and F1
metrics for intents and slots, either in tree-based or bracket-based manner.</p>
<p>The following assumptions are taken on intent frames:
1. The root node is an intent,
2. Children of intents are always slots, and children of slots are always intents.</p>
<p>For tree-based metrics, a node (an intent or slot) in the predicted frame is
considered a true positive only if the subtree rooted at this node has an exact copy
in the gold frame, otherwise it is considered a false positive. A false negative is
a node in the gold frame that does not have an exact subtree match in the predicted
frame.</p>
<p>For bracket-based metrics, a node in the predicted frame is considered a true
positive if there is a node in the gold frame having the same label and span (but
not necessarily the same children). The definitions of false positives and false
negatives are similar to the above.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>frame_pairs</strong> – List of predicted and gold intent frames.</p></li>
<li><p><strong>tree_based</strong> – Whether to compute tree-based metrics (if True) or bracket-based
metrics (if False).</p></li>
<li><p><strong>overall_metrics</strong> – Whether to compute overall (merging intents and slots) metrics
or not. Defaults to True.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>IntentSlotMetrics, containing precision/recall/F1 metrics for intents and slots.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compute_metric_at_k">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compute_metric_at_k</code><span class="sig-paren">(</span><em class="sig-param">references: List[pytext.metrics.intent_slot_metrics.Node], hypothesis: List[List[pytext.metrics.intent_slot_metrics.Node]], metric_fn: Callable[[pytext.metrics.intent_slot_metrics.Node, pytext.metrics.intent_slot_metrics.Node], bool] = &lt;function &lt;lambda&gt;&gt;</em><span class="sig-paren">)</span> → List[float]<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compute_metric_at_k"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compute_metric_at_k" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes a boolean metric at each position in the ranked list of hypothesis,
and returns an average for each position over all examples.
By default metric_fn is comparing if frames are equal.</p>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compute_prf1_metrics">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compute_prf1_metrics</code><span class="sig-paren">(</span><em class="sig-param">nodes_pairs: Sequence[pytext.metrics.intent_slot_metrics.NodesPredictionPair]</em><span class="sig-paren">)</span> → Tuple[pytext.metrics.AllConfusions, pytext.metrics.PRF1Metrics]<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compute_prf1_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compute_prf1_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes precision/recall/F1 metrics given a list of predicted and expected sets of
nodes.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>nodes_pairs</strong> – List of predicted and expected node sets.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A tuple, of which the first member contains the confusion information, and the
second member contains the computed precision/recall/F1 metrics.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.intent_slot_metrics.compute_top_intent_accuracy">
<code class="sig-prename descclassname">pytext.metrics.intent_slot_metrics.</code><code class="sig-name descname">compute_top_intent_accuracy</code><span class="sig-paren">(</span><em class="sig-param">frame_pairs: Sequence[pytext.metrics.intent_slot_metrics.FramePredictionPair]</em><span class="sig-paren">)</span> → float<a class="reference internal" href="../_modules/pytext/metrics/intent_slot_metrics.html#compute_top_intent_accuracy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.intent_slot_metrics.compute_top_intent_accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes accuracy of the top-level intent.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>frame_pairs</strong> – List of predicted and gold intent frames.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Prediction accuracy of the top-level intent.</p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.metrics.language_model_metrics">
<span id="pytext-metrics-language-model-metrics-module"></span><h2>pytext.metrics.language_model_metrics module<a class="headerlink" href="#module-pytext.metrics.language_model_metrics" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.metrics.language_model_metrics.LanguageModelMetric">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.language_model_metrics.</code><code class="sig-name descname">LanguageModelMetric</code><a class="reference internal" href="../_modules/pytext/metrics/language_model_metrics.html#LanguageModelMetric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.language_model_metrics.LanguageModelMetric" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Class for language model metrics.</p>
<dl class="attribute">
<dt id="pytext.metrics.language_model_metrics.LanguageModelMetric.perplexity_per_word">
<code class="sig-name descname">perplexity_per_word</code><a class="headerlink" href="#pytext.metrics.language_model_metrics.LanguageModelMetric.perplexity_per_word" title="Permalink to this definition">¶</a></dt>
<dd><p>Average perplexity per word of the dataset.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">perplexity_per_word</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.language_model_metrics.LanguageModelMetric.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/metrics/language_model_metrics.html#LanguageModelMetric.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.language_model_metrics.LanguageModelMetric.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.language_model_metrics.compute_language_model_metric">
<code class="sig-prename descclassname">pytext.metrics.language_model_metrics.</code><code class="sig-name descname">compute_language_model_metric</code><span class="sig-paren">(</span><em class="sig-param">loss_per_word: float</em><span class="sig-paren">)</span> → pytext.metrics.language_model_metrics.LanguageModelMetric<a class="reference internal" href="../_modules/pytext/metrics/language_model_metrics.html#compute_language_model_metric"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.language_model_metrics.compute_language_model_metric" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</div>
<div class="section" id="module-pytext.metrics.squad_metrics">
<span id="pytext-metrics-squad-metrics-module"></span><h2>pytext.metrics.squad_metrics module<a class="headerlink" href="#module-pytext.metrics.squad_metrics" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.metrics.squad_metrics.SquadMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.squad_metrics.</code><code class="sig-name descname">SquadMetrics</code><span class="sig-paren">(</span><em class="sig-param">num_examples</em>, <em class="sig-param">exact_matches</em>, <em class="sig-param">f1_score</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/metrics/squad_metrics.html#SquadMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.squad_metrics.SquadMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<dl class="method">
<dt id="pytext.metrics.squad_metrics.SquadMetrics.exact_matches">
<em class="property">property </em><code class="sig-name descname">exact_matches</code><a class="headerlink" href="#pytext.metrics.squad_metrics.SquadMetrics.exact_matches" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.squad_metrics.SquadMetrics.f1_score">
<em class="property">property </em><code class="sig-name descname">f1_score</code><a class="headerlink" href="#pytext.metrics.squad_metrics.SquadMetrics.f1_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.squad_metrics.SquadMetrics.num_examples">
<em class="property">property </em><code class="sig-name descname">num_examples</code><a class="headerlink" href="#pytext.metrics.squad_metrics.SquadMetrics.num_examples" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.squad_metrics.SquadMetrics.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/metrics/squad_metrics.html#SquadMetrics.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.squad_metrics.SquadMetrics.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.metrics">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pytext.metrics" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.metrics.AllConfusions">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">AllConfusions</code><a class="reference internal" href="../_modules/pytext/metrics.html#AllConfusions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.AllConfusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Aggregated class for per label confusions.</p>
<dl class="attribute">
<dt id="pytext.metrics.AllConfusions.per_label_confusions">
<code class="sig-name descname">per_label_confusions</code><a class="headerlink" href="#pytext.metrics.AllConfusions.per_label_confusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Per label confusion information.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.AllConfusions.confusions">
<code class="sig-name descname">confusions</code><a class="headerlink" href="#pytext.metrics.AllConfusions.confusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Overall TP, FP and FN counts across the labels in
<cite>per_label_confusions</cite>.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.AllConfusions.compute_metrics">
<code class="sig-name descname">compute_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → pytext.metrics.PRF1Metrics<a class="reference internal" href="../_modules/pytext/metrics.html#AllConfusions.compute_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.AllConfusions.compute_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">confusions</code></dt>
<dd></dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">per_label_confusions</code></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.ClassificationMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">ClassificationMetrics</code><a class="reference internal" href="../_modules/pytext/metrics.html#ClassificationMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.ClassificationMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Metric class for various classification metrics.</p>
<dl class="attribute">
<dt id="pytext.metrics.ClassificationMetrics.accuracy">
<code class="sig-name descname">accuracy</code><a class="headerlink" href="#pytext.metrics.ClassificationMetrics.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>Overall accuracy of predictions.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.ClassificationMetrics.macro_prf1_metrics">
<code class="sig-name descname">macro_prf1_metrics</code><a class="headerlink" href="#pytext.metrics.ClassificationMetrics.macro_prf1_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Macro precision/recall/F1 scores.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.ClassificationMetrics.per_label_soft_scores">
<code class="sig-name descname">per_label_soft_scores</code><a class="headerlink" href="#pytext.metrics.ClassificationMetrics.per_label_soft_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Per label soft metrics.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.ClassificationMetrics.mcc">
<code class="sig-name descname">mcc</code><a class="headerlink" href="#pytext.metrics.ClassificationMetrics.mcc" title="Permalink to this definition">¶</a></dt>
<dd><p>Matthews correlation coefficient.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.ClassificationMetrics.roc_auc">
<code class="sig-name descname">roc_auc</code><a class="headerlink" href="#pytext.metrics.ClassificationMetrics.roc_auc" title="Permalink to this definition">¶</a></dt>
<dd><p>Area under the Receiver Operating Characteristic curve.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.ClassificationMetrics.loss">
<code class="sig-name descname">loss</code><a class="headerlink" href="#pytext.metrics.ClassificationMetrics.loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Training loss (only used for selecting best model, no need to print).</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">accuracy</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">loss</code></dt>
<dd><p>Alias for field number 5</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">macro_prf1_metrics</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">mcc</code></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">per_label_soft_scores</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.ClassificationMetrics.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><em class="sig-param">report_pep=False</em><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/metrics.html#ClassificationMetrics.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.ClassificationMetrics.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.metrics.ClassificationMetrics.print_pep">
<code class="sig-name descname">print_pep</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/metrics.html#ClassificationMetrics.print_pep"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.ClassificationMetrics.print_pep" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">roc_auc</code></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.Confusions">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">Confusions</code><span class="sig-paren">(</span><em class="sig-param">TP: int = 0</em>, <em class="sig-param">FP: int = 0</em>, <em class="sig-param">FN: int = 0</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/metrics.html#Confusions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.Confusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Confusion information for a collection of predictions.</p>
<dl class="attribute">
<dt id="pytext.metrics.Confusions.TP">
<code class="sig-name descname">TP</code><a class="headerlink" href="#pytext.metrics.Confusions.TP" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of true positives.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.Confusions.FP">
<code class="sig-name descname">FP</code><a class="headerlink" href="#pytext.metrics.Confusions.FP" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of false positives.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.Confusions.FN">
<code class="sig-name descname">FN</code><a class="headerlink" href="#pytext.metrics.Confusions.FN" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of false negatives.</p>
</dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">FN</code></dt>
<dd></dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">FP</code></dt>
<dd></dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">TP</code></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.metrics.Confusions.compute_metrics">
<code class="sig-name descname">compute_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → pytext.metrics.PRF1Scores<a class="reference internal" href="../_modules/pytext/metrics.html#Confusions.compute_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.Confusions.compute_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.LabelListPrediction">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">LabelListPrediction</code><a class="reference internal" href="../_modules/pytext/metrics.html#LabelListPrediction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.LabelListPrediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Label list predictions of an example.</p>
<dl class="attribute">
<dt id="pytext.metrics.LabelListPrediction.label_scores">
<code class="sig-name descname">label_scores</code><a class="headerlink" href="#pytext.metrics.LabelListPrediction.label_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Confidence scores that each label receives.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.LabelListPrediction.predicted_label">
<code class="sig-name descname">predicted_label</code><a class="headerlink" href="#pytext.metrics.LabelListPrediction.predicted_label" title="Permalink to this definition">¶</a></dt>
<dd><p>List of indices of the predicted label.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.LabelListPrediction.expected_label">
<code class="sig-name descname">expected_label</code><a class="headerlink" href="#pytext.metrics.LabelListPrediction.expected_label" title="Permalink to this definition">¶</a></dt>
<dd><p>List of indices of the true label.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">expected_label</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">label_scores</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">predicted_label</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.LabelPrediction">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">LabelPrediction</code><a class="reference internal" href="../_modules/pytext/metrics.html#LabelPrediction"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.LabelPrediction" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Label predictions of an example.</p>
<dl class="attribute">
<dt id="pytext.metrics.LabelPrediction.label_scores">
<code class="sig-name descname">label_scores</code><a class="headerlink" href="#pytext.metrics.LabelPrediction.label_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Confidence scores that each label receives.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.LabelPrediction.predicted_label">
<code class="sig-name descname">predicted_label</code><a class="headerlink" href="#pytext.metrics.LabelPrediction.predicted_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Index of the predicted label. This is usually the label with
the highest confidence score in label_scores.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.LabelPrediction.expected_label">
<code class="sig-name descname">expected_label</code><a class="headerlink" href="#pytext.metrics.LabelPrediction.expected_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Index of the true label.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">expected_label</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">label_scores</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">predicted_label</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.MacroPRF1Metrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">MacroPRF1Metrics</code><a class="reference internal" href="../_modules/pytext/metrics.html#MacroPRF1Metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.MacroPRF1Metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Aggregated metric class for macro precision/recall/F1 scores.</p>
<dl class="attribute">
<dt id="pytext.metrics.MacroPRF1Metrics.per_label_scores">
<code class="sig-name descname">per_label_scores</code><a class="headerlink" href="#pytext.metrics.MacroPRF1Metrics.per_label_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Mapping from label string to the corresponding
precision/recall/F1 scores.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.MacroPRF1Metrics.macro_scores">
<code class="sig-name descname">macro_scores</code><a class="headerlink" href="#pytext.metrics.MacroPRF1Metrics.macro_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Macro precision/recall/F1 scores across the labels in
<cite>per_label_scores</cite>.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">macro_scores</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">per_label_scores</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.MacroPRF1Metrics.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><em class="sig-param">indentation=''</em><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/metrics.html#MacroPRF1Metrics.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.MacroPRF1Metrics.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.MacroPRF1Scores">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">MacroPRF1Scores</code><a class="reference internal" href="../_modules/pytext/metrics.html#MacroPRF1Scores"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.MacroPRF1Scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Macro precision/recall/F1 scores (averages across each label).</p>
<dl class="attribute">
<dt id="pytext.metrics.MacroPRF1Scores.num_label">
<code class="sig-name descname">num_label</code><a class="headerlink" href="#pytext.metrics.MacroPRF1Scores.num_label" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of distinct labels.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.MacroPRF1Scores.precision">
<code class="sig-name descname">precision</code><a class="headerlink" href="#pytext.metrics.MacroPRF1Scores.precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Equally weighted average of precisions for each label.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.MacroPRF1Scores.recall">
<code class="sig-name descname">recall</code><a class="headerlink" href="#pytext.metrics.MacroPRF1Scores.recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Equally weighted average of recalls for each label.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.MacroPRF1Scores.f1">
<code class="sig-name descname">f1</code><a class="headerlink" href="#pytext.metrics.MacroPRF1Scores.f1" title="Permalink to this definition">¶</a></dt>
<dd><p>Equally weighted average of F1 scores for each label.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">f1</code></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.MacroPRF1Scores.num_labels">
<em class="property">property </em><code class="sig-name descname">num_labels</code><a class="headerlink" href="#pytext.metrics.MacroPRF1Scores.num_labels" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">precision</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">recall</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
</dd></dl>
<dl class="data">
<dt id="pytext.metrics.PRECISION_AT_RECALL_THRESHOLDS">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">PRECISION_AT_RECALL_THRESHOLDS</code><em class="property"> = [0.2, 0.4, 0.6, 0.8, 0.9]</em><a class="headerlink" href="#pytext.metrics.PRECISION_AT_RECALL_THRESHOLDS" title="Permalink to this definition">¶</a></dt>
<dd><p>Basic metric classes and functions for single-label prediction problems.
Extending to multi-label support</p>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.PRF1Metrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">PRF1Metrics</code><a class="reference internal" href="../_modules/pytext/metrics.html#PRF1Metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.PRF1Metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Metric class for all types of precision/recall/F1 scores.</p>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Metrics.per_label_scores">
<code class="sig-name descname">per_label_scores</code><a class="headerlink" href="#pytext.metrics.PRF1Metrics.per_label_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Map from label string to the corresponding precision/recall/F1
scores.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Metrics.macro_scores">
<code class="sig-name descname">macro_scores</code><a class="headerlink" href="#pytext.metrics.PRF1Metrics.macro_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Macro precision/recall/F1 scores across the labels in
<cite>per_label_scores</cite>.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Metrics.micro_scores">
<code class="sig-name descname">micro_scores</code><a class="headerlink" href="#pytext.metrics.PRF1Metrics.micro_scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Micro (regular) precision/recall/F1 scores for the same
collection of predictions.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">macro_scores</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">micro_scores</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">per_label_scores</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.PRF1Metrics.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/metrics.html#PRF1Metrics.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.PRF1Metrics.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.PRF1Scores">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">PRF1Scores</code><a class="reference internal" href="../_modules/pytext/metrics.html#PRF1Scores"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.PRF1Scores" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Precision/recall/F1 scores for a collection of predictions.</p>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Scores.true_positives">
<code class="sig-name descname">true_positives</code><a class="headerlink" href="#pytext.metrics.PRF1Scores.true_positives" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of true positives.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Scores.false_positives">
<code class="sig-name descname">false_positives</code><a class="headerlink" href="#pytext.metrics.PRF1Scores.false_positives" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of false positives.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Scores.false_negatives">
<code class="sig-name descname">false_negatives</code><a class="headerlink" href="#pytext.metrics.PRF1Scores.false_negatives" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of false negatives.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Scores.precision">
<code class="sig-name descname">precision</code><a class="headerlink" href="#pytext.metrics.PRF1Scores.precision" title="Permalink to this definition">¶</a></dt>
<dd><p>TP / (TP + FP).</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Scores.recall">
<code class="sig-name descname">recall</code><a class="headerlink" href="#pytext.metrics.PRF1Scores.recall" title="Permalink to this definition">¶</a></dt>
<dd><p>TP / (TP + FN).</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PRF1Scores.f1">
<code class="sig-name descname">f1</code><a class="headerlink" href="#pytext.metrics.PRF1Scores.f1" title="Permalink to this definition">¶</a></dt>
<dd><p>2 * TP / (2 * TP + FP + FN).</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">f1</code></dt>
<dd><p>Alias for field number 5</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">false_negatives</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">false_positives</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">precision</code></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">recall</code></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">true_positives</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.PairwiseRankingMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">PairwiseRankingMetrics</code><a class="reference internal" href="../_modules/pytext/metrics.html#PairwiseRankingMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.PairwiseRankingMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Metric class for pairwise ranking</p>
<dl class="attribute">
<dt id="pytext.metrics.PairwiseRankingMetrics.num_examples">
<code class="sig-name descname">num_examples</code><a class="headerlink" href="#pytext.metrics.PairwiseRankingMetrics.num_examples" title="Permalink to this definition">¶</a></dt>
<dd><p>number of samples</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PairwiseRankingMetrics.accuracy">
<code class="sig-name descname">accuracy</code><a class="headerlink" href="#pytext.metrics.PairwiseRankingMetrics.accuracy" title="Permalink to this definition">¶</a></dt>
<dd><p>how many times did we rank in the correct order</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PairwiseRankingMetrics.average_score_difference">
<code class="sig-name descname">average_score_difference</code><a class="headerlink" href="#pytext.metrics.PairwiseRankingMetrics.average_score_difference" title="Permalink to this definition">¶</a></dt>
<dd><p>average score(higherRank) - score(lowerRank)</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">accuracy</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">average_score_difference</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">num_examples</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.PairwiseRankingMetrics.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/metrics.html#PairwiseRankingMetrics.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.PairwiseRankingMetrics.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.PerLabelConfusions">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">PerLabelConfusions</code><a class="reference internal" href="../_modules/pytext/metrics.html#PerLabelConfusions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.PerLabelConfusions" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">object</span></code></p>
<p>Per label confusion information.</p>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.label_confusions_map">
<code class="sig-name descname">label_confusions_map</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.label_confusions_map" title="Permalink to this definition">¶</a></dt>
<dd><p>Map from label string to the corresponding confusion
counts.</p>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.a">
<code class="sig-name descname">a</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.a" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.b">
<code class="sig-name descname">b</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.b" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.c">
<code class="sig-name descname">c</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.c" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.metrics.PerLabelConfusions.compute_metrics">
<code class="sig-name descname">compute_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → pytext.metrics.MacroPRF1Metrics<a class="reference internal" href="../_modules/pytext/metrics.html#PerLabelConfusions.compute_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.compute_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.e">
<code class="sig-name descname">e</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.e" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.f">
<code class="sig-name descname">f</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.f" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.i">
<code class="sig-name descname">i</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.i" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.l">
<code class="sig-name descname">l</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.l" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt>
<code class="sig-name descname">label_confusions_map</code></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.m">
<code class="sig-name descname">m</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.m" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.n">
<code class="sig-name descname">n</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.n" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.o">
<code class="sig-name descname">o</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.o" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.p">
<code class="sig-name descname">p</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.p" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.s">
<code class="sig-name descname">s</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.s" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.PerLabelConfusions.u">
<code class="sig-name descname">u</code><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.u" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.metrics.PerLabelConfusions.update">
<code class="sig-name descname">update</code><span class="sig-paren">(</span><em class="sig-param">label: str</em>, <em class="sig-param">item: str</em>, <em class="sig-param">count: int</em><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/metrics.html#PerLabelConfusions.update"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.PerLabelConfusions.update" title="Permalink to this definition">¶</a></dt>
<dd><p>Increase one of TP, FP or FN count for a label by certain amount.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>label</strong> – Label to be modified.</p></li>
<li><p><strong>item</strong> – Type of count to be modified, should be one of “TP”, “FP” or “FN”.</p></li>
<li><p><strong>count</strong> – Amount to be added to the count.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>None</p>
</dd>
</dl>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.RealtimeMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">RealtimeMetrics</code><a class="reference internal" href="../_modules/pytext/metrics.html#RealtimeMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.RealtimeMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Realtime Metrics for tracking training progress and performance.</p>
<dl class="attribute">
<dt id="pytext.metrics.RealtimeMetrics.samples">
<code class="sig-name descname">samples</code><a class="headerlink" href="#pytext.metrics.RealtimeMetrics.samples" title="Permalink to this definition">¶</a></dt>
<dd><p>number of samples</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.RealtimeMetrics.tps">
<code class="sig-name descname">tps</code><a class="headerlink" href="#pytext.metrics.RealtimeMetrics.tps" title="Permalink to this definition">¶</a></dt>
<dd><p>tokens per second</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.RealtimeMetrics.ups">
<code class="sig-name descname">ups</code><a class="headerlink" href="#pytext.metrics.RealtimeMetrics.ups" title="Permalink to this definition">¶</a></dt>
<dd><p>updates per second</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">samples</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">tps</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">ups</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.RegressionMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">RegressionMetrics</code><a class="reference internal" href="../_modules/pytext/metrics.html#RegressionMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.RegressionMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Metrics for regression tasks.</p>
<dl class="attribute">
<dt id="pytext.metrics.RegressionMetrics.num_examples">
<code class="sig-name descname">num_examples</code><a class="headerlink" href="#pytext.metrics.RegressionMetrics.num_examples" title="Permalink to this definition">¶</a></dt>
<dd><p>number of examples</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.RegressionMetrics.pearson_correlation">
<code class="sig-name descname">pearson_correlation</code><a class="headerlink" href="#pytext.metrics.RegressionMetrics.pearson_correlation" title="Permalink to this definition">¶</a></dt>
<dd><p>correlation between predictions and labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="attribute">
<dt id="pytext.metrics.RegressionMetrics.mse">
<code class="sig-name descname">mse</code><a class="headerlink" href="#pytext.metrics.RegressionMetrics.mse" title="Permalink to this definition">¶</a></dt>
<dd><p>mean-squared error between predictions and labels</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">mse</code></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">num_examples</code></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">pearson_correlation</code></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.RegressionMetrics.print_metrics">
<code class="sig-name descname">print_metrics</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/metrics.html#RegressionMetrics.print_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.RegressionMetrics.print_metrics" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.metrics.SoftClassificationMetrics">
<em class="property">class </em><code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">SoftClassificationMetrics</code><a class="reference internal" href="../_modules/pytext/metrics.html#SoftClassificationMetrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.SoftClassificationMetrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">tuple</span></code></p>
<p>Classification scores that are independent of thresholds.</p>
<dl class="method">
<dt id="pytext.metrics.SoftClassificationMetrics.average_precision">
<em class="property">property </em><code class="sig-name descname">average_precision</code><a class="headerlink" href="#pytext.metrics.SoftClassificationMetrics.average_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.SoftClassificationMetrics.decision_thresh_at_precision">
<em class="property">property </em><code class="sig-name descname">decision_thresh_at_precision</code><a class="headerlink" href="#pytext.metrics.SoftClassificationMetrics.decision_thresh_at_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.SoftClassificationMetrics.decision_thresh_at_recall">
<em class="property">property </em><code class="sig-name descname">decision_thresh_at_recall</code><a class="headerlink" href="#pytext.metrics.SoftClassificationMetrics.decision_thresh_at_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.SoftClassificationMetrics.precision_at_recall">
<em class="property">property </em><code class="sig-name descname">precision_at_recall</code><a class="headerlink" href="#pytext.metrics.SoftClassificationMetrics.precision_at_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.SoftClassificationMetrics.recall_at_precision">
<em class="property">property </em><code class="sig-name descname">recall_at_precision</code><a class="headerlink" href="#pytext.metrics.SoftClassificationMetrics.recall_at_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>
<dl class="method">
<dt id="pytext.metrics.SoftClassificationMetrics.roc_auc">
<em class="property">property </em><code class="sig-name descname">roc_auc</code><a class="headerlink" href="#pytext.metrics.SoftClassificationMetrics.roc_auc" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 5</p>
</dd></dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.average_precision_score">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">average_precision_score</code><span class="sig-paren">(</span><em class="sig-param">y_true_sorted: numpy.ndarray</em>, <em class="sig-param">y_score_sorted: numpy.ndarray</em><span class="sig-paren">)</span> → float<a class="reference internal" href="../_modules/pytext/metrics.html#average_precision_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.average_precision_score" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes average precision, which summarizes the precision-recall curve as the
precisions achieved at each threshold weighted by the increase in recall since the
previous threshold.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true_sorted</strong> – Numpy array sorted according to decreasing confidence scores
indicating whether each prediction is correct.</p></li>
<li><p><strong>Numpy array of confidence scores for the predictions in</strong> (<em>y_score_sorted</em>) – decreasing order.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Average precision score.</p>
</dd>
</dl>
<p>TODO: This is too slow, improve the performance</p>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_classification_metrics">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_classification_metrics</code><span class="sig-paren">(</span><em class="sig-param">predictions: Sequence[pytext.metrics.LabelPrediction], label_names: Sequence[str], loss: float, average_precisions: bool = True, recall_at_precision_thresholds: Sequence[float] = [0.2, 0.4, 0.6, 0.8, 0.9], precision_at_recall_thresholds: Sequence[float] = [0.2, 0.4, 0.6, 0.8, 0.9]</em><span class="sig-paren">)</span> → pytext.metrics.ClassificationMetrics<a class="reference internal" href="../_modules/pytext/metrics.html#compute_classification_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_classification_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>A general function that computes classification metrics given a list of label
predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> – Label predictions, including the confidence score for each label.</p></li>
<li><p><strong>label_names</strong> – Indexed label names.</p></li>
<li><p><strong>average_precisions</strong> – Whether to compute average precisions for labels or not.
Defaults to True.</p></li>
<li><p><strong>recall_at_precision_thresholds</strong> – precision thresholds at which
to calculate recall</p></li>
<li><p><strong>precision_at_recall_thresholds</strong> – recall thresholds at which
to calculate precision</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ClassificationMetrics which contains various classification metrics.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_matthews_correlation_coefficients">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_matthews_correlation_coefficients</code><span class="sig-paren">(</span><em class="sig-param">TP: int</em>, <em class="sig-param">FP: int</em>, <em class="sig-param">FN: int</em>, <em class="sig-param">TN: int</em><span class="sig-paren">)</span> → float<a class="reference internal" href="../_modules/pytext/metrics.html#compute_matthews_correlation_coefficients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_matthews_correlation_coefficients" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes Matthews correlation coefficient, a way to summarize all four counts (TP,
FP, FN, TN) in the confusion matrix of binary classification.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TP</strong> – Number of true positives.</p></li>
<li><p><strong>FP</strong> – Number of false positives.</p></li>
<li><p><strong>FN</strong> – Number of false negatives.</p></li>
<li><p><strong>TN</strong> – Number of true negatives.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Matthews correlation coefficient, which is <cite>sqrt((TP + FP) * (TP + FN) *
(TN + FP) * (TN + FN))</cite>.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_multi_label_classification_metrics">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_multi_label_classification_metrics</code><span class="sig-paren">(</span><em class="sig-param">predictions: Sequence[pytext.metrics.LabelListPrediction], label_names: Sequence[str], loss: float, average_precisions: bool = True, recall_at_precision_thresholds: Sequence[float] = [0.2, 0.4, 0.6, 0.8, 0.9], precision_at_recall_thresholds: Sequence[float] = [0.2, 0.4, 0.6, 0.8, 0.9]</em><span class="sig-paren">)</span> → pytext.metrics.ClassificationMetrics<a class="reference internal" href="../_modules/pytext/metrics.html#compute_multi_label_classification_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_multi_label_classification_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>A general function that computes classification metrics given a list of multi-label
predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> – multi-label predictions,
including the confidence score for each label.</p></li>
<li><p><strong>label_names</strong> – Indexed label names.</p></li>
<li><p><strong>average_precisions</strong> – Whether to compute average precisions for labels or not.
Defaults to True.</p></li>
<li><p><strong>recall_at_precision_thresholds</strong> – precision thresholds at which
to calculate recall</p></li>
<li><p><strong>precision_at_recall_thresholds</strong> – recall thresholds at which
to calculate precision</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>ClassificationMetrics which contains various classification metrics.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_multi_label_soft_metrics">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_multi_label_soft_metrics</code><span class="sig-paren">(</span><em class="sig-param">predictions: Sequence[pytext.metrics.LabelListPrediction], label_names: Sequence[str], recall_at_precision_thresholds: Sequence[float] = [0.2, 0.4, 0.6, 0.8, 0.9], precision_at_recall_thresholds: Sequence[float] = [0.2, 0.4, 0.6, 0.8, 0.9]</em><span class="sig-paren">)</span> → Dict[str, pytext.metrics.SoftClassificationMetrics]<a class="reference internal" href="../_modules/pytext/metrics.html#compute_multi_label_soft_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_multi_label_soft_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes multi-label soft classification metrics</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> – multi-label predictions,
including the confidence score for each label.</p></li>
<li><p><strong>label_names</strong> – Indexed label names.</p></li>
<li><p><strong>recall_at_precision_thresholds</strong> – precision thresholds at which to calculate
recall</p></li>
<li><p><strong>precision_at_recall_thresholds</strong> – recall thresholds at which to calculate
precision</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict from label strings to their corresponding soft metrics.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_pairwise_ranking_metrics">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_pairwise_ranking_metrics</code><span class="sig-paren">(</span><em class="sig-param">predictions: Sequence[int], scores: Sequence[float]</em><span class="sig-paren">)</span> → pytext.metrics.PairwiseRankingMetrics<a class="reference internal" href="../_modules/pytext/metrics.html#compute_pairwise_ranking_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_pairwise_ranking_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes metrics for pairwise ranking given sequences of predictions and scores</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> – 1 if ranking was correct, 0 if ranking was incorrect</p></li>
<li><p><strong>scores</strong> – score(higher-ranked-sample) - score(lower-ranked-sample)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>PairwiseRankingMetrics object</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_prf1">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_prf1</code><span class="sig-paren">(</span><em class="sig-param">tp: int</em>, <em class="sig-param">fp: int</em>, <em class="sig-param">fn: int</em><span class="sig-paren">)</span> → Tuple[float, float, float]<a class="reference internal" href="../_modules/pytext/metrics.html#compute_prf1"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_prf1" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_regression_metrics">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_regression_metrics</code><span class="sig-paren">(</span><em class="sig-param">predictions: Sequence[float], targets: Sequence[float]</em><span class="sig-paren">)</span> → pytext.metrics.RegressionMetrics<a class="reference internal" href="../_modules/pytext/metrics.html#compute_regression_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_regression_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes metrics for regression tasks.abs</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> – 1-D sequence of float predictions</p></li>
<li><p><strong>targets</strong> – 1-D sequence of float labels</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>RegressionMetrics object</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_roc_auc">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_roc_auc</code><span class="sig-paren">(</span><em class="sig-param">predictions: Sequence[pytext.metrics.LabelPrediction], target_class: int = 0</em><span class="sig-paren">)</span> → Optional[float]<a class="reference internal" href="../_modules/pytext/metrics.html#compute_roc_auc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_roc_auc" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes area under the Receiver Operating Characteristic curve, for binary
classification. Implementation based off of (and explained at)
<a class="reference external" href="https://www.ibm.com/developerworks/community/blogs/jfp/entry/Fast_Computation_of_AUC_ROC_score?lang=en">https://www.ibm.com/developerworks/community/blogs/jfp/entry/Fast_Computation_of_AUC_ROC_score?lang=en</a>.</p>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.compute_soft_metrics">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">compute_soft_metrics</code><span class="sig-paren">(</span><em class="sig-param">predictions: Sequence[pytext.metrics.LabelPrediction], label_names: Sequence[str], recall_at_precision_thresholds: Sequence[float] = [0.2, 0.4, 0.6, 0.8, 0.9], precision_at_recall_thresholds: Sequence[float] = [0.2, 0.4, 0.6, 0.8, 0.9]</em><span class="sig-paren">)</span> → Dict[str, pytext.metrics.SoftClassificationMetrics]<a class="reference internal" href="../_modules/pytext/metrics.html#compute_soft_metrics"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.compute_soft_metrics" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes soft classification metrics given a list of label predictions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>predictions</strong> – Label predictions, including the confidence score for each label.</p></li>
<li><p><strong>label_names</strong> – Indexed label names.</p></li>
<li><p><strong>recall_at_precision_thresholds</strong> – precision thresholds at which to calculate
recall</p></li>
<li><p><strong>precision_at_recall_thresholds</strong> – recall thresholds at which to calculate
precision</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dict from label strings to their corresponding soft metrics.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.precision_at_recall">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">precision_at_recall</code><span class="sig-paren">(</span><em class="sig-param">y_true_sorted: numpy.ndarray, y_score_sorted: numpy.ndarray, thresholds: Sequence[float]</em><span class="sig-paren">)</span> → Tuple[Dict[float, float], Dict[float, float]]<a class="reference internal" href="../_modules/pytext/metrics.html#precision_at_recall"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.precision_at_recall" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes precision at various recall levels</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true_sorted</strong> – Numpy array sorted according to decreasing confidence scores
indicating whether each prediction is correct.</p></li>
<li><p><strong>y_score_sorted</strong> – Numpy array of confidence scores for the predictions in
decreasing order.</p></li>
<li><p><strong>thresholds</strong> – Sequence of floats indicating the requested recall thresholds</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary of maximum precision at requested recall thresholds.
Dictionary of decision thresholds resulting in max precision at
requested recall thresholds.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.recall_at_precision">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">recall_at_precision</code><span class="sig-paren">(</span><em class="sig-param">y_true_sorted: numpy.ndarray, y_score_sorted: numpy.ndarray, thresholds: Sequence[float]</em><span class="sig-paren">)</span> → Dict[float, float]<a class="reference internal" href="../_modules/pytext/metrics.html#recall_at_precision"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.recall_at_precision" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes recall at various precision levels</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>y_true_sorted</strong> – Numpy array sorted according to decreasing confidence scores
indicating whether each prediction is correct.</p></li>
<li><p><strong>y_score_sorted</strong> – Numpy array of confidence scores for the predictions in
decreasing order.</p></li>
<li><p><strong>thresholds</strong> – Sequence of floats indicating the requested precision thresholds</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Dictionary of maximum recall at requested precision thresholds.</p>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.metrics.safe_division">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">safe_division</code><span class="sig-paren">(</span><em class="sig-param">n: Union[int, float], d: int</em><span class="sig-paren">)</span> → float<a class="reference internal" href="../_modules/pytext/metrics.html#safe_division"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.safe_division" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="function">
<dt id="pytext.metrics.sort_by_score">
<code class="sig-prename descclassname">pytext.metrics.</code><code class="sig-name descname">sort_by_score</code><span class="sig-paren">(</span><em class="sig-param">y_true_list: Sequence[bool], y_score_list: Sequence[float]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/metrics.html#sort_by_score"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.metrics.sort_by_score" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</div>
</div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">PyText</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../train_your_first_model.html">Train your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../execute_your_first_model.html">Execute your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize_your_model.html">Visualize Model Training with TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytext_models_in_your_app.html">Use PyText models in your app</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serving_models_in_production.html">Serve Models in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_files.html">Config Files Explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_commands.html">Config Commands</a></li>
</ul>
<p class="caption"><span class="caption-text">Training More Advanced Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../atis_tutorial.html">Train Intent-Slot model on ATIS Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hierarchical_intent_slot_tutorial.html">Hierarchical intent and slot filling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../disjoint_multitask_tutorial.html">Multitask training with disjoint datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_training_tutorial.html">Data Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../xlm_r.html">XLM-RoBERTa</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending PyText</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasource_tutorial.html">Custom Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorizer.html">Custom Tensorizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dense.html">Using External Dense Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../create_new_model.html">Creating A New Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hacking_pytext.html">Hacking PyText</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../configs/pytext.html">pytext</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="pytext.html">pytext package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="pytext.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.builtin_task">pytext.builtin_task module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.main">pytext.main module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.workflow">pytext.workflow module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext">Module contents</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="../index.html">Documentation overview</a><ul>
<li><a href="pytext.html">pytext package</a><ul>
<li>Previous: <a href="pytext.metric_reporters.html" title="previous chapter">pytext.metric_reporters package</a></li>
<li>Next: <a href="pytext.models.html" title="next chapter">pytext.models package</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="../search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>