
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<div class="section" id="pytext-models-package">
<h1>pytext.models package<a class="headerlink" href="#pytext-models-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.decoders.html">pytext.models.decoders package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.decoders.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.decoders.html#module-pytext.models.decoders.decoder_base">pytext.models.decoders.decoder_base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.decoders.html#module-pytext.models.decoders.intent_slot_model_decoder">pytext.models.decoders.intent_slot_model_decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.decoders.html#module-pytext.models.decoders.mlp_decoder">pytext.models.decoders.mlp_decoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.decoders.html#module-pytext.models.decoders.mlp_decoder_query_response">pytext.models.decoders.mlp_decoder_query_response module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.decoders.html#module-pytext.models.decoders">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.embeddings.html">pytext.models.embeddings package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.embeddings.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.embeddings.html#module-pytext.models.embeddings.char_embedding">pytext.models.embeddings.char_embedding module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.embeddings.html#module-pytext.models.embeddings.contextual_token_embedding">pytext.models.embeddings.contextual_token_embedding module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.embeddings.html#module-pytext.models.embeddings.dict_embedding">pytext.models.embeddings.dict_embedding module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.embeddings.html#module-pytext.models.embeddings.embedding_base">pytext.models.embeddings.embedding_base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.embeddings.html#module-pytext.models.embeddings.embedding_list">pytext.models.embeddings.embedding_list module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.embeddings.html#module-pytext.models.embeddings.word_embedding">pytext.models.embeddings.word_embedding module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.embeddings.html#module-pytext.models.embeddings">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.ensembles.html">pytext.models.ensembles package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.ensembles.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.ensembles.html#module-pytext.models.ensembles.bagging_doc_ensemble">pytext.models.ensembles.bagging_doc_ensemble module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.ensembles.html#module-pytext.models.ensembles.bagging_intent_slot_ensemble">pytext.models.ensembles.bagging_intent_slot_ensemble module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.ensembles.html#module-pytext.models.ensembles.ensemble">pytext.models.ensembles.ensemble module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.ensembles.html#module-pytext.models.ensembles">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.language_models.html">pytext.models.language_models package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.language_models.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.language_models.html#module-pytext.models.language_models.lmlstm">pytext.models.language_models.lmlstm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.language_models.html#module-pytext.models.language_models">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.output_layers.html">pytext.models.output_layers package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.distance_output_layer">pytext.models.output_layers.distance_output_layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.doc_classification_output_layer">pytext.models.output_layers.doc_classification_output_layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.doc_regression_output_layer">pytext.models.output_layers.doc_regression_output_layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.intent_slot_output_layer">pytext.models.output_layers.intent_slot_output_layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.lm_output_layer">pytext.models.output_layers.lm_output_layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.output_layer_base">pytext.models.output_layers.output_layer_base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.pairwise_ranking_output_layer">pytext.models.output_layers.pairwise_ranking_output_layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.squad_output_layer">pytext.models.output_layers.squad_output_layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.utils">pytext.models.output_layers.utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers.word_tagging_output_layer">pytext.models.output_layers.word_tagging_output_layer module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.output_layers.html#module-pytext.models.output_layers">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.qna.html">pytext.models.qna package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.qna.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.qna.html#module-pytext.models.qna.bert_squad_qa">pytext.models.qna.bert_squad_qa module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.qna.html#module-pytext.models.qna.dr_qa">pytext.models.qna.dr_qa module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.qna.html#module-pytext.models.qna">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.representations.html">pytext.models.representations package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="pytext.models.representations.transformer.html">pytext.models.representations.transformer package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.representations.transformer.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.multihead_attention">pytext.models.representations.transformer.multihead_attention module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.positional_embedding">pytext.models.representations.transformer.positional_embedding module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.residual_mlp">pytext.models.representations.transformer.residual_mlp module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.sentence_encoder">pytext.models.representations.transformer.sentence_encoder module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer.transformer">pytext.models.representations.transformer.transformer module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.representations.transformer.html#module-pytext.models.representations.transformer">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.attention">pytext.models.representations.attention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.augmented_lstm">pytext.models.representations.augmented_lstm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.bilstm">pytext.models.representations.bilstm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.bilstm_doc_attention">pytext.models.representations.bilstm_doc_attention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.bilstm_doc_slot_attention">pytext.models.representations.bilstm_doc_slot_attention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.bilstm_slot_attn">pytext.models.representations.bilstm_slot_attn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.biseqcnn">pytext.models.representations.biseqcnn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.contextual_intent_slot_rep">pytext.models.representations.contextual_intent_slot_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.deepcnn">pytext.models.representations.deepcnn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.docnn">pytext.models.representations.docnn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.huggingface_bert_sentence_encoder">pytext.models.representations.huggingface_bert_sentence_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.jointcnn_rep">pytext.models.representations.jointcnn_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.ordered_neuron_lstm">pytext.models.representations.ordered_neuron_lstm module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.pair_rep">pytext.models.representations.pair_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.pass_through">pytext.models.representations.pass_through module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.pooling">pytext.models.representations.pooling module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.pure_doc_attention">pytext.models.representations.pure_doc_attention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.representation_base">pytext.models.representations.representation_base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.seq_rep">pytext.models.representations.seq_rep module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.slot_attention">pytext.models.representations.slot_attention module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.sparse_transformer_sentence_encoder">pytext.models.representations.sparse_transformer_sentence_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.stacked_bidirectional_rnn">pytext.models.representations.stacked_bidirectional_rnn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.traced_transformer_encoder">pytext.models.representations.traced_transformer_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.transformer_sentence_encoder">pytext.models.representations.transformer_sentence_encoder module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations.transformer_sentence_encoder_base">pytext.models.representations.transformer_sentence_encoder_base module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.representations.html#module-pytext.models.representations">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.semantic_parsers.html">pytext.models.semantic_parsers package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.semantic_parsers.html#subpackages">Subpackages</a><ul>
<li class="toctree-l3"><a class="reference internal" href="pytext.models.semantic_parsers.rnng.html">pytext.models.semantic_parsers.rnng package</a><ul>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.semantic_parsers.rnng.html#submodules">Submodules</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.semantic_parsers.rnng.html#module-pytext.models.semantic_parsers.rnng.rnng_constant">pytext.models.semantic_parsers.rnng.rnng_constant module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.semantic_parsers.rnng.html#module-pytext.models.semantic_parsers.rnng.rnng_data_structures">pytext.models.semantic_parsers.rnng.rnng_data_structures module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.semantic_parsers.rnng.html#module-pytext.models.semantic_parsers.rnng.rnng_parser">pytext.models.semantic_parsers.rnng.rnng_parser module</a></li>
<li class="toctree-l4"><a class="reference internal" href="pytext.models.semantic_parsers.rnng.html#module-pytext.models.semantic_parsers.rnng">Module contents</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.semantic_parsers.html#module-pytext.models.semantic_parsers">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pytext.models.seq_models.html">pytext.models.seq_models package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.seq_models.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.seq_models.html#module-pytext.models.seq_models.contextual_intent_slot">pytext.models.seq_models.contextual_intent_slot module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.seq_models.html#module-pytext.models.seq_models.seqnn">pytext.models.seq_models.seqnn module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.seq_models.html#module-pytext.models.seq_models">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-pytext.models.bert_classification_models">
<span id="pytext-models-bert-classification-models-module"></span><h2>pytext.models.bert_classification_models module<a class="headerlink" href="#module-pytext.models.bert_classification_models" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.bert_classification_models.BertPairwiseModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.bert_classification_models.</code><code class="sig-name descname">BertPairwiseModel</code><span class="sig-paren">(</span><em class="sig-param">encoder1</em>, <em class="sig-param">encoder2</em>, <em class="sig-param">decoder</em>, <em class="sig-param">output_layer</em>, <em class="sig-param">encode_relations</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#BertPairwiseModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.BertPairwiseModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.pair_classification_model.BasePairwiseModel" title="pytext.models.pair_classification_model.BasePairwiseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.pair_classification_model.BasePairwiseModel</span></code></a></p>
<p>Bert Pairwise classification model</p>
<p>The model takes two sets of tokens (left and right), calculates their
representations separately using shared BERT encoder and passes them to
the decoder along with their absolute difference and elementwise product,
all concatenated. Used for e.g. natural language inference.</p>
<dl class="method">
<dt id="pytext.models.bert_classification_models.BertPairwiseModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#BertPairwiseModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.BertPairwiseModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.BertPairwiseModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#BertPairwiseModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.BertPairwiseModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.BertPairwiseModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input_tuple1: Tuple[torch.Tensor, ...], input_tuple2: Tuple[torch.Tensor, ...]</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#BertPairwiseModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.BertPairwiseModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.BertPairwiseModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.bert_classification_models.BertPairwiseModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#BertPairwiseModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.BertPairwiseModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.BertPairwiseModel.save_modules">
<code class="sig-name descname">save_modules</code><span class="sig-paren">(</span><em class="sig-param">base_path: str = ''</em>, <em class="sig-param">suffix: str = ''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#BertPairwiseModel.save_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.BertPairwiseModel.save_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Save each sub-module in separate files for reusing later.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.bert_classification_models.NewBertModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.bert_classification_models.</code><code class="sig-name descname">NewBertModel</code><span class="sig-paren">(</span><em class="sig-param">encoder</em>, <em class="sig-param">decoder</em>, <em class="sig-param">output_layer</em>, <em class="sig-param">stage=&lt;Stage.TRAIN: 'Training'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#NewBertModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.NewBertModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.BaseModel" title="pytext.models.model.BaseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.BaseModel</span></code></a></p>
<p>BERT single sentence classification.</p>
<dl class="attribute">
<dt id="pytext.models.bert_classification_models.NewBertModel.SUPPORT_FP16_OPTIMIZER">
<code class="sig-name descname">SUPPORT_FP16_OPTIMIZER</code><em class="property"> = True</em><a class="headerlink" href="#pytext.models.bert_classification_models.NewBertModel.SUPPORT_FP16_OPTIMIZER" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.NewBertModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#NewBertModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.NewBertModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.NewBertModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#NewBertModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.NewBertModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.NewBertModel.caffe2_export">
<code class="sig-name descname">caffe2_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">tensor_dict</em>, <em class="sig-param">path</em>, <em class="sig-param">export_onnx_path=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#NewBertModel.caffe2_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.NewBertModel.caffe2_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.NewBertModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">encoder_inputs: Tuple[torch.Tensor, ...], *args</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#NewBertModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.NewBertModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.bert_classification_models.NewBertModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.bert_classification_models.NewBertModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_classification_models.html#NewBertModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_classification_models.NewBertModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.bert_regression_model">
<span id="pytext-models-bert-regression-model-module"></span><h2>pytext.models.bert_regression_model module<a class="headerlink" href="#module-pytext.models.bert_regression_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.bert_regression_model.NewBertRegressionModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.bert_regression_model.</code><code class="sig-name descname">NewBertRegressionModel</code><span class="sig-paren">(</span><em class="sig-param">encoder</em>, <em class="sig-param">decoder</em>, <em class="sig-param">output_layer</em>, <em class="sig-param">stage=&lt;Stage.TRAIN: 'Training'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_regression_model.html#NewBertRegressionModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_regression_model.NewBertRegressionModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.bert_classification_models.NewBertModel" title="pytext.models.bert_classification_models.NewBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.bert_classification_models.NewBertModel</span></code></a></p>
<p>BERT single sentence (or concatenated sentences) regression.</p>
<dl class="method">
<dt id="pytext.models.bert_regression_model.NewBertRegressionModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.bert_regression_model.NewBertRegressionModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/bert_regression_model.html#NewBertRegressionModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.bert_regression_model.NewBertRegressionModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.crf">
<span id="pytext-models-crf-module"></span><h2>pytext.models.crf module<a class="headerlink" href="#module-pytext.models.crf" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.crf.CRF">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.crf.</code><code class="sig-name descname">CRF</code><span class="sig-paren">(</span><em class="sig-param">num_tags: int</em>, <em class="sig-param">ignore_index: int</em>, <em class="sig-param">default_label_pad_index: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/crf.html#CRF"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.crf.CRF" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code></p>
<p>Compute the log-likelihood of the input assuming a conditional random field
model.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>num_tags</strong> – The number of tags</p>
</dd>
</dl>
<dl class="method">
<dt id="pytext.models.crf.CRF.decode">
<code class="sig-name descname">decode</code><span class="sig-paren">(</span><em class="sig-param">emissions: torch.Tensor</em>, <em class="sig-param">seq_lens: torch.Tensor</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/crf.html#CRF.decode"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.crf.CRF.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Given a set of emission probabilities, return the predicted tags.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>emissions</strong> – Emission probabilities with expected shape of
batch_size * seq_len * num_labels</p></li>
<li><p><strong>seq_lens</strong> – Length of each input.</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.crf.CRF.export_to_caffe2">
<code class="sig-name descname">export_to_caffe2</code><span class="sig-paren">(</span><em class="sig-param">workspace</em>, <em class="sig-param">init_net</em>, <em class="sig-param">predict_net</em>, <em class="sig-param">logits_output_name</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/crf.html#CRF.export_to_caffe2"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.crf.CRF.export_to_caffe2" title="Permalink to this definition">¶</a></dt>
<dd><p>Exports the crf layer to caffe2 by manually adding the necessary operators
to the init_net and predict net.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>init_net</strong> – caffe2 init net created by the current graph</p></li>
<li><p><strong>predict_net</strong> – caffe2 net created by the current graph</p></li>
<li><p><strong>workspace</strong> – caffe2 current workspace</p></li>
<li><p><strong>output_names</strong> – current output names of the caffe2 net</p></li>
<li><p><strong>py_model</strong> – original pytorch model object</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The updated predictions blob name</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>string</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.crf.CRF.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">emissions: torch.Tensor</em>, <em class="sig-param">tags: torch.Tensor</em>, <em class="sig-param">reduce: bool = True</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/crf.html#CRF.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.crf.CRF.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute log-likelihood of input.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>emissions</strong> – Emission values for different tags for each input. The
expected shape is batch_size * seq_len * num_labels. Padding is
should be on the right side of the input.</p></li>
<li><p><strong>tags</strong> – Actual tags for each token in the input. Expected shape is
batch_size * seq_len</p></li>
</ul>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.crf.CRF.get_transitions">
<code class="sig-name descname">get_transitions</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/crf.html#CRF.get_transitions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.crf.CRF.get_transitions" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.crf.CRF.reset_parameters">
<code class="sig-name descname">reset_parameters</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/models/crf.html#CRF.reset_parameters"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.crf.CRF.reset_parameters" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.crf.CRF.set_transitions">
<code class="sig-name descname">set_transitions</code><span class="sig-paren">(</span><em class="sig-param">transitions: torch.Tensor = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/crf.html#CRF.set_transitions"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.crf.CRF.set_transitions" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.disjoint_multitask_model">
<span id="pytext-models-disjoint-multitask-model-module"></span><h2>pytext.models.disjoint_multitask_model module<a class="headerlink" href="#module-pytext.models.disjoint_multitask_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.disjoint_multitask_model.DisjointMultitaskModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.disjoint_multitask_model.</code><code class="sig-name descname">DisjointMultitaskModel</code><span class="sig-paren">(</span><em class="sig-param">models</em>, <em class="sig-param">loss_weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#DisjointMultitaskModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.Model" title="pytext.models.model.Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.Model</span></code></a></p>
<p>Wrapper model to train multiple PyText models that share parameters.
Designed to be used for multi-tasking when the tasks have disjoint datasets.</p>
<p>Modules which have the same shared_module_key and type share parameters.
Only need to configure the first such module in full in each case.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>models</strong> (<em>type</em>) – Dictionary of models of sub-tasks.</p>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.disjoint_multitask_model.DisjointMultitaskModel.current_model">
<code class="sig-name descname">current_model</code><a class="headerlink" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel.current_model" title="Permalink to this definition">¶</a></dt>
<dd><p>Current model to route the input batch to.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>type</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.DisjointMultitaskModel.contextualize">
<code class="sig-name descname">contextualize</code><span class="sig-paren">(</span><em class="sig-param">context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#DisjointMultitaskModel.contextualize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel.contextualize" title="Permalink to this definition">¶</a></dt>
<dd><p>Add additional context into model. <cite>context</cite> can be anything that
helps maintaining/updating state. For example, it is used by
<a class="reference internal" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel" title="pytext.models.disjoint_multitask_model.DisjointMultitaskModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">DisjointMultitaskModel</span></code></a> for changing the task that should be
trained with a given iterator.</p>
</dd></dl>
<dl class="method">
<dt>
<em class="property">property </em><code class="sig-name descname">current_model</code></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.DisjointMultitaskModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#DisjointMultitaskModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.DisjointMultitaskModel.get_loss">
<code class="sig-name descname">get_loss</code><span class="sig-paren">(</span><em class="sig-param">logits</em>, <em class="sig-param">targets</em>, <em class="sig-param">context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#DisjointMultitaskModel.get_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel.get_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.DisjointMultitaskModel.get_pred">
<code class="sig-name descname">get_pred</code><span class="sig-paren">(</span><em class="sig-param">logits</em>, <em class="sig-param">targets=None</em>, <em class="sig-param">context=None</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#DisjointMultitaskModel.get_pred"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel.get_pred" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.DisjointMultitaskModel.save_modules">
<code class="sig-name descname">save_modules</code><span class="sig-paren">(</span><em class="sig-param">base_path</em>, <em class="sig-param">suffix=''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#DisjointMultitaskModel.save_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel.save_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Save each sub-module in separate files for reusing later.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.disjoint_multitask_model.</code><code class="sig-name descname">NewDisjointMultitaskModel</code><span class="sig-paren">(</span><em class="sig-param">models</em>, <em class="sig-param">loss_weights</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#NewDisjointMultitaskModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.disjoint_multitask_model.DisjointMultitaskModel" title="pytext.models.disjoint_multitask_model.DisjointMultitaskModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.disjoint_multitask_model.DisjointMultitaskModel</span></code></a></p>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel.arrange_model_context">
<code class="sig-name descname">arrange_model_context</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#NewDisjointMultitaskModel.arrange_model_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel.arrange_model_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#NewDisjointMultitaskModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#NewDisjointMultitaskModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel.caffe2_export">
<code class="sig-name descname">caffe2_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">tensor_dict</em>, <em class="sig-param">path</em>, <em class="sig-param">export_onnx_path=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/disjoint_multitask_model.html#NewDisjointMultitaskModel.caffe2_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.disjoint_multitask_model.NewDisjointMultitaskModel.caffe2_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.distributed_model">
<span id="pytext-models-distributed-model-module"></span><h2>pytext.models.distributed_model module<a class="headerlink" href="#module-pytext.models.distributed_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.distributed_model.DistributedModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.distributed_model.</code><code class="sig-name descname">DistributedModel</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/distributed_model.html#DistributedModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.distributed_model.DistributedModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.parallel.distributed.DistributedDataParallel</span></code></p>
<p>Wrapper model class to train models in distributed data parallel manner.
The way to use this class to train your module in distributed manner is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">distributed_model</span> <span class="o">=</span> <span class="n">DistributedModel</span><span class="p">(</span>
    <span class="n">module</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">device_id0</span><span class="p">,</span> <span class="n">device_id1</span><span class="p">],</span>
    <span class="n">output_device</span><span class="o">=</span><span class="n">device_id0</span><span class="p">,</span>
    <span class="n">broadcast_buffers</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>where, <cite>model</cite> is the object of the actual model class you want to train in
distributed manner.</p>
<dl class="method">
<dt id="pytext.models.distributed_model.DistributedModel.cpu">
<code class="sig-name descname">cpu</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/distributed_model.html#DistributedModel.cpu"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.distributed_model.DistributedModel.cpu" title="Permalink to this definition">¶</a></dt>
<dd><p>Moves all model parameters and buffers to the CPU.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>self</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>Module</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.distributed_model.DistributedModel.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><em class="sig-param">stage=&lt;Stage.TEST: 'Test'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/distributed_model.html#DistributedModel.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.distributed_model.DistributedModel.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to set stage</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.distributed_model.DistributedModel.load_state_dict">
<code class="sig-name descname">load_state_dict</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/distributed_model.html#DistributedModel.load_state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.distributed_model.DistributedModel.load_state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Copies parameters and buffers from <a class="reference internal" href="#pytext.models.distributed_model.DistributedModel.state_dict" title="pytext.models.distributed_model.DistributedModel.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> into
this module and its descendants. If <code class="xref py py-attr docutils literal notranslate"><span class="pre">strict</span></code> is <code class="docutils literal notranslate"><span class="pre">True</span></code>, then
the keys of <a class="reference internal" href="#pytext.models.distributed_model.DistributedModel.state_dict" title="pytext.models.distributed_model.DistributedModel.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> must exactly match the keys returned
by this module’s <code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>state_dict</strong> (<em>dict</em>) – a dict containing parameters and
persistent buffers.</p></li>
<li><p><strong>strict</strong> (<em>bool</em><em>, </em><em>optional</em>) – whether to strictly enforce that the keys
in <a class="reference internal" href="#pytext.models.distributed_model.DistributedModel.state_dict" title="pytext.models.distributed_model.DistributedModel.state_dict"><code class="xref py py-attr docutils literal notranslate"><span class="pre">state_dict</span></code></a> match the keys returned by this module’s
<code class="xref py py-meth docutils literal notranslate"><span class="pre">state_dict()</span></code> function. Default: <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><ul class="simple">
<li><p><strong>missing_keys</strong> is a list of str containing the missing keys</p></li>
<li><p><strong>unexpected_keys</strong> is a list of str containing the unexpected keys</p></li>
</ul>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="docutils literal notranslate"><span class="pre">NamedTuple</span></code> with <code class="docutils literal notranslate"><span class="pre">missing_keys</span></code> and <code class="docutils literal notranslate"><span class="pre">unexpected_keys</span></code> fields</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.distributed_model.DistributedModel.state_dict">
<code class="sig-name descname">state_dict</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/distributed_model.html#DistributedModel.state_dict"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.distributed_model.DistributedModel.state_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a dictionary containing a whole state of the module.</p>
<p>Both parameters and persistent buffers (e.g. running averages) are
included. Keys are corresponding parameter and buffer names.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>a dictionary containing a whole state of the module</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p>dict</p>
</dd>
</dl>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
<span class="go">['bias', 'weight']</span>
</pre></div>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.distributed_model.DistributedModel.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">mode=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/distributed_model.html#DistributedModel.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.distributed_model.DistributedModel.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to set stage</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.doc_model">
<span id="pytext-models-doc-model-module"></span><h2>pytext.models.doc_model module<a class="headerlink" href="#module-pytext.models.doc_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.doc_model.ByteTokensDocumentModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.doc_model.</code><code class="sig-name descname">ByteTokensDocumentModel</code><span class="sig-paren">(</span><em class="sig-param">embedding: pytext.models.embeddings.embedding_base.EmbeddingBase</em>, <em class="sig-param">representation: pytext.models.representations.representation_base.RepresentationBase</em>, <em class="sig-param">decoder: pytext.models.decoders.decoder_base.DecoderBase</em>, <em class="sig-param">output_layer: pytext.models.output_layers.output_layer_base.OutputLayerBase</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#ByteTokensDocumentModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.ByteTokensDocumentModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.doc_model.DocModel" title="pytext.models.doc_model.DocModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.doc_model.DocModel</span></code></a></p>
<p>DocModel that receives both word IDs and byte IDs as inputs (concatenating
word and byte-token embeddings to represent input tokens).</p>
<dl class="method">
<dt id="pytext.models.doc_model.ByteTokensDocumentModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#ByteTokensDocumentModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.ByteTokensDocumentModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.ByteTokensDocumentModel.create_embedding">
<em class="property">classmethod </em><code class="sig-name descname">create_embedding</code><span class="sig-paren">(</span><em class="sig-param">config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#ByteTokensDocumentModel.create_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.ByteTokensDocumentModel.create_embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.ByteTokensDocumentModel.get_export_input_names">
<code class="sig-name descname">get_export_input_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#ByteTokensDocumentModel.get_export_input_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.ByteTokensDocumentModel.get_export_input_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.ByteTokensDocumentModel.torchscriptify">
<code class="sig-name descname">torchscriptify</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">traced_model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#ByteTokensDocumentModel.torchscriptify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.ByteTokensDocumentModel.torchscriptify" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.doc_model.DocModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.doc_model.</code><code class="sig-name descname">DocModel</code><span class="sig-paren">(</span><em class="sig-param">embedding: pytext.models.embeddings.embedding_base.EmbeddingBase</em>, <em class="sig-param">representation: pytext.models.representations.representation_base.RepresentationBase</em>, <em class="sig-param">decoder: pytext.models.decoders.decoder_base.DecoderBase</em>, <em class="sig-param">output_layer: pytext.models.output_layers.output_layer_base.OutputLayerBase</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.Model" title="pytext.models.model.Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.Model</span></code></a></p>
<p>DocModel that’s compatible with the new Model abstraction, which is responsible
for describing which inputs it expects and arranging its input tensors.</p>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.caffe2_export">
<code class="sig-name descname">caffe2_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">tensor_dict</em>, <em class="sig-param">path</em>, <em class="sig-param">export_onnx_path=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.caffe2_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.caffe2_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.create_decoder">
<em class="property">classmethod </em><code class="sig-name descname">create_decoder</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.doc_model.DocModel.Config</em>, <em class="sig-param">representation_dim: int</em>, <em class="sig-param">num_labels: int</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.create_decoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.create_decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.create_embedding">
<em class="property">classmethod </em><code class="sig-name descname">create_embedding</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.doc_model.DocModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.create_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.create_embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.doc_model.DocModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.get_export_input_names">
<code class="sig-name descname">get_export_input_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.get_export_input_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.get_export_input_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.get_export_output_names">
<code class="sig-name descname">get_export_output_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.get_export_output_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.get_export_output_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.torchscriptify">
<code class="sig-name descname">torchscriptify</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">traced_model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.torchscriptify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.torchscriptify" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.DocModel.vocab_to_export">
<code class="sig-name descname">vocab_to_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocModel.vocab_to_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocModel.vocab_to_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.doc_model.DocRegressionModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.doc_model.</code><code class="sig-name descname">DocRegressionModel</code><span class="sig-paren">(</span><em class="sig-param">embedding: pytext.models.embeddings.embedding_base.EmbeddingBase</em>, <em class="sig-param">representation: pytext.models.representations.representation_base.RepresentationBase</em>, <em class="sig-param">decoder: pytext.models.decoders.decoder_base.DecoderBase</em>, <em class="sig-param">output_layer: pytext.models.output_layers.output_layer_base.OutputLayerBase</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocRegressionModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocRegressionModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.doc_model.DocModel" title="pytext.models.doc_model.DocModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.doc_model.DocModel</span></code></a></p>
<p>Model that’s compatible with the new Model abstraction, and is configured for
regression tasks (specifically for labels, predictions, and loss).</p>
<dl class="method">
<dt id="pytext.models.doc_model.DocRegressionModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.doc_model.DocRegressionModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#DocRegressionModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.DocRegressionModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.doc_model.PersonalizedDocModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.doc_model.</code><code class="sig-name descname">PersonalizedDocModel</code><span class="sig-paren">(</span><em class="sig-param">embedding: pytext.models.embeddings.embedding_base.EmbeddingBase</em>, <em class="sig-param">representation: pytext.models.representations.representation_base.RepresentationBase</em>, <em class="sig-param">decoder: pytext.models.decoders.decoder_base.DecoderBase</em>, <em class="sig-param">output_layer: pytext.models.output_layers.output_layer_base.OutputLayerBase</em>, <em class="sig-param">user_embedding: Optional[pytext.models.embeddings.embedding_base.EmbeddingBase] = None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#PersonalizedDocModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.PersonalizedDocModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.doc_model.DocModel" title="pytext.models.doc_model.DocModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.doc_model.DocModel</span></code></a></p>
<p>DocModel that includes a user embedding which learns user features to produce
personalized prediction. In this class, user-embedding is fed directly to
the decoder (i.e., does not go through the encoders).</p>
<dl class="method">
<dt id="pytext.models.doc_model.PersonalizedDocModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#PersonalizedDocModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.PersonalizedDocModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.PersonalizedDocModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/doc_model.html#PersonalizedDocModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.PersonalizedDocModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.PersonalizedDocModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#PersonalizedDocModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.PersonalizedDocModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.PersonalizedDocModel.get_export_input_names">
<code class="sig-name descname">get_export_input_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#PersonalizedDocModel.get_export_input_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.PersonalizedDocModel.get_export_input_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.PersonalizedDocModel.torchscriptify">
<code class="sig-name descname">torchscriptify</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">traced_model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#PersonalizedDocModel.torchscriptify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.PersonalizedDocModel.torchscriptify" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.doc_model.PersonalizedDocModel.vocab_to_export">
<code class="sig-name descname">vocab_to_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/doc_model.html#PersonalizedDocModel.vocab_to_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.doc_model.PersonalizedDocModel.vocab_to_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.joint_model">
<span id="pytext-models-joint-model-module"></span><h2>pytext.models.joint_model module<a class="headerlink" href="#module-pytext.models.joint_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.joint_model.IntentSlotModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.joint_model.</code><code class="sig-name descname">IntentSlotModel</code><span class="sig-paren">(</span><em class="sig-param">default_doc_loss_weight</em>, <em class="sig-param">default_word_loss_weight</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.Model" title="pytext.models.model.Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.Model</span></code></a></p>
<p>A joint intent-slot model. This is framed as a model to do document
classification model and word tagging tasks where the embedding and text
representation layers are shared for both tasks.</p>
<p>The supported representation layers are based on bidirectional LSTM or CNN.</p>
<p>It can be instantiated just like any other <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<p>This is in the new data handling design involving tensorizers; that is the
difference between this and JointModel</p>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.arrange_model_context">
<code class="sig-name descname">arrange_model_context</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.arrange_model_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.arrange_model_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.caffe2_export">
<code class="sig-name descname">caffe2_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">tensor_dict</em>, <em class="sig-param">path</em>, <em class="sig-param">export_onnx_path=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.caffe2_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.caffe2_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.create_embedding">
<em class="property">classmethod </em><code class="sig-name descname">create_embedding</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.create_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.create_embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.get_export_input_names">
<code class="sig-name descname">get_export_input_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.get_export_input_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.get_export_input_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.get_export_output_names">
<code class="sig-name descname">get_export_output_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.get_export_output_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.get_export_output_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.get_weights_context">
<code class="sig-name descname">get_weights_context</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.get_weights_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.get_weights_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.joint_model.IntentSlotModel.vocab_to_export">
<code class="sig-name descname">vocab_to_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/joint_model.html#IntentSlotModel.vocab_to_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.joint_model.IntentSlotModel.vocab_to_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.masked_lm">
<span id="pytext-models-masked-lm-module"></span><h2>pytext.models.masked_lm module<a class="headerlink" href="#module-pytext.models.masked_lm" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.masked_lm.MaskedLanguageModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.masked_lm.</code><code class="sig-name descname">MaskedLanguageModel</code><span class="sig-paren">(</span><em class="sig-param">encoder: pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase</em>, <em class="sig-param">decoder: pytext.models.decoders.mlp_decoder.MLPDecoder</em>, <em class="sig-param">output_layer: pytext.models.output_layers.lm_output_layer.LMOutputLayer</em>, <em class="sig-param">token_tensorizer: pytext.data.bert_tensorizer.BERTTensorizerBase</em>, <em class="sig-param">vocab: pytext.data.utils.Vocabulary</em>, <em class="sig-param">mask_prob: float = 0.15</em>, <em class="sig-param">mask_bos: float = False</em>, <em class="sig-param">masking_strategy: pytext.models.masking_utils.MaskingStrategy = &lt;MaskingStrategy.RANDOM: 'random'&gt;</em>, <em class="sig-param">stage: pytext.common.constants.Stage = &lt;Stage.TRAIN: 'Training'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/masked_lm.html#MaskedLanguageModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.masked_lm.MaskedLanguageModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.BaseModel" title="pytext.models.model.BaseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.BaseModel</span></code></a></p>
<p>Masked language model for BERT style pre-training.</p>
<dl class="attribute">
<dt id="pytext.models.masked_lm.MaskedLanguageModel.SUPPORT_FP16_OPTIMIZER">
<code class="sig-name descname">SUPPORT_FP16_OPTIMIZER</code><em class="property"> = True</em><a class="headerlink" href="#pytext.models.masked_lm.MaskedLanguageModel.SUPPORT_FP16_OPTIMIZER" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.masked_lm.MaskedLanguageModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/masked_lm.html#MaskedLanguageModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.masked_lm.MaskedLanguageModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.masked_lm.MaskedLanguageModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/masked_lm.html#MaskedLanguageModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.masked_lm.MaskedLanguageModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.masked_lm.MaskedLanguageModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/masked_lm.html#MaskedLanguageModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.masked_lm.MaskedLanguageModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.masked_lm.MaskedLanguageModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.masked_lm.MaskedLanguageModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/masked_lm.html#MaskedLanguageModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.masked_lm.MaskedLanguageModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.masking_utils">
<span id="pytext-models-masking-utils-module"></span><h2>pytext.models.masking_utils module<a class="headerlink" href="#module-pytext.models.masking_utils" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.masking_utils.MaskingStrategy">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.masking_utils.</code><code class="sig-name descname">MaskingStrategy</code><a class="reference internal" href="../_modules/pytext/models/masking_utils.html#MaskingStrategy"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.masking_utils.MaskingStrategy" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">enum.Enum</span></code></p>
<p>An enumeration.</p>
<dl class="attribute">
<dt id="pytext.models.masking_utils.MaskingStrategy.FREQUENCY">
<code class="sig-name descname">FREQUENCY</code><em class="property"> = 'frequency_based'</em><a class="headerlink" href="#pytext.models.masking_utils.MaskingStrategy.FREQUENCY" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.masking_utils.MaskingStrategy.RANDOM">
<code class="sig-name descname">RANDOM</code><em class="property"> = 'random'</em><a class="headerlink" href="#pytext.models.masking_utils.MaskingStrategy.RANDOM" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="function">
<dt id="pytext.models.masking_utils.frequency_based_masking">
<code class="sig-prename descclassname">pytext.models.masking_utils.</code><code class="sig-name descname">frequency_based_masking</code><span class="sig-paren">(</span><em class="sig-param">tokens: None._VariableFunctions.tensor</em>, <em class="sig-param">token_sampling_weights: numpy.ndarray</em>, <em class="sig-param">mask_prob: float</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/masking_utils.html#frequency_based_masking"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.masking_utils.frequency_based_masking" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to mask tokens based on frequency.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ol class="arabic simple">
<li><p>tokens: Tensor with token ids of shape (batch_size x seq_len)</p></li>
<li><dl class="simple">
<dt>token_sampling_weights: numpy array with shape (batch_size x seq_len)</dt><dd><p>and each element representing the sampling weight assicated with
the corresponding token in tokens</p>
</dd>
</dl>
</li>
<li><p>mask_prob: Probability of masking a particular token</p></li>
</ol>
</dd>
<dt>Outputs:</dt><dd><dl class="simple">
<dt>mask: Tensor with same shape as input tokens (batch_size x seq_len)</dt><dd><p>with masked  tokens represented by a 1 and everything else as 0.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
<dl class="function">
<dt id="pytext.models.masking_utils.random_masking">
<code class="sig-prename descclassname">pytext.models.masking_utils.</code><code class="sig-name descname">random_masking</code><span class="sig-paren">(</span><em class="sig-param">tokens: None._VariableFunctions.tensor</em>, <em class="sig-param">mask_prob: float</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/masking_utils.html#random_masking"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.masking_utils.random_masking" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to mask tokens randomly.</p>
<dl class="simple">
<dt>Inputs:</dt><dd><ol class="arabic simple">
<li><p>tokens: Tensor with token ids of shape (batch_size x seq_len)</p></li>
<li><p>mask_prob: Probability of masking a particular token</p></li>
</ol>
</dd>
<dt>Outputs:</dt><dd><dl class="simple">
<dt>mask: Tensor with same shape as input tokens (batch_size x seq_len)</dt><dd><p>with masked  tokens represented by a 1 and everything else as 0.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.model">
<span id="pytext-models-model-module"></span><h2>pytext.models.model module<a class="headerlink" href="#module-pytext.models.model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.model.BaseModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.model.</code><code class="sig-name descname">BaseModel</code><span class="sig-paren">(</span><em class="sig-param">stage: pytext.common.constants.Stage = &lt;Stage.TRAIN: 'Training'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference internal" href="pytext.config.html#pytext.config.component.Component" title="pytext.config.component.Component"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.config.component.Component</span></code></a></p>
<p>Base model class which inherits from nn.Module. Also has a stage flag to
indicate it’s in <cite>train</cite>, <cite>eval</cite>, or <cite>test</cite> stage.
This is because the built-in train/eval flag in PyTorch can’t distinguish eval
and test, which is required to support some use cases.</p>
<dl class="attribute">
<dt id="pytext.models.model.BaseModel.SUPPORT_FP16_OPTIMIZER">
<code class="sig-name descname">SUPPORT_FP16_OPTIMIZER</code><em class="property"> = False</em><a class="headerlink" href="#pytext.models.model.BaseModel.SUPPORT_FP16_OPTIMIZER" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.arrange_model_context">
<code class="sig-name descname">arrange_model_context</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.arrange_model_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.arrange_model_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.caffe2_export">
<code class="sig-name descname">caffe2_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">tensor_dict</em>, <em class="sig-param">path</em>, <em class="sig-param">export_onnx_path=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.caffe2_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.caffe2_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.contextualize">
<code class="sig-name descname">contextualize</code><span class="sig-paren">(</span><em class="sig-param">context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.contextualize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.contextualize" title="Permalink to this definition">¶</a></dt>
<dd><p>Add additional context into model. <cite>context</cite> can be anything that
helps maintaining/updating state. For example, it is used by
<code class="xref py py-class docutils literal notranslate"><span class="pre">DisjointMultitaskModel</span></code> for changing the task that should be
trained with a given iterator.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><em class="sig-param">stage=&lt;Stage.TEST: 'Test'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to explicitly maintain the stage (train, eval, test).</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.get_loss">
<code class="sig-name descname">get_loss</code><span class="sig-paren">(</span><em class="sig-param">logit</em>, <em class="sig-param">target</em>, <em class="sig-param">context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.get_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.get_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.get_param_groups_for_optimizer">
<code class="sig-name descname">get_param_groups_for_optimizer</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → List[Dict[str, List[torch.nn.parameter.Parameter]]]<a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.get_param_groups_for_optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.get_param_groups_for_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of parameter groups of the format {“params”: param_list}.
The parameter groups loosely correspond to layers and are ordered from low
to high. Currently, only the embedding layer can provide multiple param groups,
and other layers are put into one param group. The output of this method
is passed to the optimizer so that schedulers can change learning rates
by layer.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.get_pred">
<code class="sig-name descname">get_pred</code><span class="sig-paren">(</span><em class="sig-param">logit</em>, <em class="sig-param">target=None</em>, <em class="sig-param">context=None</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.get_pred"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.get_pred" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.prepare_for_onnx_export_">
<code class="sig-name descname">prepare_for_onnx_export_</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.prepare_for_onnx_export_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.prepare_for_onnx_export_" title="Permalink to this definition">¶</a></dt>
<dd><p>Make model exportable via ONNX trace.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.quantize">
<code class="sig-name descname">quantize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.quantize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantize the model during export.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.save_modules">
<code class="sig-name descname">save_modules</code><span class="sig-paren">(</span><em class="sig-param">base_path: str = ''</em>, <em class="sig-param">suffix: str = ''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.save_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.save_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Save each sub-module in separate files for reusing later.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">mode=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to explicitly maintain the stage (train, eval, test).</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.BaseModel.train_batch">
<em class="property">classmethod </em><code class="sig-name descname">train_batch</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">batch</em>, <em class="sig-param">state=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.train_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.BaseModel.train_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.model.Model">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.model.</code><code class="sig-name descname">Model</code><span class="sig-paren">(</span><em class="sig-param">embedding: pytext.models.embeddings.embedding_base.EmbeddingBase</em>, <em class="sig-param">representation: pytext.models.representations.representation_base.RepresentationBase</em>, <em class="sig-param">decoder: pytext.models.decoders.decoder_base.DecoderBase</em>, <em class="sig-param">output_layer: pytext.models.output_layers.output_layer_base.OutputLayerBase</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#Model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.Model" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.BaseModel" title="pytext.models.model.BaseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.BaseModel</span></code></a></p>
<p>Generic single-task model class that expects four components:</p>
<ol class="arabic simple">
<li><p><cite>Embedding</cite></p></li>
<li><p><cite>Representation</cite></p></li>
<li><p><cite>Decoder</cite></p></li>
<li><p><cite>Output Layer</cite></p></li>
</ol>
<p>Forward pass: <cite>embedding -&gt; representation -&gt; decoder -&gt; output_layer</cite></p>
<p>These four components have specific responsibilities as described below.</p>
<p><cite>Embedding</cite> layer should implement the way to represent each token in the
input text. It can be as simple as just token/word embedding or can be
composed of multiple ways to represent a token, e.g., word embedding,
character embedding, etc.</p>
<p><cite>Representation</cite> layer should implement the way to encode the entire input
text such that the output vector(s) can be used by decoder to produce logits.
There is no restriction on the number of inputs it should encode. There is
also not restriction on the number of ways to encode input.</p>
<p><cite>Decoder</cite> layer should implement the way to consume the output of model’s
representation and produce logits that can be used by the output layer to
compute loss or generate predictions (and prediction scores/confidence)</p>
<p><cite>Output layer</cite> should implement the way loss computation is done as well as
the logic to generate predictions from the logits.</p>
<p>Let us discuss the joint intent-slot model as a case to go over these layers.
The model predicts intent of input utterance and the slots in the utterance.
(Refer to <a class="reference internal" href="../atis_tutorial.html"><span class="doc">Train Intent-Slot model on ATIS Dataset</span></a> for details about intent-slot model.)</p>
<ol class="arabic simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingList</span></code> layer is tasked with representing tokens. To do so we
can use learnable word embedding table in conjunction with learnable character
embedding table that are distilled to token level representation using CNN and
pooling.
Note: This class is meant to be reused by all models. It acts as a container
of all the different ways of representing a token/word.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BiLSTMDocSlotAttention</span></code> is tasked with encoding the embedded input
string for intent classification and slot filling. In order to do that it has a
shared bidirectional LSTM layer followed by separate attention layers for
document level attention and word level attention. Finally it produces two
vectors per utterance.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">IntentSlotModelDecoder</span></code> accepts the two input vectors from
<cite>BiLSTMDocSlotAttention</cite> and produces logits for intent classification and
slot filling. Conditioned on a flag it can also use the probabilities from
intent classification for slot filling.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">IntentSlotOutputLayer</span></code> implements the logic behind computing loss and
prediction, as well as, how to export this layer to export to Caffe2. This is
used by model exporter as a post-processing Caffe2 operator.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedding</strong> (<em>EmbeddingBase</em>) – Description of parameter <cite>embedding</cite>.</p></li>
<li><p><strong>representation</strong> (<em>RepresentationBase</em>) – Description of parameter <cite>representation</cite>.</p></li>
<li><p><strong>decoder</strong> (<em>DecoderBase</em>) – Description of parameter <cite>decoder</cite>.</p></li>
<li><p><strong>output_layer</strong> (<em>OutputLayerBase</em>) – Description of parameter <cite>output_layer</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.model.Model.embedding">
<code class="sig-name descname">embedding</code><a class="headerlink" href="#pytext.models.model.Model.embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.model.Model.representation">
<code class="sig-name descname">representation</code><a class="headerlink" href="#pytext.models.model.Model.representation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.model.Model.decoder">
<code class="sig-name descname">decoder</code><a class="headerlink" href="#pytext.models.model.Model.decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.model.Model.output_layer">
<code class="sig-name descname">output_layer</code><a class="headerlink" href="#pytext.models.model.Model.output_layer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.Model.compose_embedding">
<em class="property">classmethod </em><code class="sig-name descname">compose_embedding</code><span class="sig-paren">(</span><em class="sig-param">sub_emb_module_dict: Dict[str, pytext.models.embeddings.embedding_base.EmbeddingBase], metadata</em><span class="sig-paren">)</span> → pytext.models.embeddings.embedding_list.EmbeddingList<a class="reference internal" href="../_modules/pytext/models/model.html#Model.compose_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.Model.compose_embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Default implementation is to compose an instance of
<code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingList</span></code> with all the sub-embedding modules. You should
override this class method if you want to implement a specific way to
embed tokens/words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sub_emb_module_dict</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>EmbeddingBase</em><em>]</em>) – Named dictionary of
embedding modules each of which implement a way to embed/encode
a token.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingList</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>EmbeddingList</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.Model.create_embedding">
<em class="property">classmethod </em><code class="sig-name descname">create_embedding</code><span class="sig-paren">(</span><em class="sig-param">feat_config: pytext.config.field_config.FeatureConfig</em>, <em class="sig-param">metadata: pytext.data.data_handler.CommonMetadata</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#Model.create_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.Model.create_embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.model.Model.create_sub_embs">
<em class="property">classmethod </em><code class="sig-name descname">create_sub_embs</code><span class="sig-paren">(</span><em class="sig-param">emb_config: pytext.config.field_config.FeatureConfig</em>, <em class="sig-param">metadata: pytext.data.data_handler.CommonMetadata</em><span class="sig-paren">)</span> → Dict[str, pytext.models.embeddings.embedding_base.EmbeddingBase]<a class="reference internal" href="../_modules/pytext/models/model.html#Model.create_sub_embs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.Model.create_sub_embs" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the embedding modules defined in the <cite>emb_config</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>emb_config</strong> (<em>FeatureConfig</em>) – Object containing all the sub-embedding
configurations.</p></li>
<li><p><strong>metadata</strong> (<em>CommonMetadata</em>) – Object containing features and label metadata.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Named dictionary of embedding modules.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Dict[str, EmbeddingBase]</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.Model.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/model.html#Model.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.Model.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.model.Model.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.model.Model.Config</em>, <em class="sig-param">feat_config: pytext.config.field_config.FeatureConfig</em>, <em class="sig-param">metadata: pytext.data.data_handler.CommonMetadata</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#Model.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.Model.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.model.ModelInputBase">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.model.</code><code class="sig-name descname">ModelInputBase</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#ModelInputBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.ModelInputBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.config.html#pytext.config.pytext_config.ConfigBase" title="pytext.config.pytext_config.ConfigBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.config.pytext_config.ConfigBase</span></code></a></p>
<p>Base class for model inputs.</p>
</dd></dl>
<dl class="class">
<dt id="pytext.models.model.ModelInputMeta">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.model.</code><code class="sig-name descname">ModelInputMeta</code><a class="reference internal" href="../_modules/pytext/models/model.html#ModelInputMeta"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.model.ModelInputMeta" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.config.html#pytext.config.pytext_config.ConfigBaseMeta" title="pytext.config.pytext_config.ConfigBaseMeta"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.config.pytext_config.ConfigBaseMeta</span></code></a></p>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.module">
<span id="pytext-models-module-module"></span><h2>pytext.models.module module<a class="headerlink" href="#module-pytext.models.module" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.module.Module">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.module.</code><code class="sig-name descname">Module</code><span class="sig-paren">(</span><em class="sig-param">config=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/module.html#Module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.module.Module" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference internal" href="pytext.config.html#pytext.config.component.Component" title="pytext.config.component.Component"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.config.component.Component</span></code></a></p>
<p>Generic module class that serves as base class for all PyText modules.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>config</strong> (<em>type</em>) – Module’s <cite>config</cite> object. Specific contents of this object
depends on the module. Defaults to None.</p>
</dd>
</dl>
<dl class="method">
<dt id="pytext.models.module.Module.freeze">
<code class="sig-name descname">freeze</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → None<a class="reference internal" href="../_modules/pytext/models/module.html#Module.freeze"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.module.Module.freeze" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="function">
<dt id="pytext.models.module.create_module">
<code class="sig-prename descclassname">pytext.models.module.</code><code class="sig-name descname">create_module</code><span class="sig-paren">(</span><em class="sig-param">module_config</em>, <em class="sig-param">*args</em>, <em class="sig-param">create_fn=&lt;function _create_module_from_registry&gt;</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/module.html#create_module"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.module.create_module" title="Permalink to this definition">¶</a></dt>
<dd><p>Create module object given the module’s config object. It depends on the
global shared module registry. Hence, your module must be available for the
registry. This entails that your module must be imported somewhere in the
code path during module creation (ideally in your model class) for the module
to be visible for registry.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module_config</strong> (<em>type</em>) – Module config object.</p></li>
<li><p><strong>create_fn</strong> (<em>type</em>) – The function to use for creating the module. Use this
parameter if your module creation requires custom code and pass your
function here. Defaults to <cite>_create_module_from_registry()</cite>.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Description of returned object.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>type</p>
</dd>
</dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.pair_classification_model">
<span id="pytext-models-pair-classification-model-module"></span><h2>pytext.models.pair_classification_model module<a class="headerlink" href="#module-pytext.models.pair_classification_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.pair_classification_model.BasePairwiseModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.pair_classification_model.</code><code class="sig-name descname">BasePairwiseModel</code><span class="sig-paren">(</span><em class="sig-param">decoder: pytext.models.decoders.decoder_base.DecoderBase</em>, <em class="sig-param">output_layer: pytext.models.output_layers.output_layer_base.OutputLayerBase</em>, <em class="sig-param">encode_relations: bool</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#BasePairwiseModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.BasePairwiseModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.BaseModel" title="pytext.models.model.BaseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.BaseModel</span></code></a></p>
<p>A base classification model that scores a pair of texts.</p>
<p>Subclasses need to implement the from_config, forward and save_modules.</p>
<dl class="method">
<dt id="pytext.models.pair_classification_model.BasePairwiseModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input1: Tuple[torch.Tensor, ...], input2: Tuple[torch.Tensor, ...]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#BasePairwiseModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.BasePairwiseModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.pair_classification_model.BasePairwiseModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.pair_classification_model.BasePairwiseModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#BasePairwiseModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.BasePairwiseModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.pair_classification_model.BasePairwiseModel.save_modules">
<code class="sig-name descname">save_modules</code><span class="sig-paren">(</span><em class="sig-param">base_path: str = ''</em>, <em class="sig-param">suffix: str = ''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#BasePairwiseModel.save_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.BasePairwiseModel.save_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Save each sub-module in separate files for reusing later.</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.pair_classification_model.PairwiseModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.pair_classification_model.</code><code class="sig-name descname">PairwiseModel</code><span class="sig-paren">(</span><em class="sig-param">embeddings: torch.nn.modules.container.ModuleList</em>, <em class="sig-param">representations: torch.nn.modules.container.ModuleList</em>, <em class="sig-param">decoder: pytext.models.decoders.mlp_decoder.MLPDecoder</em>, <em class="sig-param">output_layer: pytext.models.output_layers.doc_classification_output_layer.ClassificationOutputLayer</em>, <em class="sig-param">encode_relations: bool</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#PairwiseModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.PairwiseModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.pair_classification_model.BasePairwiseModel" title="pytext.models.pair_classification_model.BasePairwiseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.pair_classification_model.BasePairwiseModel</span></code></a></p>
<p>A classification model that scores a pair of texts, for example, a model for
natural language inference.</p>
<p>The model shares embedding space (so it doesn’t support
pairs of texts where left and right are in different languages). It uses
bidirectional LSTM or CNN to represent the two documents, and concatenates
them along with their absolute difference and elementwise product. This
concatenated pair representation is passed to a multi-layer perceptron to
decode to label/target space.</p>
<p>See <a class="reference external" href="https://arxiv.org/pdf/1705.02364.pdf">https://arxiv.org/pdf/1705.02364.pdf</a> for more details.</p>
<p>It can be instantiated just like any other <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<dl class="attribute">
<dt id="pytext.models.pair_classification_model.PairwiseModel.EMBEDDINGS">
<code class="sig-name descname">EMBEDDINGS</code><em class="property"> = ['embedding']</em><a class="headerlink" href="#pytext.models.pair_classification_model.PairwiseModel.EMBEDDINGS" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.pair_classification_model.PairwiseModel.INPUTS_PAIR">
<code class="sig-name descname">INPUTS_PAIR</code><em class="property"> = [['tokens1'], ['tokens2']]</em><a class="headerlink" href="#pytext.models.pair_classification_model.PairwiseModel.INPUTS_PAIR" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.pair_classification_model.PairwiseModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#PairwiseModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.PairwiseModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.pair_classification_model.PairwiseModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#PairwiseModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.PairwiseModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.pair_classification_model.PairwiseModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">input1: Tuple[torch.Tensor, ...], input2: Tuple[torch.Tensor, ...]</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#PairwiseModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.PairwiseModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.pair_classification_model.PairwiseModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.pair_classification_model.PairwiseModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#PairwiseModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.PairwiseModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.pair_classification_model.PairwiseModel.save_modules">
<code class="sig-name descname">save_modules</code><span class="sig-paren">(</span><em class="sig-param">base_path: str = ''</em>, <em class="sig-param">suffix: str = ''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/pair_classification_model.html#PairwiseModel.save_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.pair_classification_model.PairwiseModel.save_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Save each sub-module in separate files for reusing later.</p>
</dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.query_document_pairwise_ranking_model">
<span id="pytext-models-query-document-pairwise-ranking-model-module"></span><h2>pytext.models.query_document_pairwise_ranking_model module<a class="headerlink" href="#module-pytext.models.query_document_pairwise_ranking_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.query_document_pairwise_ranking_model.</code><code class="sig-name descname">QueryDocPairwiseRankingModel</code><span class="sig-paren">(</span><em class="sig-param">embeddings: torch.nn.modules.container.ModuleList</em>, <em class="sig-param">representations: torch.nn.modules.container.ModuleList</em>, <em class="sig-param">decoder: pytext.models.decoders.mlp_decoder.MLPDecoder</em>, <em class="sig-param">output_layer: pytext.models.output_layers.doc_classification_output_layer.ClassificationOutputLayer</em>, <em class="sig-param">encode_relations: bool</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/query_document_pairwise_ranking_model.html#QueryDocPairwiseRankingModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.pair_classification_model.PairwiseModel" title="pytext.models.pair_classification_model.PairwiseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.pair_classification_model.PairwiseModel</span></code></a></p>
<p>Pairwise ranking model
This model takes in a query, and two responses (pos_response and neg_response)
It passes representations of the query and the two responses to a decoder
pos_response should be ranked higher than neg_response - this is ensured by training
with a ranking hinge loss function</p>
<dl class="method">
<dt id="pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/query_document_pairwise_ranking_model.html#QueryDocPairwiseRankingModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/query_document_pairwise_ranking_model.html#QueryDocPairwiseRankingModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">pos_response: Tuple[torch.Tensor, torch.Tensor], neg_response: Tuple[torch.Tensor, torch.Tensor], query: Tuple[torch.Tensor, torch.Tensor]</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/query_document_pairwise_ranking_model.html#QueryDocPairwiseRankingModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/query_document_pairwise_ranking_model.html#QueryDocPairwiseRankingModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.query_document_pairwise_ranking_model.QueryDocPairwiseRankingModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.roberta">
<span id="pytext-models-roberta-module"></span><h2>pytext.models.roberta module<a class="headerlink" href="#module-pytext.models.roberta" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.roberta.RoBERTa">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.roberta.</code><code class="sig-name descname">RoBERTa</code><span class="sig-paren">(</span><em class="sig-param">encoder</em>, <em class="sig-param">decoder</em>, <em class="sig-param">output_layer</em>, <em class="sig-param">stage=&lt;Stage.TRAIN: 'Training'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTa"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTa" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.bert_classification_models.NewBertModel" title="pytext.models.bert_classification_models.NewBertModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.bert_classification_models.NewBertModel</span></code></a></p>
<dl class="method">
<dt id="pytext.models.roberta.RoBERTa.torchscriptify">
<code class="sig-name descname">torchscriptify</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">traced_model</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTa.torchscriptify"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTa.torchscriptify" title="Permalink to this definition">¶</a></dt>
<dd><p>Using the traced model, create a ScriptModule which has a nicer API that
includes generating tensors from simple data types, and returns classified
values according to the output layer (eg. as a dict mapping class name to score)</p>
</dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.roberta.RoBERTaEncoder">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.roberta.</code><code class="sig-name descname">RoBERTaEncoder</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.roberta.RoBERTaEncoder.Config</em>, <em class="sig-param">output_encoded_layers: bool</em>, <em class="sig-param">**kwarg</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTaEncoder"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTaEncoder" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.roberta.RoBERTaEncoderBase" title="pytext.models.roberta.RoBERTaEncoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.roberta.RoBERTaEncoderBase</span></code></a></p>
<p>A PyTorch RoBERTa implementation</p>
</dd></dl>
<dl class="class">
<dt id="pytext.models.roberta.RoBERTaEncoderBase">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.roberta.</code><code class="sig-name descname">RoBERTaEncoderBase</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase.Config</em>, <em class="sig-param">output_encoded_layers=False</em>, <em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTaEncoderBase"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTaEncoderBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="pytext.models.representations.html#pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase" title="pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase</span></code></a></p>
</dd></dl>
<dl class="class">
<dt id="pytext.models.roberta.RoBERTaEncoderJit">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.roberta.</code><code class="sig-name descname">RoBERTaEncoderJit</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.roberta.RoBERTaEncoderJit.Config</em>, <em class="sig-param">output_encoded_layers: bool</em>, <em class="sig-param">**kwarg</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTaEncoderJit"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTaEncoderJit" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.roberta.RoBERTaEncoderBase" title="pytext.models.roberta.RoBERTaEncoderBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.roberta.RoBERTaEncoderBase</span></code></a></p>
<p>A TorchScript RoBERTa implementation</p>
</dd></dl>
<dl class="class">
<dt id="pytext.models.roberta.RoBERTaWordTaggingModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.roberta.</code><code class="sig-name descname">RoBERTaWordTaggingModel</code><span class="sig-paren">(</span><em class="sig-param">encoder</em>, <em class="sig-param">decoder</em>, <em class="sig-param">output_layer</em>, <em class="sig-param">stage=&lt;Stage.TRAIN: 'Training'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTaWordTaggingModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTaWordTaggingModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.BaseModel" title="pytext.models.model.BaseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.BaseModel</span></code></a></p>
<p>Single Sentence Token-level Classification Model using XLM.</p>
<dl class="method">
<dt id="pytext.models.roberta.RoBERTaWordTaggingModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTaWordTaggingModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTaWordTaggingModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.roberta.RoBERTaWordTaggingModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTaWordTaggingModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTaWordTaggingModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.roberta.RoBERTaWordTaggingModel.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">encoder_inputs: Tuple[torch.Tensor, ...], *args</em><span class="sig-paren">)</span> → torch.Tensor<a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTaWordTaggingModel.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTaWordTaggingModel.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.roberta.RoBERTaWordTaggingModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.roberta.RoBERTaWordTaggingModel.Config, tensorizers: Dict[str, pytext.data.tensorizers.Tensorizer]</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#RoBERTaWordTaggingModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.RoBERTaWordTaggingModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="function">
<dt id="pytext.models.roberta.init_params">
<code class="sig-prename descclassname">pytext.models.roberta.</code><code class="sig-name descname">init_params</code><span class="sig-paren">(</span><em class="sig-param">module</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/roberta.html#init_params"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.roberta.init_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Initialize the RoBERTa weights for pre-training from scratch.</p>
</dd></dl>
</div>
<div class="section" id="module-pytext.models.word_model">
<span id="pytext-models-word-model-module"></span><h2>pytext.models.word_model module<a class="headerlink" href="#module-pytext.models.word_model" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.word_model.WordTaggingLiteModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.word_model.</code><code class="sig-name descname">WordTaggingLiteModel</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingLiteModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingLiteModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.word_model.WordTaggingModel" title="pytext.models.word_model.WordTaggingModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.word_model.WordTaggingModel</span></code></a></p>
<p>Also a word tagging model, but uses bytes as inputs to the model. Using
bytes instead of words, the model does not need to store a word embedding
table mapping words in the vocab to their embedding vector representations,
but instead compute them on the fly using CharacterEmbedding. This produces
an exported/serialized model that requires much less storage space as well
as less memory during run/inference time.</p>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingLiteModel.arrange_model_context">
<code class="sig-name descname">arrange_model_context</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingLiteModel.arrange_model_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingLiteModel.arrange_model_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingLiteModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingLiteModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingLiteModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingLiteModel.create_embedding">
<em class="property">classmethod </em><code class="sig-name descname">create_embedding</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingLiteModel.create_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingLiteModel.create_embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingLiteModel.get_export_input_names">
<code class="sig-name descname">get_export_input_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingLiteModel.get_export_input_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingLiteModel.get_export_input_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingLiteModel.vocab_to_export">
<code class="sig-name descname">vocab_to_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingLiteModel.vocab_to_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingLiteModel.vocab_to_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.word_model.WordTaggingModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.word_model.</code><code class="sig-name descname">WordTaggingModel</code><span class="sig-paren">(</span><em class="sig-param">*args</em>, <em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.Model" title="pytext.models.model.Model"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.Model</span></code></a></p>
<p>Word tagging model. It can be used for any task that requires predicting the
tag for a word/token. For example, the following tasks can be modeled as word
tagging tasks. This is not an exhaustive list.
1. Part of speech tagging.
2. Named entity recognition.
3. Slot filling for task oriented dialog.</p>
<p>It can be instantiated just like any other <code class="xref py py-class docutils literal notranslate"><span class="pre">Model</span></code>.</p>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.arrange_model_context">
<code class="sig-name descname">arrange_model_context</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.arrange_model_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.arrange_model_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.caffe2_export">
<code class="sig-name descname">caffe2_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">tensor_dict</em>, <em class="sig-param">path</em>, <em class="sig-param">export_onnx_path=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.caffe2_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.caffe2_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.create_embedding">
<em class="property">classmethod </em><code class="sig-name descname">create_embedding</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.create_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.create_embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config</em>, <em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.get_export_input_names">
<code class="sig-name descname">get_export_input_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.get_export_input_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.get_export_input_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.get_export_output_names">
<code class="sig-name descname">get_export_output_names</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.get_export_output_names"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.get_export_output_names" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.word_model.WordTaggingModel.vocab_to_export">
<code class="sig-name descname">vocab_to_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/word_model.html#WordTaggingModel.vocab_to_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.word_model.WordTaggingModel.vocab_to_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
<div class="section" id="module-pytext.models">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-pytext.models" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="pytext.models.Model">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.</code><code class="sig-name descname">Model</code><span class="sig-paren">(</span><em class="sig-param">embedding: pytext.models.embeddings.embedding_base.EmbeddingBase</em>, <em class="sig-param">representation: pytext.models.representations.representation_base.RepresentationBase</em>, <em class="sig-param">decoder: pytext.models.decoders.decoder_base.DecoderBase</em>, <em class="sig-param">output_layer: pytext.models.output_layers.output_layer_base.OutputLayerBase</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#Model"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.Model" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#pytext.models.model.BaseModel" title="pytext.models.model.BaseModel"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.models.model.BaseModel</span></code></a></p>
<p>Generic single-task model class that expects four components:</p>
<ol class="arabic simple">
<li><p><cite>Embedding</cite></p></li>
<li><p><cite>Representation</cite></p></li>
<li><p><cite>Decoder</cite></p></li>
<li><p><cite>Output Layer</cite></p></li>
</ol>
<p>Forward pass: <cite>embedding -&gt; representation -&gt; decoder -&gt; output_layer</cite></p>
<p>These four components have specific responsibilities as described below.</p>
<p><cite>Embedding</cite> layer should implement the way to represent each token in the
input text. It can be as simple as just token/word embedding or can be
composed of multiple ways to represent a token, e.g., word embedding,
character embedding, etc.</p>
<p><cite>Representation</cite> layer should implement the way to encode the entire input
text such that the output vector(s) can be used by decoder to produce logits.
There is no restriction on the number of inputs it should encode. There is
also not restriction on the number of ways to encode input.</p>
<p><cite>Decoder</cite> layer should implement the way to consume the output of model’s
representation and produce logits that can be used by the output layer to
compute loss or generate predictions (and prediction scores/confidence)</p>
<p><cite>Output layer</cite> should implement the way loss computation is done as well as
the logic to generate predictions from the logits.</p>
<p>Let us discuss the joint intent-slot model as a case to go over these layers.
The model predicts intent of input utterance and the slots in the utterance.
(Refer to <a class="reference internal" href="../atis_tutorial.html"><span class="doc">Train Intent-Slot model on ATIS Dataset</span></a> for details about intent-slot model.)</p>
<ol class="arabic simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingList</span></code> layer is tasked with representing tokens. To do so we
can use learnable word embedding table in conjunction with learnable character
embedding table that are distilled to token level representation using CNN and
pooling.
Note: This class is meant to be reused by all models. It acts as a container
of all the different ways of representing a token/word.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">BiLSTMDocSlotAttention</span></code> is tasked with encoding the embedded input
string for intent classification and slot filling. In order to do that it has a
shared bidirectional LSTM layer followed by separate attention layers for
document level attention and word level attention. Finally it produces two
vectors per utterance.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">IntentSlotModelDecoder</span></code> accepts the two input vectors from
<cite>BiLSTMDocSlotAttention</cite> and produces logits for intent classification and
slot filling. Conditioned on a flag it can also use the probabilities from
intent classification for slot filling.</p></li>
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">IntentSlotOutputLayer</span></code> implements the logic behind computing loss and
prediction, as well as, how to export this layer to export to Caffe2. This is
used by model exporter as a post-processing Caffe2 operator.</p></li>
</ol>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>embedding</strong> (<em>EmbeddingBase</em>) – Description of parameter <cite>embedding</cite>.</p></li>
<li><p><strong>representation</strong> (<em>RepresentationBase</em>) – Description of parameter <cite>representation</cite>.</p></li>
<li><p><strong>decoder</strong> (<em>DecoderBase</em>) – Description of parameter <cite>decoder</cite>.</p></li>
<li><p><strong>output_layer</strong> (<em>OutputLayerBase</em>) – Description of parameter <cite>output_layer</cite>.</p></li>
</ul>
</dd>
</dl>
<dl class="attribute">
<dt id="pytext.models.Model.embedding">
<code class="sig-name descname">embedding</code><a class="headerlink" href="#pytext.models.Model.embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.Model.representation">
<code class="sig-name descname">representation</code><a class="headerlink" href="#pytext.models.Model.representation" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.Model.decoder">
<code class="sig-name descname">decoder</code><a class="headerlink" href="#pytext.models.Model.decoder" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="attribute">
<dt id="pytext.models.Model.output_layer">
<code class="sig-name descname">output_layer</code><a class="headerlink" href="#pytext.models.Model.output_layer" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.Model.compose_embedding">
<em class="property">classmethod </em><code class="sig-name descname">compose_embedding</code><span class="sig-paren">(</span><em class="sig-param">sub_emb_module_dict: Dict[str, pytext.models.embeddings.embedding_base.EmbeddingBase], metadata</em><span class="sig-paren">)</span> → pytext.models.embeddings.embedding_list.EmbeddingList<a class="reference internal" href="../_modules/pytext/models/model.html#Model.compose_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.Model.compose_embedding" title="Permalink to this definition">¶</a></dt>
<dd><p>Default implementation is to compose an instance of
<code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingList</span></code> with all the sub-embedding modules. You should
override this class method if you want to implement a specific way to
embed tokens/words.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>sub_emb_module_dict</strong> (<em>Dict</em><em>[</em><em>str</em><em>, </em><em>EmbeddingBase</em><em>]</em>) – Named dictionary of
embedding modules each of which implement a way to embed/encode
a token.</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An instance of <code class="xref py py-class docutils literal notranslate"><span class="pre">EmbeddingList</span></code>.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>EmbeddingList</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.Model.create_embedding">
<em class="property">classmethod </em><code class="sig-name descname">create_embedding</code><span class="sig-paren">(</span><em class="sig-param">feat_config: pytext.config.field_config.FeatureConfig</em>, <em class="sig-param">metadata: pytext.data.data_handler.CommonMetadata</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#Model.create_embedding"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.Model.create_embedding" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.Model.create_sub_embs">
<em class="property">classmethod </em><code class="sig-name descname">create_sub_embs</code><span class="sig-paren">(</span><em class="sig-param">emb_config: pytext.config.field_config.FeatureConfig</em>, <em class="sig-param">metadata: pytext.data.data_handler.CommonMetadata</em><span class="sig-paren">)</span> → Dict[str, pytext.models.embeddings.embedding_base.EmbeddingBase]<a class="reference internal" href="../_modules/pytext/models/model.html#Model.create_sub_embs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.Model.create_sub_embs" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates the embedding modules defined in the <cite>emb_config</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>emb_config</strong> (<em>FeatureConfig</em>) – Object containing all the sub-embedding
configurations.</p></li>
<li><p><strong>metadata</strong> (<em>CommonMetadata</em>) – Object containing features and label metadata.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Named dictionary of embedding modules.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>Dict[str, EmbeddingBase]</p>
</dd>
</dl>
</dd></dl>
<dl class="method">
<dt id="pytext.models.Model.forward">
<code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param">*inputs</em><span class="sig-paren">)</span> → List[torch.Tensor]<a class="reference internal" href="../_modules/pytext/models/model.html#Model.forward"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.Model.forward" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the <code class="xref py py-class docutils literal notranslate"><span class="pre">Module</span></code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div>
</dd></dl>
<dl class="method">
<dt id="pytext.models.Model.from_config">
<em class="property">classmethod </em><code class="sig-name descname">from_config</code><span class="sig-paren">(</span><em class="sig-param">config: pytext.models.model.Model.Config</em>, <em class="sig-param">feat_config: pytext.config.field_config.FeatureConfig</em>, <em class="sig-param">metadata: pytext.data.data_handler.CommonMetadata</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#Model.from_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.Model.from_config" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
<dl class="class">
<dt id="pytext.models.BaseModel">
<em class="property">class </em><code class="sig-prename descclassname">pytext.models.</code><code class="sig-name descname">BaseModel</code><span class="sig-paren">(</span><em class="sig-param">stage: pytext.common.constants.Stage = &lt;Stage.TRAIN: 'Training'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.modules.module.Module</span></code>, <a class="reference internal" href="pytext.config.html#pytext.config.component.Component" title="pytext.config.component.Component"><code class="xref py py-class docutils literal notranslate"><span class="pre">pytext.config.component.Component</span></code></a></p>
<p>Base model class which inherits from nn.Module. Also has a stage flag to
indicate it’s in <cite>train</cite>, <cite>eval</cite>, or <cite>test</cite> stage.
This is because the built-in train/eval flag in PyTorch can’t distinguish eval
and test, which is required to support some use cases.</p>
<dl class="attribute">
<dt id="pytext.models.BaseModel.SUPPORT_FP16_OPTIMIZER">
<code class="sig-name descname">SUPPORT_FP16_OPTIMIZER</code><em class="property"> = False</em><a class="headerlink" href="#pytext.models.BaseModel.SUPPORT_FP16_OPTIMIZER" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.arrange_model_context">
<code class="sig-name descname">arrange_model_context</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.arrange_model_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.arrange_model_context" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.arrange_model_inputs">
<code class="sig-name descname">arrange_model_inputs</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.arrange_model_inputs"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.arrange_model_inputs" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.arrange_targets">
<code class="sig-name descname">arrange_targets</code><span class="sig-paren">(</span><em class="sig-param">tensor_dict</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.arrange_targets"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.arrange_targets" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.caffe2_export">
<code class="sig-name descname">caffe2_export</code><span class="sig-paren">(</span><em class="sig-param">tensorizers</em>, <em class="sig-param">tensor_dict</em>, <em class="sig-param">path</em>, <em class="sig-param">export_onnx_path=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.caffe2_export"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.caffe2_export" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.contextualize">
<code class="sig-name descname">contextualize</code><span class="sig-paren">(</span><em class="sig-param">context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.contextualize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.contextualize" title="Permalink to this definition">¶</a></dt>
<dd><p>Add additional context into model. <cite>context</cite> can be anything that
helps maintaining/updating state. For example, it is used by
<code class="xref py py-class docutils literal notranslate"><span class="pre">DisjointMultitaskModel</span></code> for changing the task that should be
trained with a given iterator.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.eval">
<code class="sig-name descname">eval</code><span class="sig-paren">(</span><em class="sig-param">stage=&lt;Stage.TEST: 'Test'&gt;</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.eval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.eval" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to explicitly maintain the stage (train, eval, test).</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.get_loss">
<code class="sig-name descname">get_loss</code><span class="sig-paren">(</span><em class="sig-param">logit</em>, <em class="sig-param">target</em>, <em class="sig-param">context</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.get_loss"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.get_loss" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.get_param_groups_for_optimizer">
<code class="sig-name descname">get_param_groups_for_optimizer</code><span class="sig-paren">(</span><span class="sig-paren">)</span> → List[Dict[str, List[torch.nn.parameter.Parameter]]]<a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.get_param_groups_for_optimizer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.get_param_groups_for_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns a list of parameter groups of the format {“params”: param_list}.
The parameter groups loosely correspond to layers and are ordered from low
to high. Currently, only the embedding layer can provide multiple param groups,
and other layers are put into one param group. The output of this method
is passed to the optimizer so that schedulers can change learning rates
by layer.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.get_pred">
<code class="sig-name descname">get_pred</code><span class="sig-paren">(</span><em class="sig-param">logit</em>, <em class="sig-param">target=None</em>, <em class="sig-param">context=None</em>, <em class="sig-param">*args</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.get_pred"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.get_pred" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.prepare_for_onnx_export_">
<code class="sig-name descname">prepare_for_onnx_export_</code><span class="sig-paren">(</span><em class="sig-param">**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.prepare_for_onnx_export_"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.prepare_for_onnx_export_" title="Permalink to this definition">¶</a></dt>
<dd><p>Make model exportable via ONNX trace.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.quantize">
<code class="sig-name descname">quantize</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.quantize"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.quantize" title="Permalink to this definition">¶</a></dt>
<dd><p>Quantize the model during export.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.save_modules">
<code class="sig-name descname">save_modules</code><span class="sig-paren">(</span><em class="sig-param">base_path: str = ''</em>, <em class="sig-param">suffix: str = ''</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.save_modules"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.save_modules" title="Permalink to this definition">¶</a></dt>
<dd><p>Save each sub-module in separate files for reusing later.</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.train">
<code class="sig-name descname">train</code><span class="sig-paren">(</span><em class="sig-param">mode=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.train"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.train" title="Permalink to this definition">¶</a></dt>
<dd><p>Override to explicitly maintain the stage (train, eval, test).</p>
</dd></dl>
<dl class="method">
<dt id="pytext.models.BaseModel.train_batch">
<em class="property">classmethod </em><code class="sig-name descname">train_batch</code><span class="sig-paren">(</span><em class="sig-param">model</em>, <em class="sig-param">batch</em>, <em class="sig-param">state=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/pytext/models/model.html#BaseModel.train_batch"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#pytext.models.BaseModel.train_batch" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">PyText</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../train_your_first_model.html">Train your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../execute_your_first_model.html">Execute your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize_your_model.html">Visualize Model Training with TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytext_models_in_your_app.html">Use PyText models in your app</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serving_models_in_production.html">Serve Models in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_files.html">Config Files Explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_commands.html">Config Commands</a></li>
</ul>
<p class="caption"><span class="caption-text">Training More Advanced Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../atis_tutorial.html">Train Intent-Slot model on ATIS Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hierarchical_intent_slot_tutorial.html">Hierarchical intent and slot filling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../disjoint_multitask_tutorial.html">Multitask training with disjoint datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_training_tutorial.html">Data Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../xlm_r.html">XLM-RoBERTa</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending PyText</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasource_tutorial.html">Custom Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorizer.html">Custom Tensorizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dense.html">Using External Dense Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../create_new_model.html">Creating A New Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hacking_pytext.html">Hacking PyText</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../configs/pytext.html">pytext</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="pytext.html">pytext package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="pytext.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.builtin_task">pytext.builtin_task module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.main">pytext.main module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext.workflow">pytext.workflow module</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.html#module-pytext">Module contents</a></li>
</ul>
</li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="../index.html">Documentation overview</a><ul>
<li><a href="pytext.html">pytext package</a><ul>
<li>Previous: <a href="pytext.metrics.html" title="previous chapter">pytext.metrics package</a></li>
<li>Next: <a href="pytext.models.decoders.html" title="next chapter">pytext.models.decoders package</a></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="../search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>