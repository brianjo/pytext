
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<div class="section" id="trainer-config">
<h1>Trainer.Config<a class="headerlink" href="#trainer-config" title="Permalink to this headline">Â¶</a></h1>
<p><strong>Component:</strong> <a class="reference internal" href="../modules/pytext.trainers.html#pytext.trainers.trainer.Trainer" title="pytext.trainers.trainer.Trainer"><code class="xref py py-class docutils literal notranslate"><span class="pre">Trainer</span></code></a></p>
<dl class="class">
<dt>
<em class="property">class </em><code class="sig-prename descclassname">Trainer.</code><code class="sig-name descname">Config</code><a class="reference internal" href="../_modules/pytext/trainers/trainer.html#Trainer.Config"><span class="viewcode-link">[source]</span></a></dt>
<dd><p><strong>Bases:</strong> <a class="reference internal" href="../modules/pytext.config.html#pytext.config.pytext_config.ConfigBase" title="pytext.config.pytext_config.ConfigBase"><code class="xref py py-class docutils literal notranslate"><span class="pre">ConfigBase</span></code></a></p>
</dd></dl>
<p><strong>All Attributes (including base classes)</strong></p>
<blockquote>
<div><dl class="simple">
<dt><strong>epochs</strong>: int = <code class="docutils literal notranslate"><span class="pre">10</span></code></dt><dd><p>Training epochs</p>
</dd>
<dt><strong>early_stop_after</strong>: int = <code class="docutils literal notranslate"><span class="pre">0</span></code></dt><dd><p>Stop after how many epochs when the eval metric is not improving</p>
</dd>
<dt><strong>max_clip_norm</strong>: Optional[float] = <code class="docutils literal notranslate"><span class="pre">None</span></code></dt><dd><p>Clip gradient norm if set</p>
</dd>
<dt><strong>report_train_metrics</strong>: bool = <code class="docutils literal notranslate"><span class="pre">True</span></code></dt><dd><p>Whether metrics on training data should be computed and reported.</p>
</dd>
<dt><strong>target_time_limit_seconds</strong>: Optional[int] = <code class="docutils literal notranslate"><span class="pre">None</span></code></dt><dd><p>Target time limit for training, default (None) to no time limit.</p>
</dd>
<dt><strong>do_eval</strong>: bool = <code class="docutils literal notranslate"><span class="pre">True</span></code></dt><dd><p>Whether to do evaluation and model selection based on it.</p>
</dd>
<dt><strong>load_best_model_after_train</strong>: bool = <code class="docutils literal notranslate"><span class="pre">True</span></code></dt><dd><p></p>
</dd>
<dt><strong>num_samples_to_log_progress</strong>: int = <code class="docutils literal notranslate"><span class="pre">1000</span></code></dt><dd><p>Number of samples for logging training progress.</p>
</dd>
<dt><strong>num_accumulated_batches</strong>: int = <code class="docutils literal notranslate"><span class="pre">1</span></code></dt><dd><p>Number of forward &amp; backward per batch before update gradients, the
actual_batch_size = batch_size x num_accumulated_batches</p>
</dd>
<dt><strong>num_batches_per_epoch</strong>: Optional[int] = <code class="docutils literal notranslate"><span class="pre">None</span></code></dt><dd><p>Define epoch as a fixed number of batches. Subsequent epochs will continue
to iterate through the data, cycling through it when they reach the end.
If not set, use exactly one pass through the dataset as one epoch.
This configuration only affects the train epochs, test and eval
will always test their entire datasets.</p>
</dd>
<dt><strong>optimizer</strong>: <a class="reference internal" href="pytext.optimizer.optimizers.Optimizer.Config.html"><span class="doc">Optimizer.Config</span></a> = <a class="reference internal" href="pytext.optimizer.optimizers.Adam.Config.html"><span class="doc">Adam.Config</span></a>()</dt><dd><p>config for optimizer, used in parameter update</p>
</dd>
<dt><strong>scheduler</strong>: Optional[<a class="reference internal" href="pytext.optimizer.scheduler.Scheduler.Config.html"><span class="doc">Scheduler.Config</span></a>] = <code class="docutils literal notranslate"><span class="pre">None</span></code></dt><dd><p></p>
</dd>
<dt><strong>sparsifier</strong>: Optional[<a class="reference internal" href="pytext.optimizer.sparsifiers.sparsifier.Sparsifier.Config.html"><span class="doc">Sparsifier.Config</span></a>] = <code class="docutils literal notranslate"><span class="pre">None</span></code></dt><dd><p></p>
</dd>
<dt><strong>fp16_args</strong>: <a class="reference internal" href="pytext.optimizer.fp16_optimizer.FP16Optimizer.Config.html"><span class="doc">FP16Optimizer.Config</span></a> = <a class="reference internal" href="pytext.optimizer.fp16_optimizer.FP16OptimizerFairseq.Config.html"><span class="doc">FP16OptimizerFairseq.Config</span></a>()</dt><dd><p>Define arguments for fp16 training. A fp16_optimizer will be created
and wraps the original optimizer, which will scale loss during
backward and master weight will be maintained on original optimizer.
<a class="reference external" href="https://arxiv.org/abs/1710.03740">https://arxiv.org/abs/1710.03740</a></p>
</dd>
</dl>
</div></blockquote>
<dl class="simple">
<dt><strong>Subclasses</strong></dt><dd><ul class="simple">
<li><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TaskTrainer.Config</span></code></p></li>
</ul>
</dd>
</dl>
<p><strong>Default JSON</strong></p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">"epochs"</span><span class="p">:</span> <span class="mi">10</span><span class="p">,</span>
    <span class="nt">"early_stop_after"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
    <span class="nt">"max_clip_norm"</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
    <span class="nt">"report_train_metrics"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
    <span class="nt">"target_time_limit_seconds"</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
    <span class="nt">"do_eval"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
    <span class="nt">"load_best_model_after_train"</span><span class="p">:</span> <span class="kc">true</span><span class="p">,</span>
    <span class="nt">"num_samples_to_log_progress"</span><span class="p">:</span> <span class="mi">1000</span><span class="p">,</span>
    <span class="nt">"num_accumulated_batches"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
    <span class="nt">"num_batches_per_epoch"</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
    <span class="nt">"optimizer"</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">"Adam"</span><span class="p">:</span> <span class="p">{</span>
            <span class="nt">"lr"</span><span class="p">:</span> <span class="mf">0.001</span><span class="p">,</span>
            <span class="nt">"weight_decay"</span><span class="p">:</span> <span class="mf">1e-05</span><span class="p">,</span>
            <span class="nt">"eps"</span><span class="p">:</span> <span class="mf">1e-08</span>
        <span class="p">}</span>
    <span class="p">},</span>
    <span class="nt">"scheduler"</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
    <span class="nt">"sparsifier"</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
    <span class="nt">"fp16_args"</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">"FP16OptimizerFairseq"</span><span class="p">:</span> <span class="p">{</span>
            <span class="nt">"init_loss_scale"</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
            <span class="nt">"scale_window"</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
            <span class="nt">"scale_tolerance"</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
            <span class="nt">"threshold_loss_scale"</span><span class="p">:</span> <span class="kc">null</span><span class="p">,</span>
            <span class="nt">"min_loss_scale"</span><span class="p">:</span> <span class="mf">0.0001</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">PyText</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../train_your_first_model.html">Train your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../execute_your_first_model.html">Execute your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize_your_model.html">Visualize Model Training with TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../pytext_models_in_your_app.html">Use PyText models in your app</a></li>
<li class="toctree-l1"><a class="reference internal" href="../serving_models_in_production.html">Serve Models in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_files.html">Config Files Explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="../config_commands.html">Config Commands</a></li>
</ul>
<p class="caption"><span class="caption-text">Training More Advanced Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../atis_tutorial.html">Train Intent-Slot model on ATIS Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hierarchical_intent_slot_tutorial.html">Hierarchical intent and slot filling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../disjoint_multitask_tutorial.html">Multitask training with disjoint datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../distributed_training_tutorial.html">Data Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../xlm_r.html">XLM-RoBERTa</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending PyText</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../datasource_tutorial.html">Custom Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tensorizer.html">Custom Tensorizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dense.html">Using External Dense Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../create_new_model.html">Creating A New Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../hacking_pytext.html">Hacking PyText</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="pytext.html">pytext</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pytext.config.html">config</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.data.html">data</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.exporters.html">exporters</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.loss.html">loss</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.metric_reporters.html">metric_reporters</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.models.html">models</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.optimizer.html">optimizer</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytext.task.html">task</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="pytext.trainers.html">trainers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../modules/pytext.html">pytext package</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="../index.html">Documentation overview</a><ul>
<li><a href="pytext.html">pytext</a><ul>
<li><a href="pytext.trainers.html">trainers</a><ul>
<li><a href="pytext.trainers.trainer.html">trainer</a><ul>
<li>Previous: <a href="pytext.trainers.trainer.TaskTrainer.Config.html" title="previous chapter">TaskTrainer.Config</a></li>
<li>Next: <a href="pytext.trainers.trainer.TrainerBase.Config.html" title="next chapter">TrainerBase.Config</a></li>
</ul></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="../search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>