
<script type="text/javascript" id="documentation_options" data-url_root="./"
  src="/js/documentation_options.js"></script>
<script type="text/javascript" src="/js/jquery.js"></script>
<script type="text/javascript" src="/js/underscore.js"></script>
<script type="text/javascript" src="/js/doctools.js"></script>
<script type="text/javascript" src="/js/language_data.js"></script>
<script type="text/javascript" src="/js/searchtools.js"></script>
<div class="sphinx"><div class="document">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body" role="main">
<h1>Source code for pytext.models.model</h1><div class="highlight"><pre>
<span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="c1"># Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved</span>

<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.jit</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="kn">as</span> <span class="nn">nn</span>
<span class="kn">from</span> <span class="nn">pytext.common.constants</span> <span class="kn">import</span> <span class="n">Stage</span>
<span class="kn">from</span> <span class="nn">pytext.config.component</span> <span class="kn">import</span> <span class="n">Component</span><span class="p">,</span> <span class="n">ComponentType</span>
<span class="kn">from</span> <span class="nn">pytext.config.doc_classification</span> <span class="kn">import</span> <span class="n">ModelInput</span>
<span class="kn">from</span> <span class="nn">pytext.config.field_config</span> <span class="kn">import</span> <span class="n">FeatureConfig</span>
<span class="kn">from</span> <span class="nn">pytext.config.pytext_config</span> <span class="kn">import</span> <span class="n">ConfigBase</span><span class="p">,</span> <span class="n">ConfigBaseMeta</span>
<span class="kn">from</span> <span class="nn">pytext.config.serialize</span> <span class="kn">import</span> <span class="n">_is_optional</span>
<span class="kn">from</span> <span class="nn">pytext.data</span> <span class="kn">import</span> <span class="n">CommonMetadata</span>
<span class="kn">from</span> <span class="nn">pytext.data.tensorizers</span> <span class="kn">import</span> <span class="n">Tensorizer</span>
<span class="kn">from</span> <span class="nn">pytext.models.module</span> <span class="kn">import</span> <span class="n">create_module</span>
<span class="kn">from</span> <span class="nn">pytext.utils.file_io</span> <span class="kn">import</span> <span class="n">PathManager</span>
<span class="kn">from</span> <span class="nn">pytext.utils.precision</span> <span class="kn">import</span> <span class="n">maybe_float</span>
<span class="kn">from</span> <span class="nn">torch.jit</span> <span class="kn">import</span> <span class="n">quantized</span>

<span class="kn">from</span> <span class="nn">.decoders</span> <span class="kn">import</span> <span class="n">DecoderBase</span>
<span class="kn">from</span> <span class="nn">.embeddings</span> <span class="kn">import</span> <span class="n">EmbeddingBase</span><span class="p">,</span> <span class="n">EmbeddingList</span>
<span class="kn">from</span> <span class="nn">.output_layers</span> <span class="kn">import</span> <span class="n">OutputLayerBase</span>
<span class="kn">from</span> <span class="nn">.representations.representation_base</span> <span class="kn">import</span> <span class="n">RepresentationBase</span>


<span class="k">def</span> <span class="nf">_assert_tensorizer_type</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">t</span> <span class="ow">is</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="bp">None</span><span class="p">)</span> <span class="ow">and</span> <span class="ow">not</span> <span class="nb">issubclass</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Tensorizer</span><span class="o">.</span><span class="n">Config</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span>
            <span class="n">f</span><span class="s2">"ModelInput configuration should only include tensorizers: {t}"</span>
        <span class="p">)</span>


<div class="viewcode-block" id="ModelInputMeta"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.ModelInputMeta">[docs]</a><span class="k">class</span> <span class="nc">ModelInputMeta</span><span class="p">(</span><span class="n">ConfigBaseMeta</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__new__</span><span class="p">(</span><span class="n">metacls</span><span class="p">,</span> <span class="n">typename</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">namespace</span><span class="p">):</span>
        <span class="n">annotations</span> <span class="o">=</span> <span class="n">namespace</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">"__annotations__"</span><span class="p">,</span> <span class="p">{})</span>
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">annotations</span><span class="o">.</span><span class="n">values</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="s2">"__origin__"</span><span class="p">,</span> <span class="s2">""</span><span class="p">)</span> <span class="ow">is</span> <span class="n">Union</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">ut</span> <span class="ow">in</span> <span class="n">t</span><span class="o">.</span><span class="n">__args__</span><span class="p">:</span>
                    <span class="n">_assert_tensorizer_type</span><span class="p">(</span><span class="n">ut</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">_assert_tensorizer_type</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__new__</span><span class="p">(</span><span class="n">metacls</span><span class="p">,</span> <span class="n">typename</span><span class="p">,</span> <span class="n">bases</span><span class="p">,</span> <span class="n">namespace</span><span class="p">)</span></div>


<div class="viewcode-block" id="ModelInputBase"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.ModelInputBase">[docs]</a><span class="k">class</span> <span class="nc">ModelInputBase</span><span class="p">(</span><span class="n">ConfigBase</span><span class="p">,</span> <span class="n">metaclass</span><span class="o">=</span><span class="n">ModelInputMeta</span><span class="p">):</span>
    <span class="sd">"""Base class for model inputs."""</span></div>


<div class="viewcode-block" id="BaseModel"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel">[docs]</a><span class="k">class</span> <span class="nc">BaseModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">Component</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Base model class which inherits from nn.Module. Also has a stage flag to</span>
<span class="sd">    indicate it's in `train`, `eval`, or `test` stage.</span>
<span class="sd">    This is because the built-in train/eval flag in PyTorch can't distinguish eval</span>
<span class="sd">    and test, which is required to support some use cases.</span>
<span class="sd">    """</span>

    <span class="n">__EXPANSIBLE__</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="n">__COMPONENT_TYPE__</span> <span class="o">=</span> <span class="n">ComponentType</span><span class="o">.</span><span class="n">MODEL</span>

    <span class="n">SUPPORT_FP16_OPTIMIZER</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">class</span> <span class="nc">Config</span><span class="p">(</span><span class="n">Component</span><span class="o">.</span><span class="n">Config</span><span class="p">):</span>
        <span class="k">class</span> <span class="nc">ModelInput</span><span class="p">(</span><span class="n">ModelInputBase</span><span class="p">):</span>
            <span class="k">pass</span>

        <span class="n">inputs</span><span class="p">:</span> <span class="n">ModelInput</span> <span class="o">=</span> <span class="n">ModelInput</span><span class="p">()</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="p">:</span> <span class="n">Stage</span> <span class="o">=</span> <span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">=</span> <span class="n">stage</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">module_list</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">find_unused_parameters</span> <span class="o">=</span> <span class="bp">True</span>

<div class="viewcode-block" id="BaseModel.train"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.train">[docs]</a>    <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="sd">"""Override to explicitly maintain the stage (train, eval, test)."""</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">train</span><span class="p">(</span><span class="n">mode</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">=</span> <span class="n">Stage</span><span class="o">.</span><span class="n">TRAIN</span></div>

<div class="viewcode-block" id="BaseModel.eval"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.eval">[docs]</a>    <span class="k">def</span> <span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stage</span><span class="o">=</span><span class="n">Stage</span><span class="o">.</span><span class="n">TEST</span><span class="p">):</span>
        <span class="sd">"""Override to explicitly maintain the stage (train, eval, test)."""</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">stage</span> <span class="o">=</span> <span class="n">stage</span></div>

<div class="viewcode-block" id="BaseModel.contextualize"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.contextualize">[docs]</a>    <span class="k">def</span> <span class="nf">contextualize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="sd">"""Add additional context into model. `context` can be anything that</span>
<span class="sd">        helps maintaining/updating state. For example, it is used by</span>
<span class="sd">        :class:`~DisjointMultitaskModel` for changing the task that should be</span>
<span class="sd">        trained with a given iterator.</span>
<span class="sd">        """</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">context</span> <span class="o">=</span> <span class="n">context</span></div>

<div class="viewcode-block" id="BaseModel.get_loss"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.get_loss">[docs]</a>    <span class="k">def</span> <span class="nf">get_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseModel.get_pred"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.get_pred">[docs]</a>    <span class="k">def</span> <span class="nf">get_pred</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logit</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span><span class="o">.</span><span class="n">get_pred</span><span class="p">(</span><span class="n">logit</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseModel.save_modules"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.save_modules">[docs]</a>    <span class="k">def</span> <span class="nf">save_modules</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">base_path</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">""</span><span class="p">,</span> <span class="n">suffix</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">""</span><span class="p">):</span>
        <span class="sd">"""Save each sub-module in separate files for reusing later."""</span>

        <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="n">save_path</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">"save_path"</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">save_path</span><span class="p">:</span>
                <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_path</span><span class="p">,</span> <span class="n">module</span><span class="o">.</span><span class="n">save_path</span> <span class="o">+</span> <span class="n">suffix</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"Saving state of module {type(module).__name__} to {path} ..."</span><span class="p">)</span>
                <span class="k">with</span> <span class="n">PathManager</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">"wb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">save_file</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">ScriptModule</span><span class="p">):</span>
                        <span class="n">module</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">save_file</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">module</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="n">save_file</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">save</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseModel.prepare_for_onnx_export_"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.prepare_for_onnx_export_">[docs]</a>    <span class="k">def</span> <span class="nf">prepare_for_onnx_export_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">"""Make model exportable via ONNX trace."""</span>

        <span class="k">def</span> <span class="nf">apply_prepare_for_onnx_export_</span><span class="p">(</span><span class="n">module</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">module</span> <span class="o">!=</span> <span class="bp">self</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="s2">"prepare_for_onnx_export_"</span><span class="p">):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">prepare_for_onnx_export_</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">apply_prepare_for_onnx_export_</span><span class="p">)</span></div>

<div class="viewcode-block" id="BaseModel.quantize"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.quantize">[docs]</a>    <span class="k">def</span> <span class="nf">quantize</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">"""Quantize the model during export."""</span>
        <span class="c1"># by default only quantize the linear modules, override this method if your</span>
        <span class="c1"># model wants other modules quantized.</span>
        <span class="c1"># By default we dynamic quantize Linear for PyText models.</span>
        <span class="c1"># Todo: we can also add quantized torch.nn.LSTM/GRU support in the future.</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">quantize_dynamic</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span> <span class="p">{</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">},</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">qint8</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span>
        <span class="p">)</span></div>

<div class="viewcode-block" id="BaseModel.get_param_groups_for_optimizer"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.get_param_groups_for_optimizer">[docs]</a>    <span class="k">def</span> <span class="nf">get_param_groups_for_optimizer</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">]]]:</span>
        <span class="sd">"""</span>
<span class="sd">        Returns a list of parameter groups of the format {"params": param_list}.</span>
<span class="sd">        The parameter groups loosely correspond to layers and are ordered from low</span>
<span class="sd">        to high. Currently, only the embedding layer can provide multiple param groups,</span>
<span class="sd">        and other layers are put into one param group. The output of this method</span>
<span class="sd">        is passed to the optimizer so that schedulers can change learning rates</span>
<span class="sd">        by layer.</span>
<span class="sd">        """</span>
        <span class="n">non_emb_params</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
        <span class="n">model_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">non_emb_params</span><span class="p">]</span>

        <span class="c1"># some subclasses of Model (e.g. Ensemble) do not have embeddings</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">"embedding"</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">emb_params_by_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">get_param_groups_for_optimizer</span><span class="p">()</span>

            <span class="c1"># Delete params from the embedding layers</span>
            <span class="k">for</span> <span class="n">emb_params</span> <span class="ow">in</span> <span class="n">emb_params_by_layer</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">emb_params</span><span class="p">:</span>
                    <span class="k">del</span> <span class="n">non_emb_params</span><span class="p">[</span><span class="s2">"embedding.</span><span class="si">%s</span><span class="s2">"</span> <span class="o">%</span> <span class="n">name</span><span class="p">]</span>

            <span class="n">model_params</span> <span class="o">=</span> <span class="n">emb_params_by_layer</span> <span class="o">+</span> <span class="n">model_params</span>
            <span class="n">print_str</span> <span class="o">=</span> <span class="p">(</span>
                <span class="s2">"Model has </span><span class="si">%d</span><span class="s2"> param groups (</span><span class="si">%d</span><span class="s2"> from embedding module) for optimizer"</span>
            <span class="p">)</span>
            <span class="k">print</span><span class="p">(</span><span class="n">print_str</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model_params</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">emb_params_by_layer</span><span class="p">)))</span>

        <span class="n">model_params</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">"params"</span><span class="p">:</span> <span class="n">params</span><span class="o">.</span><span class="n">values</span><span class="p">()}</span> <span class="k">for</span> <span class="n">params</span> <span class="ow">in</span> <span class="n">model_params</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">model_params</span></div>

    <span class="c1">##################################</span>
    <span class="c1">#    New Model functions         #</span>
    <span class="c1">##################################</span>
    <span class="c1"># TODO: add back after migration</span>
    <span class="c1"># @classmethod</span>
    <span class="c1"># def from_config(cls, config: Config, tensorizers: Dict[str, Tensorizer]):</span>
    <span class="c1">#     raise NotImplementedError</span>
<div class="viewcode-block" id="BaseModel.train_batch"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.train_batch">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">train_batch</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="c1"># This is a class method so that it works when model is a DistributedModel</span>
        <span class="c1"># wrapper. Otherwise the forward call here skips the DDP forward call.</span>

        <span class="c1"># Forward pass through the network.</span>
        <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">arrange_model_inputs</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">model_context</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">arrange_model_context</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">targets</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">arrange_targets</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">model_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">model_inputs</span><span class="p">)</span>

        <span class="c1"># Add stage to context.</span>
        <span class="k">if</span> <span class="n">state</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">model_context</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
                <span class="n">model_context</span> <span class="o">=</span> <span class="p">{</span><span class="s2">"stage"</span><span class="p">:</span> <span class="n">state</span><span class="o">.</span><span class="n">stage</span><span class="p">}</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">model_context</span><span class="p">[</span><span class="s2">"stage"</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">stage</span>

        <span class="c1"># Compute loss and predictions.</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">maybe_float</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">get_loss</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">model_context</span><span class="p">))</span>
        <span class="n">predictions</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">get_pred</span><span class="p">(</span><span class="n">model_outputs</span><span class="p">,</span> <span class="n">context</span><span class="o">=</span><span class="n">model_context</span><span class="p">)</span>

        <span class="c1"># Pack results and return them.</span>
        <span class="n">metric_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">predictions</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">model_inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">metric_data</span></div>

<div class="viewcode-block" id="BaseModel.arrange_model_inputs"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.arrange_model_inputs">[docs]</a>    <span class="k">def</span> <span class="nf">arrange_model_inputs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_dict</span><span class="p">):</span>
        <span class="c1"># should raise NotImplementedError after migration is done</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="BaseModel.arrange_targets"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.arrange_targets">[docs]</a>    <span class="k">def</span> <span class="nf">arrange_targets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_dict</span><span class="p">):</span>
        <span class="c1"># should raise NotImplementedError after migration is done</span>
        <span class="k">pass</span></div>

<div class="viewcode-block" id="BaseModel.arrange_model_context"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.arrange_model_context">[docs]</a>    <span class="k">def</span> <span class="nf">arrange_model_context</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensor_dict</span><span class="p">):</span>
        <span class="c1"># should raise NotImplementedError after migration is done</span>
        <span class="k">return</span> <span class="bp">None</span></div>

<div class="viewcode-block" id="BaseModel.caffe2_export"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.BaseModel.caffe2_export">[docs]</a>    <span class="k">def</span> <span class="nf">caffe2_export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tensorizers</span><span class="p">,</span> <span class="n">tensor_dict</span><span class="p">,</span> <span class="n">path</span><span class="p">,</span> <span class="n">export_onnx_path</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="k">pass</span></div></div>


<div class="viewcode-block" id="Model"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.Model">[docs]</a><span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="sd">"""</span>
<span class="sd">    Generic single-task model class that expects four components:</span>

<span class="sd">    1. `Embedding`</span>
<span class="sd">    2. `Representation`</span>
<span class="sd">    3. `Decoder`</span>
<span class="sd">    4. `Output Layer`</span>

<span class="sd">    Forward pass: `embedding -&gt; representation -&gt; decoder -&gt; output_layer`</span>

<span class="sd">    These four components have specific responsibilities as described below.</span>

<span class="sd">    `Embedding` layer should implement the way to represent each token in the</span>
<span class="sd">    input text. It can be as simple as just token/word embedding or can be</span>
<span class="sd">    composed of multiple ways to represent a token, e.g., word embedding,</span>
<span class="sd">    character embedding, etc.</span>

<span class="sd">    `Representation` layer should implement the way to encode the entire input</span>
<span class="sd">    text such that the output vector(s) can be used by decoder to produce logits.</span>
<span class="sd">    There is no restriction on the number of inputs it should encode. There is</span>
<span class="sd">    also not restriction on the number of ways to encode input.</span>

<span class="sd">    `Decoder` layer should implement the way to consume the output of model's</span>
<span class="sd">    representation and produce logits that can be used by the output layer to</span>
<span class="sd">    compute loss or generate predictions (and prediction scores/confidence)</span>

<span class="sd">    `Output layer` should implement the way loss computation is done as well as</span>
<span class="sd">    the logic to generate predictions from the logits.</span>

<span class="sd">    Let us discuss the joint intent-slot model as a case to go over these layers.</span>
<span class="sd">    The model predicts intent of input utterance and the slots in the utterance.</span>
<span class="sd">    (Refer to :doc:`/atis_tutorial` for details about intent-slot model.)</span>

<span class="sd">    1. :class:`~EmbeddingList` layer is tasked with representing tokens. To do so we</span>
<span class="sd">       can use learnable word embedding table in conjunction with learnable character</span>
<span class="sd">       embedding table that are distilled to token level representation using CNN and</span>
<span class="sd">       pooling.</span>
<span class="sd">       Note: This class is meant to be reused by all models. It acts as a container</span>
<span class="sd">       of all the different ways of representing a token/word.</span>
<span class="sd">    2. :class:`~BiLSTMDocSlotAttention` is tasked with encoding the embedded input</span>
<span class="sd">       string for intent classification and slot filling. In order to do that it has a</span>
<span class="sd">       shared bidirectional LSTM layer followed by separate attention layers for</span>
<span class="sd">       document level attention and word level attention. Finally it produces two</span>
<span class="sd">       vectors per utterance.</span>
<span class="sd">    3. :class:`~IntentSlotModelDecoder` accepts the two input vectors from</span>
<span class="sd">       `BiLSTMDocSlotAttention` and produces logits for intent classification and</span>
<span class="sd">       slot filling. Conditioned on a flag it can also use the probabilities from</span>
<span class="sd">       intent classification for slot filling.</span>
<span class="sd">    4. :class:`~IntentSlotOutputLayer` implements the logic behind computing loss and</span>
<span class="sd">       prediction, as well as, how to export this layer to export to Caffe2. This is</span>
<span class="sd">       used by model exporter as a post-processing Caffe2 operator.</span>


<span class="sd">    Args:</span>
<span class="sd">        embedding (EmbeddingBase): Description of parameter `embedding`.</span>
<span class="sd">        representation (RepresentationBase): Description of parameter `representation`.</span>
<span class="sd">        decoder (DecoderBase): Description of parameter `decoder`.</span>
<span class="sd">        output_layer (OutputLayerBase): Description of parameter `output_layer`.</span>

<span class="sd">    Attributes:</span>
<span class="sd">        embedding</span>
<span class="sd">        representation</span>
<span class="sd">        decoder</span>
<span class="sd">        output_layer</span>

<span class="sd">    """</span>

    <span class="n">__EXPANSIBLE__</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="k">class</span> <span class="nc">Config</span><span class="p">(</span><span class="n">BaseModel</span><span class="o">.</span><span class="n">Config</span><span class="p">):</span>
        <span class="n">representation</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">decoder</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="n">output_layer</span> <span class="o">=</span> <span class="bp">None</span>
        <span class="c1"># This config flag tells the model whether its parameters are being</span>
        <span class="c1"># loaded from saved state, hence it can skip certain initialization</span>
        <span class="c1"># steps such as loading pre-trained embeddings from file.</span>
        <span class="c1">#</span>
        <span class="c1"># TODO (geoffreygoh): Using config for such a purpose is really a hack,</span>
        <span class="c1"># and the alternative is either to pickle model objects directly so we</span>
        <span class="c1"># can skip initialization, or to pass this flag as an additional param</span>
        <span class="c1"># to create_model (which will involve changing from_config method of</span>
        <span class="c1"># every model in the repository). Clean this up once the above pickling</span>
        <span class="c1"># solution is fully explored.</span>
        <span class="n">init_from_saved_state</span> <span class="o">=</span> <span class="bp">False</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">embedding</span><span class="p">:</span> <span class="n">EmbeddingBase</span><span class="p">,</span>
        <span class="n">representation</span><span class="p">:</span> <span class="n">RepresentationBase</span><span class="p">,</span>
        <span class="n">decoder</span><span class="p">:</span> <span class="n">DecoderBase</span><span class="p">,</span>
        <span class="n">output_layer</span><span class="p">:</span> <span class="n">OutputLayerBase</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="bp">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">representation</span> <span class="o">=</span> <span class="n">representation</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span> <span class="o">=</span> <span class="n">decoder</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">output_layer</span>

<div class="viewcode-block" id="Model.create_sub_embs"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.Model.create_sub_embs">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">create_sub_embs</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">emb_config</span><span class="p">:</span> <span class="n">FeatureConfig</span><span class="p">,</span> <span class="n">metadata</span><span class="p">:</span> <span class="n">CommonMetadata</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EmbeddingBase</span><span class="p">]:</span>
        <span class="sd">"""</span>
<span class="sd">        Creates the embedding modules defined in the `emb_config`.</span>

<span class="sd">        Args:</span>
<span class="sd">            emb_config (FeatureConfig): Object containing all the sub-embedding</span>
<span class="sd">                configurations.</span>
<span class="sd">            metadata (CommonMetadata): Object containing features and label metadata.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, EmbeddingBase]: Named dictionary of embedding modules.</span>

<span class="sd">        """</span>
        <span class="n">sub_emb_module_dict</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">config</span> <span class="ow">in</span> <span class="n">emb_config</span><span class="o">.</span><span class="n">_asdict</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">issubclass</span><span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="s2">"__COMPONENT__"</span><span class="p">,</span> <span class="nb">object</span><span class="p">),</span> <span class="n">EmbeddingBase</span><span class="p">):</span>
                <span class="n">sub_emb_module_dict</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">create_module</span><span class="p">(</span>
                    <span class="n">config</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s2">"{name} is not a config of embedding, skipping"</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">sub_emb_module_dict</span></div>

<div class="viewcode-block" id="Model.compose_embedding"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.Model.compose_embedding">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">compose_embedding</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">sub_emb_module_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">EmbeddingBase</span><span class="p">],</span> <span class="n">metadata</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">EmbeddingList</span><span class="p">:</span>
        <span class="sd">"""Default implementation is to compose an instance of</span>
<span class="sd">        :class:`~EmbeddingList` with all the sub-embedding modules. You should</span>
<span class="sd">        override this class method if you want to implement a specific way to</span>
<span class="sd">        embed tokens/words.</span>

<span class="sd">        Args:</span>
<span class="sd">            sub_emb_module_dict (Dict[str, EmbeddingBase]): Named dictionary of</span>
<span class="sd">                embedding modules each of which implement a way to embed/encode</span>
<span class="sd">                a token.</span>

<span class="sd">        Returns:</span>
<span class="sd">            EmbeddingList: An instance of :class:`~EmbeddingList`.</span>

<span class="sd">        """</span>
        <span class="k">return</span> <span class="n">EmbeddingList</span><span class="p">(</span><span class="n">sub_emb_module_dict</span><span class="o">.</span><span class="n">values</span><span class="p">(),</span> <span class="n">concat</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.create_embedding"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.Model.create_embedding">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">create_embedding</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">feat_config</span><span class="p">:</span> <span class="n">FeatureConfig</span><span class="p">,</span> <span class="n">metadata</span><span class="p">:</span> <span class="n">CommonMetadata</span><span class="p">):</span>
        <span class="n">sub_emb_module_dict</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">create_sub_embs</span><span class="p">(</span><span class="n">feat_config</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span>
        <span class="n">emb_module</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">compose_embedding</span><span class="p">(</span><span class="n">sub_emb_module_dict</span><span class="p">,</span> <span class="n">metadata</span><span class="p">)</span>
        <span class="n">emb_module</span><span class="o">.</span><span class="n">config</span> <span class="o">=</span> <span class="n">feat_config</span>
        <span class="k">return</span> <span class="n">emb_module</span></div>

<div class="viewcode-block" id="Model.from_config"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.Model.from_config">[docs]</a>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_config</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">feat_config</span><span class="p">:</span> <span class="n">FeatureConfig</span><span class="p">,</span> <span class="n">metadata</span><span class="p">:</span> <span class="n">CommonMetadata</span>
    <span class="p">):</span>
        <span class="n">embedding</span> <span class="o">=</span> <span class="n">create_module</span><span class="p">(</span>
            <span class="n">feat_config</span><span class="p">,</span> <span class="n">create_fn</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">create_embedding</span><span class="p">,</span> <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span>
        <span class="p">)</span>
        <span class="n">representation</span> <span class="o">=</span> <span class="n">create_module</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">representation</span><span class="p">,</span> <span class="n">embed_dim</span><span class="o">=</span><span class="n">embedding</span><span class="o">.</span><span class="n">embedding_dim</span>
        <span class="p">)</span>
        <span class="c1"># Find all inputs for the decoder layer</span>
        <span class="n">decoder_in_dim</span> <span class="o">=</span> <span class="n">representation</span><span class="o">.</span><span class="n">representation_dim</span>
        <span class="n">decoder_input_features_count</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">decoder_feat</span> <span class="ow">in</span> <span class="p">(</span><span class="n">ModelInput</span><span class="o">.</span><span class="n">DENSE_FEAT</span><span class="p">,):</span>  <span class="c1"># Only 1 right now.</span>
            <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">feat_config</span><span class="p">,</span> <span class="n">decoder_feat</span><span class="p">,</span> <span class="bp">False</span><span class="p">):</span>
                <span class="n">decoder_input_features_count</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">decoder_in_dim</span> <span class="o">+=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">feat_config</span><span class="p">,</span> <span class="n">ModelInput</span><span class="o">.</span><span class="n">DENSE_FEAT</span><span class="p">)</span><span class="o">.</span><span class="n">dim</span>
        <span class="n">decoder</span> <span class="o">=</span> <span class="n">create_module</span><span class="p">(</span>
            <span class="n">config</span><span class="o">.</span><span class="n">decoder</span><span class="p">,</span> <span class="n">in_dim</span><span class="o">=</span><span class="n">decoder_in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="o">=</span><span class="n">metadata</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="p">)</span>
        <span class="n">decoder</span><span class="o">.</span><span class="n">num_decoder_modules</span> <span class="o">=</span> <span class="n">decoder_input_features_count</span>
        <span class="n">output_layer</span> <span class="o">=</span> <span class="n">create_module</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">output_layer</span><span class="p">,</span> <span class="n">metadata</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">representation</span><span class="p">,</span> <span class="n">decoder</span><span class="p">,</span> <span class="n">output_layer</span><span class="p">)</span></div>

<div class="viewcode-block" id="Model.forward"><a class="viewcode-back" href="../../../modules/pytext.models.html#pytext.models.model.Model.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">inputs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
        <span class="n">embedding_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">num_emb_modules</span><span class="p">]</span>
        <span class="n">token_emb</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="o">*</span><span class="n">embedding_input</span><span class="p">)</span>
        <span class="n">other_input</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">num_emb_modules</span> <span class="p">:</span> <span class="nb">len</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
            <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">num_decoder_modules</span>
        <span class="p">]</span>
        <span class="n">input_representation</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">representation</span><span class="p">(</span><span class="n">token_emb</span><span class="p">,</span> <span class="o">*</span><span class="n">other_input</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_representation</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
            <span class="n">input_representation</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_representation</span><span class="p">]</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_representation</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="c1"># since some lstm based representations return states as (h0, c0)</span>
            <span class="n">input_representation</span> <span class="o">=</span> <span class="n">input_representation</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">decoder_inputs</span><span class="p">:</span> <span class="nb">tuple</span> <span class="o">=</span> <span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">num_decoder_modules</span><span class="p">:</span>
            <span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">num_decoder_modules</span> <span class="p">:]</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
            <span class="o">*</span><span class="n">input_representation</span><span class="p">,</span> <span class="o">*</span><span class="n">decoder_inputs</span>
        <span class="p">)</span>  <span class="c1"># returned Tensor's dim = (batch_size, num_classes)</span></div></div>
</pre></div>
</div>
</div>
</div>
<div aria-label="main navigation" class="sphinxsidebar" role="navigation">
<div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">PyText</a></h1>
<h3>Navigation</h3>
<p class="caption"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../train_your_first_model.html">Train your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../execute_your_first_model.html">Execute your first model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../visualize_your_model.html">Visualize Model Training with TensorBoard</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../pytext_models_in_your_app.html">Use PyText models in your app</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../serving_models_in_production.html">Serve Models in Production</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_files.html">Config Files Explained</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../config_commands.html">Config Commands</a></li>
</ul>
<p class="caption"><span class="caption-text">Training More Advanced Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../atis_tutorial.html">Train Intent-Slot model on ATIS Dataset</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hierarchical_intent_slot_tutorial.html">Hierarchical intent and slot filling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../disjoint_multitask_tutorial.html">Multitask training with disjoint datasets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../distributed_training_tutorial.html">Data Parallel Distributed Training</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../xlm_r.html">XLM-RoBERTa</a></li>
</ul>
<p class="caption"><span class="caption-text">Extending PyText</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../overview.html">Architecture Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../datasource_tutorial.html">Custom Data Format</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../tensorizer.html">Custom Tensorizer</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../dense.html">Using External Dense Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../create_new_model.html">Creating A New Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../hacking_pytext.html">Hacking PyText</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../configs/pytext.html">pytext</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../modules/pytext.html">pytext package</a></li>
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
<li><a href="../../../index.html">Documentation overview</a><ul>
<li><a href="../../index.html">Module code</a><ul>
<li><a href="../../pytext.html">pytext</a><ul>
</ul></li>
</ul></li>
</ul></li>
</ul>
</div>
<div id="searchbox" role="search" style="display: none">
<h3 id="searchlabel">Quick search</h3>
<div class="searchformwrapper">
<form action="../../../search.html" class="search" method="get">
<input aria-labelledby="searchlabel" name="q" type="text"/>
<input type="submit" value="Go"/>
</form>
</div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
</div>
</div>
<div class="clearer"></div>
</div></div>