TokenTensorizer.Config
======================



**Component:** :class:`TokenTensorizer  <pytext.data.tensorizers.TokenTensorizer>`\ 


.. py:currentmodule:: pytext.data.tensorizers
.. py:class:: TokenTensorizer.Config
  :noindex:

  **Bases:** :class:`Tensorizer.Config <pytext.data.tensorizers.Tensorizer.Config>`\ 

  

**All Attributes (including base classes)**

  **is_input**: bool = ``True``
    \ 

  **column**: str = ``'text'``
    The name of the text column to parse from the data source.
    

  **tokenizer**: :doc:`Tokenizer.Config <pytext.data.tokenizers.tokenizer.Tokenizer.Config>` = :doc:`Tokenizer.Config <pytext.data.tokenizers.tokenizer.Tokenizer.Config>`\ ()
    The tokenizer to use to split input text into tokens.
    

  **add_bos_token**: bool = ``False``
    \ 

  **add_eos_token**: bool = ``False``
    \ 

  **use_eos_token_for_bos**: bool = ``False``
    \ 

  **max_seq_len**: Optional[int] = ``None``
    \ 

  **vocab**: :doc:`VocabConfig <pytext.data.tensorizers.VocabConfig>` = :doc:`VocabConfig <pytext.data.tensorizers.VocabConfig>`\ ()
    \ 

  **vocab_file_delimiter**: str = ``' '``
    \ 



**Subclasses**
  - :class:`SquadTensorizer.Config <pytext.data.squad_tensorizer.SquadTensorizer.Config>`\ 
  - :class:`SquadTensorizerForKD.Config <pytext.data.squad_tensorizer.SquadTensorizerForKD.Config>`\ 
  - :class:`CharacterTokenTensorizer.Config <pytext.data.tensorizers.CharacterTokenTensorizer.Config>`\ 


**Default JSON**


.. code-block:: json


  {
      "is_input": true,
      "column": "text",
      "tokenizer": {
          "Tokenizer": {
              "split_regex": "\\s+",
              "lowercase": true
          }
      },
      "add_bos_token": false,
      "add_eos_token": false,
      "use_eos_token_for_bos": false,
      "max_seq_len": null,
      "vocab": {
          "build_from_data": true,
          "size_from_data": 0,
          "vocab_files": []
      },
      "vocab_file_delimiter": " "
  }