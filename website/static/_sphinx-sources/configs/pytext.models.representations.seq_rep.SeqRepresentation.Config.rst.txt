SeqRepresentation.Config
========================



**Component:** :class:`SeqRepresentation  <pytext.models.representations.seq_rep.SeqRepresentation>`\ 


.. py:currentmodule:: pytext.models.representations.seq_rep
.. py:class:: SeqRepresentation.Config
  :noindex:

  **Bases:** :class:`RepresentationBase.Config <pytext.models.representations.representation_base.RepresentationBase.Config>`\ 

  

**All Attributes (including base classes)**

  **load_path**: Optional[str] = ``None``
    \ 

  **save_path**: Optional[str] = ``None``
    \ 

  **freeze**: bool = ``False``
    \ 

  **shared_module_key**: Optional[str] = ``None``
    \ 

  **doc_representation**: :doc:`DocNNRepresentation.Config <pytext.models.representations.docnn.DocNNRepresentation.Config>` = :doc:`DocNNRepresentation.Config <pytext.models.representations.docnn.DocNNRepresentation.Config>`\ ()
    \ 

  **seq_representation**: Union[:doc:`BiLSTMDocAttention.Config <pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.Config>`, :doc:`DocNNRepresentation.Config <pytext.models.representations.docnn.DocNNRepresentation.Config>`] = :doc:`BiLSTMDocAttention.Config <pytext.models.representations.bilstm_doc_attention.BiLSTMDocAttention.Config>`\ ()
    \ 



**Default JSON**


.. code-block:: json


  {
      "load_path": null,
      "save_path": null,
      "freeze": false,
      "shared_module_key": null,
      "doc_representation": {
          "load_path": null,
          "save_path": null,
          "freeze": false,
          "shared_module_key": null,
          "dropout": 0.4,
          "cnn": {
              "kernel_num": 100,
              "kernel_sizes": [
                  3,
                  4
              ],
              "weight_norm": false,
              "dilated": false,
              "causal": false
          }
      },
      "seq_representation": {
          "BiLSTMDocAttention": {
              "load_path": null,
              "save_path": null,
              "freeze": false,
              "shared_module_key": null,
              "dropout": 0.4,
              "lstm": {
                  "load_path": null,
                  "save_path": null,
                  "freeze": false,
                  "shared_module_key": null,
                  "dropout": 0.4,
                  "lstm_dim": 32,
                  "num_layers": 1,
                  "bidirectional": true,
                  "pack_sequence": true
              },
              "pooling": {
                  "SelfAttention": {
                      "attn_dimension": 64,
                      "dropout": 0.4
                  }
              },
              "mlp_decoder": null
          }
      }
  }