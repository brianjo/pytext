RoBERTaTokenLevelTensorizer.Config
==================================



**Component:** :class:`RoBERTaTokenLevelTensorizer  <pytext.data.roberta_tensorizer.RoBERTaTokenLevelTensorizer>`\ 


.. py:currentmodule:: pytext.data.roberta_tensorizer
.. py:class:: RoBERTaTokenLevelTensorizer.Config
  :noindex:

  **Bases:** :class:`RoBERTaTensorizer.Config <pytext.data.roberta_tensorizer.RoBERTaTensorizer.Config>`\ 

  

**All Attributes (including base classes)**

  **is_input**: bool = ``True``
    \ 

  **columns**: list[str] = ``['text']``
    \ 

  **tokenizer**: :doc:`Tokenizer.Config <pytext.data.tokenizers.tokenizer.Tokenizer.Config>` = :doc:`GPT2BPETokenizer.Config <pytext.data.tokenizers.tokenizer.GPT2BPETokenizer.Config>`\ ()
    \ 

  **base_tokenizer**: Optional[:doc:`Tokenizer.Config <pytext.data.tokenizers.tokenizer.Tokenizer.Config>`] = ``None``
    \ 

  **vocab_file**: str = ``'manifold://pytext_training/tree/static/vocabs/bpe/gpt2/dict.txt'``
    \ 

  **max_seq_len**: int = ``256``
    \ 

  **labels_columns**: list[str] = ``['label']``
    \ 

  **labels**: list[str] = ``[]``
    \ 



**Default JSON**


.. code-block:: json


  {
      "is_input": true,
      "columns": [
          "text"
      ],
      "tokenizer": {
          "GPT2BPETokenizer": {
              "bpe_encoder_path": "manifold://pytext_training/tree/static/vocabs/bpe/gpt2/encoder.json",
              "bpe_vocab_path": "manifold://pytext_training/tree/static/vocabs/bpe/gpt2/vocab.bpe"
          }
      },
      "base_tokenizer": null,
      "vocab_file": "manifold://pytext_training/tree/static/vocabs/bpe/gpt2/dict.txt",
      "max_seq_len": 256,
      "labels_columns": [
          "label"
      ],
      "labels": []
  }