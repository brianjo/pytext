BiLSTMSlotAttention.Config
==========================



**Component:** :class:`BiLSTMSlotAttention  <pytext.models.representations.bilstm_slot_attn.BiLSTMSlotAttention>`\ 


.. py:currentmodule:: pytext.models.representations.bilstm_slot_attn
.. py:class:: BiLSTMSlotAttention.Config
  :noindex:

  **Bases:** :class:`RepresentationBase.Config <pytext.models.representations.representation_base.RepresentationBase.Config>`\ 

  

**All Attributes (including base classes)**

  **load_path**: Optional[str] = ``None``
    \ 

  **save_path**: Optional[str] = ``None``
    \ 

  **freeze**: bool = ``False``
    \ 

  **shared_module_key**: Optional[str] = ``None``
    \ 

  **dropout**: float = ``0.4``
    \ 

  **lstm**: :doc:`BiLSTM.Config <pytext.models.representations.bilstm.BiLSTM.Config>` = :doc:`BiLSTM.Config <pytext.models.representations.bilstm.BiLSTM.Config>`\ ()
    \ 

  **slot_attention**: :doc:`SlotAttention.Config <pytext.models.representations.slot_attention.SlotAttention.Config>` = :doc:`SlotAttention.Config <pytext.models.representations.slot_attention.SlotAttention.Config>`\ ()
    \ 

  **mlp_decoder**: Optional[:doc:`MLPDecoder.Config <pytext.models.decoders.mlp_decoder.MLPDecoder.Config>`] = ``None``
    \ 



**Default JSON**


.. code-block:: json


  {
      "load_path": null,
      "save_path": null,
      "freeze": false,
      "shared_module_key": null,
      "dropout": 0.4,
      "lstm": {
          "load_path": null,
          "save_path": null,
          "freeze": false,
          "shared_module_key": null,
          "dropout": 0.4,
          "lstm_dim": 32,
          "num_layers": 1,
          "bidirectional": true,
          "pack_sequence": true
      },
      "slot_attention": {
          "attn_dimension": 64,
          "attention_type": "no_attention"
      },
      "mlp_decoder": null
  }