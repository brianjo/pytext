MaskedLanguageModel.Config
==========================



**Component:** :class:`MaskedLanguageModel  <pytext.models.masked_lm.MaskedLanguageModel>`\ 


.. py:currentmodule:: pytext.models.masked_lm
.. py:class:: MaskedLanguageModel.Config
  :noindex:

  **Bases:** :class:`BaseModel.Config <pytext.models.model.BaseModel.Config>`\ 

  

**All Attributes (including base classes)**

  **inputs**: :doc:`InputConfig <pytext.models.masked_lm.InputConfig>` = :doc:`InputConfig <pytext.models.masked_lm.InputConfig>`\ ()
    \ 

  **encoder**: :doc:`TransformerSentenceEncoderBase.Config <pytext.models.representations.transformer_sentence_encoder_base.TransformerSentenceEncoderBase.Config>` = :doc:`TransformerSentenceEncoder.Config <pytext.models.representations.transformer_sentence_encoder.TransformerSentenceEncoder.Config>`\ ()
    \ 

  **decoder**: :doc:`MLPDecoder.Config <pytext.models.decoders.mlp_decoder.MLPDecoder.Config>` = :doc:`MLPDecoder.Config <pytext.models.decoders.mlp_decoder.MLPDecoder.Config>`\ ()
    \ 

  **output_layer**: :doc:`LMOutputLayer.Config <pytext.models.output_layers.lm_output_layer.LMOutputLayer.Config>` = :doc:`LMOutputLayer.Config <pytext.models.output_layers.lm_output_layer.LMOutputLayer.Config>`\ ()
    \ 

  **mask_prob**: float = ``0.15``
    \ 

  **mask_bos**: bool = ``False``
    \ 

  **masking_strategy**: MaskingStrategy = ``<MaskingStrategy.RANDOM: 'random'>``
    \ 

  **tie_weights**: bool = ``True``
    \ 



**Default JSON**


.. code-block:: json


  {
      "inputs": {
          "tokens": {
              "BERTTensorizerBase": {
                  "is_input": true,
                  "columns": [
                      "text"
                  ],
                  "tokenizer": {
                      "Tokenizer": {
                          "split_regex": "\\s+",
                          "lowercase": true
                      }
                  },
                  "base_tokenizer": null,
                  "vocab_file": "",
                  "max_seq_len": 128
              }
          }
      },
      "encoder": {
          "TransformerSentenceEncoder": {
              "load_path": null,
              "save_path": null,
              "freeze": false,
              "shared_module_key": null,
              "output_dropout": 0.4,
              "embedding_dim": 768,
              "pooling": "cls_token",
              "export": false,
              "dropout": 0.1,
              "attention_dropout": 0.1,
              "activation_dropout": 0.1,
              "ffn_embedding_dim": 3072,
              "num_encoder_layers": 6,
              "num_attention_heads": 8,
              "num_segments": 2,
              "use_position_embeddings": true,
              "offset_positions_by_padding": true,
              "apply_bert_init": true,
              "encoder_normalize_before": true,
              "activation_fn": "relu",
              "projection_dim": 0,
              "max_seq_len": 128,
              "multilingual": false,
              "freeze_embeddings": false,
              "n_trans_layers_to_freeze": 0,
              "use_torchscript": false
          }
      },
      "decoder": {
          "load_path": null,
          "save_path": null,
          "freeze": false,
          "shared_module_key": null,
          "hidden_dims": [],
          "out_dim": null,
          "layer_norm": false,
          "dropout": 0.0,
          "activation": "relu"
      },
      "output_layer": {
          "load_path": null,
          "save_path": null,
          "freeze": false,
          "shared_module_key": null,
          "loss": {}
      },
      "mask_prob": 0.15,
      "mask_bos": false,
      "masking_strategy": "random",
      "tie_weights": true
  }